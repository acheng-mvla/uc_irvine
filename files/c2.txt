Table 1: summary statistics for concepts lists by party identification after pre-processing. Welfare Government American Values Democrat Republican 
DRDRDRDRD R # of unique tokens 1291 912 1552 1085 1507 966 1347 1116 1513 992 Prop. overlap 0.277 0.367 0.305 0.370 0.313 0.339 0.281 0.416 0.303 0.352 Mean list length 14.105 12.848 15.194 14.561 14.354 14.251 14.266 13.696 14.677 14.133 
(4.499) (4.665) (4.384) (4.322) (4.432) (4.558) (4.294) (4.544) (4.254) (4.352) Notes: D = Democrats and R = Republicans. Prop. overlap corresponds to the ratio of unique tokens to total tokens listed. Numbers in parentheses denote standard deviations of mean list length. 
network that is parametererized by an initial probabil ity vector ⇡ which contains the probabilities of jumping from the cue word (e.g. “animal”) to a given node (e.g. “dog”) and a transition matrix P where each element of the matrix Pij represents the probability of transition ing from word i to word j (e.g. from “dog” to “cat”) in one step on the network. For a given ⇡ and P, we can compute the likelihood of each list in our dataset and use maximum likelihood or Bayesian inference to infer the parameters of our semantic network. In the past, estimating representations in this way was not possible because the requirement that no word be repeated makes the likelihood of a true generative model non-trivial to compute. Previous models were either non-generative (e.g. Go˜ni et al. (2011)) and could not give likelihoods or were biased in their estimation process (Millsap & Meredith, 1987). Only recently has a generative model been proposed which could give likelihoods of produc ing semantic fluency lists under a set of estimated pa rameters (which determine the semantic representation). Jun, Zhu, Rogers, and Yang (2015) show that by as suming a particular model for the search process, they can estimate the semantic representation of a group that predicts new lists better than previous biased methods. Building on Austerweil et al. (2012), Jun et al. propose a model, called INVITE, whereby retrieval consists of a random walk through the semantic network with words being added to the semantic fluency list every time it reaches a new node. However, due to the constraints of the task, a word that has already been said cannot be repeated so if the random walk reaches a node that corresponds to a repeated word, no word is emitted. 
By using the same generative model for both groups, Democrats and Republicans, we are assuming that there are no systematic di↵erences in the search algorithm employed to retrieve associations. We argue that the search algorithm is likely to be a more fundamental cog nitive process independent of individual di↵erences in party identification. In Halpern and Rodriguez (2018) we tested this assumption by comparing the performance of several di↵erent models estimated separately on the two groups. The ranking of models according to the log likelihood of held-out lists was the same for both groups, lending support to our assumption. 3 
3In this model comparison, we found that INVITE yields 
Individual Di↵erences 
We divide up our data into 10 folds, stratifying on party identity. Estimation of the networks is easier and more reliable if it is limited to words that were included in several subjects’ lists. Given the spread of words that subjects used, we restrict our estimation to the top 30 tokens said for each topic. We estimated a maxi mum likelihood “population semantic network” for self identifying Democrats and Republicans (using LBFGS in rStan (Carpenter et al., 2016)) on a training set of 9 of the folds and then evaluated the log-likelihood of the heldout fold under each of these two semantic net works. Figure 2 plots an example of an estimated Demo crat semantic representation for the concept Republican. Across all ten heldout folds, for all concepts and both parties we find that the within-party log-likelihood is significantly higher than across-party log-likelihood. As a measure of how well our model is able to di↵erentiate parties, we can treat our model as a Bayesian classifier and assign the party with the higher log-likelihood to each list. Figure 1 plots the average accuracy of this classifier by concept. In all cases, the classifier is able to perform significantly better than chance 4. The accu racy score in this case has a theoretically substantive in terpretation: the larger the representational di↵erences between groups, the easier it is for a classifier to distin guish between a Republican and Democrat resulting in a larger accuracy score, for political scientists this can be understood as a measure of “polarization” (Peterson & Spirling, n.d.). To further benchmark our results we applied our method to estimate semantic representations by gender rather than by party.5 Figure 1 also plots the accuracy scores by gender for each concept. Except for the concept “Republican”, our results suggest no signif icant gender di↵erences in representations for our set of cues. Overall finding is evidence that Democrats and Republicans do indeed strongly di↵er in their represen 
better results than many simpler models (including a sim ple bag-of-words). Since it provides a good description of semantic memory retrieval and has been shown to have nice statistical properties (Jun et al., 2015), we focus on INVITE for our analyses here. 
4Since our sample is stratified by party, chance is an ac curacy score of 0.5. 5Gender has also been previously identified as a potential source of di↵erences in semantic representations (Capitani, Laiacona, & Barbarotto, 1999). 
447
tations for these concepts. 


	

	

	

	

	

	

	

	

	

	

	

	

	

	

	

	

	

	

	

	

	

	

	

	



0.7 
y 
c
a
r
grouping gender 
Table 2: Subjects’ attitudes towards welfare as a func tion of partisanship of welfare concept representation and average valence of the subjects’ retrieved lists 
Dependent variable: 
Welfare Attitude 
(1) (2) (3) (4) 
Concept Partisanship 0.335⇤⇤⇤ 0.097⇤⇤⇤ 
(0.023) (0.028) 
u
cc
a
0.6 
0.5 
s 
e
u
l
a
v
_
n
a
c
i
r
e
m
a
t 
a
r
c
o
m
e
d
t n
e
m
n
r
e
v
o
g
n 
a
c
il
b
u
p
e
r
party 
e 
r
a
f
l
e
w
Average Valence 0.662⇤⇤⇤ 0.307⇤⇤⇤ (0.069) (0.058) 
Party (Republican = 1) 0.242 0.522⇤⇤ (0.283) (0.261) 
Ideology 0.487⇤⇤⇤ 0.451⇤⇤⇤ (0.063) (0.063) 
Constant 0.123⇤ 0.094 2.952⇤⇤⇤ 1.233⇤⇤⇤ (0.070) (0.152) (0.347) (0.319) 
Observations 575 573 591 589 R2 0.270 0.453 0.136 0.449 Adjusted R2 0.269 0.450 0.134 0.446 
Notes: Ideology ranges from -3 (extremely liberal) to 3 (extremely conservative). ply using the average valence of the retrieved lists to pre 
Figure 1: Accuracy of the model in discriminating between Democrat and Republican heldout subjects 
Individual Di↵erences and Attitudes 
To explore whether retrieved semantic associations are predictive of attitude judgments we also collected data on general attitudes toward the government’s role in pro viding services (related to the concept government) and its role in guaranteeing a minimum standard of living (re lated to the concept welfare). Both attitude questions were on a seven-point Likert scale and were recoded to range from -3 (extremely liberal position) to 3 (extremely conservative position).6 We hypothesized the di↵erence in the log-likelihoods of an individual’s category fluency data under the Republican (LLR) and Democrat (LLD) models, a quantity we term concept partisanship, should be predictive of that individual’s attitude judgments (us ing the representations for welfare for the question on welfare and government for the question on government services). The more negative (positive) the concept par tisanship for subject i for concept c, the better that sub ject’s fluency list approximates the Democrat’s (Repub lican’s) estimated representation. Table 2 reports our re sults of including concept partisanship as a regressor of expressed attitudes. Concept partisanship is significant even after controlling for party aliation and ideology suggesting our representations are capturing more than group aliation. 
According to constructive models of attitudes, when responding to a survey question on attitudes individ ual’s sample from memory, compute a statistic (e.g. an average) of the valences of the sampled information and respond accordingly. Building on this intuition we next asked how much predictive leverage can we get from sim 
6Both attitude questions were taken from the American National Elections Studies (ANES) Survey. 
dict attitude judgments. This requires we first attach a valence to the retrieved words which we do using a set of 13, 915 valence norms from Warriner, Kuperman, and Brysbaert (2013). These valence norms range from a low of 1 (“unhappy”) to a high of 9 (“happy”) and subjects are instructed to respond how a word makes them feel. We emphasize this is an imperfect measure of valence to the extent that the valence of a word may change as a function of context and party aliation yet it provides for an acceptable first approximation. Our results con firm that average valence of the retrieved lists is a sig nificant predictor of expressed attitudes consistent with constructive models of attitudes (Table 2). 
Previous studies using semantic fluency tasks have ob served that subjects produce items in bursts of seman tically related words (Troyer, Moscovitch, & Winocur, 1997), consistent with semantic memory being organized in clusters of semantically related concepts. Given we found average valence of retrieved lists to be predictive of attitudes, we wondered whether valence serves as an organizing principle of semantic memory alongside se mantic similarity (Osgood, Suci, & Tannenbaum, 1978; Westbury et al., 2015). One way of testing this hypothe sis is to first assess whether clusters are present in our es timated representations and, given clusters are present, whether nodes within clusters tend to align according to valence. To evaluate the presence of clusters in our estimated representations we applied the Walktrap algo rithm (Steinhaeuser & Chawla, 2010). Intuitively this algorithm identifies as clusters the densely connected re gions of a graph in which simulated random walks tend to get “trapped”.7 Figure 2 plots the estimated Democrat semantic representation for the concept Republican with di↵erent colors representing di↵erent clusters. Walktrap algorithm identifies three distinct clusters. We draw the 
7The algorithm works best with small step sizes. We limit our random walk to 3 steps. 
448
Table 3: Mean valence by cluster in Figure 2 yellow blue green 
mean 5.62 5.77 2.68 
valence (1.0250) (1.390) (0.567) 
reader’s attention to the highly negatively valenced green cluster vis-a-vis the other relatively more neutral clus ters. Using the same valence norms we used in the re gressions above, we can estimate mean valence by clus ter (see Table 3). The green cluster (consisting of the words “corrupt”, “greedy”, “ignorant”, “liar”, “racist”, “selfish” and “uncaring”) is significantly more negatively valenced than the other two clusters. We see this as sug gestive evidence of valence serving as an important orga nizing dimension of semantic memory, a result meriting further research. 
  

Figure 2: Democrat network for the concept ”Republican.” Clusters of concepts as estimated by the Walktrap algorithm are indicated by color 
Discussion 
We have outlined a method to explore di↵erences in se mantic representations between groups and applied it to a novel domain: politics. We hypothesized that indi viduals of opposite partisanship have di↵erent semantic representations for political relevant concepts. In our data, we find evidence of di↵erences across several polit ical concepts although the magnitude of the di↵erence is found to vary by concept, with concepts related to self identity (Democrat and Republican) showing the largest di↵erences. We also hypothesized that an individual’s semantic representation of a politically relevant concept is predictive of that individual’s attitudes toward topics related with that concept. Again, we find strong con 
firmatory evidence of this hypothesis. Finally we also found evidence consistent with valence playing an im portant role, alongside semantic similarity, in the orga nization of semantic memory. 
We began by arguing that partisan di↵erences in rep resentations are likely to have emerged as a result of di↵erences in the linguistic and emotional experiences of Democrats and Republicans. We now proceed to sketch out a more general theory of the relationship between semantic memory and attitudes. We hypoth esize, that there might be a computational reason for these di↵erences that further constrains how representa tions develop and change. The organization of seman tic memory is thought to be optimized for making e- cient and accurate knowledge-based inferences and pre dictions (e.g. top-down perception (Biederman, Kubovy, & Pomerantz, 1981) and linguistic prediction (Steyvers et al., 2006). This is consistent with the fact that se mantic memory has been found to be organized accord ing to similarity in sensorimotor experiential data and language-based distributional data (Andrews, Vigliocco, & Vinson, 2009). However, many studies have suggested valence as another important dimension of semantic or ganization (Osgood et al., 1978; Westbury et al., 2015), potentially resulting from co-occurrence statistics of af fective experience (Vigliocco et al., 2009).The fact that many of our most discriminating tokens are valenced and that similarly valenced nodes seem to cluster together is consistent this theory. This begs the questions: what use is valence as an organizing principle? We hypothesize that semantic memory is also optimized for ecient and consistent evaluative judgments under limited resources. If evaluative judgments do indeed follow a sampling like process then it makes sense for valence to play an or ganizing role lest individuals produce an endless stream of conflicting evaluations. We see this as a line research meriting greater attention and believe politics as a do main is ideally suited to this task. More generally we hope the method outlined above provides a basic frame work to begin to quantitatively explore the relationship between semantic memory and attitudes and that our promising results serve to highlight the potential returns to cognitive science of branching into less traditional do mains. 
Acknowledgments 
This research was generously supported by the George Downs Prize grants as well as an NSF grant BCS 1255538 and a John S. McDonnell Foundation Scholar Award to Todd M. Gureckis. 
References 
Andrews, M., Vigliocco, G., & Vinson, D. (2009). Integrat ing experiential and distributional data to learn semantic representations. Psychological review, 116 (3), 463. 
Austerweil, J. L., Abbott, J. T., & Griths, T. L. (2012). Human memory search as a random walk in a semantic 
449
network. In Advances in neural information processing sys tems (pp. 3041–3049). Beilock, S. L., Lyons, I. M., Mattarella-Micke, A., Nusbaum, H. C., & Small, S. L. (2008). Sports experience changes the neural processing of action language. Proceedings of 
the National Academy of Sciences, 105 (36), 13269–13273. Bhatia, S. (2017). The semantic representation of prejudice and stereotypes. Cognition, 164 , 46–60. 
Biederman, I., Kubovy, M., & Pomerantz, J. (1981). Percep tual organization. On the semantics of a glance at a scene, 213–263. Borodkin, K., Kenett, Y. N., Faust, M., & Mashal, N. (2016). When pumpkin is closer to onion than to squash: The structure of the second language lexicon. Cognition, 156 , 60–70. Bousfield, W. A., & Sedgewick, C. H. W. (1944). An anal ysis of sequences of restricted associative responses. The 
Journal of General Psychology, 30 (2), 149–165. Capitani, E., Laiacona, M., & Barbarotto, R. (1999). Gen der a↵ects word retrieval of certain categories in semantic fluency tasks. Cortex , 35 (2), 273–278. 
Carpenter, B., Gelman, A., Ho↵man, M., Lee, D., Goodrich, B., Betancourt, M., . . . others (2016). Stan: A probabilis tic programming language. Journal of Statistical Software, 20 (2), 1–37. 
De Deyne, S., Navarro, D. J., Perfors, A., & Storms, G. (2016). Structure at every scale: A semantic network ac count of the similarities between unrelated concepts. Jour nal of Experimental Psychology: General, 145 (9), 1228. 
De Deyne, S., Navarro, D. J., & Storms, G. (2013). Bet ter explanations of lexical and semantic cognition using networks derived from continued rather than single-word associations. Behavior research methods, 45 (2), 480–498. 
Gentzkow, M., Shapiro, J. M., & Taddy, M. (2016). Mea suring polarization in high-dimensional data: Method and application to congressional speech (Tech. Rep.). National Bureau of Economic Research. 
Go˜ni, J., Arrondo, G., Sepulcre, J., Martincorena, I., de Mendiz´abal, N. V., Corominas-Murtra, B., . . . Wall, D. P. (2011). The semantic organization of the animal category: evidence from semantic verbal fluency and network theory. Cognitive processing, 12 (2), 183–196. 
Griths, T. L., Steyvers, M., & Tenenbaum, J. B. (2007). Topics in semantic representation. Psychological review, 114 (2), 211. Halpern, D., & Rodriguez, P. (2018). Comparing mod els of semantic search in concrete and abstract categories. PsyArXiv. 
Ji, L.-J., Zhang, Z., & Nisbett, R. E. (2004). Is it culture or is it language? examination of language e↵ects in cross cultural research on categorization. Journal of personality and social psychology, 87 (1), 57. 
Judd, C. M., Drake, R. A., Downing, J. W., & Krosnick, J. A. (1991). Some dynamic properties of attitude struc tures: Context-induced response facilitation and polariza tion. Journal of Personality and Social Psychology, 60 (2), 193. 
Jun, K.-S., Zhu, X., Rogers, T. T., & Yang, Z. (2015). Hu man memory search as initial-visit emitting random walk. In Advances in neural information processing systems (pp. 1072–1080). 
Landauer, T. K., & Dumais, S. T. (1997). A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review, 104 (2), 211. 
Lenton, A. P., Sedikides, C., & Bruder, M. (2009). A la tent semantic analysis of gender stereotype-consistency and narrowness in american english. Sex Roles, 60 (3), 269– 278. 
Lord, C. G., & Lepper, M. R. (1999). Attitude representa tion theory. In Advances in experimental social psychology (Vol. 31, pp. 265–343). Elsevier. 
Lund, K., & Burgess, C. (1996). Producing high-dimensional semantic spaces from lexical co-occurrence. Behavior re 
search methods, instruments, & computers, 28 (2), 203– 208. 
Markman, E. M. (1994). Constraints on word meaning in early language acquisition. Lingua, 92 , 199–227. Medin, D. L., Ross, N. O., Atran, S., Cox, D., Coley, J., Prott, J. B., & Blok, S. (2006). Folkbiology of freshwater fish. Cognition, 99 (3), 237–273. 
Millsap, R. E., & Meredith, W. (1987). Structure in seman tic memory: A probabilistic approach using a continuous response task. Psychometrika, 52 (1), 19–41. 
Mitchell, A., Gottfried, J., Kiley, J., & Matsa, K. (2014). Political polarization & media habits. pew research cen ter. Retrieved 11/5/2014 from http://www. journalism. org/2014/10/21/political-polarization-media-habits. 
Morris, J. S. (2007). Slanted objectivity? perceived media bias, cable news exposure, and political attitudes. Social Science Quarterly, 88 (3), 707–728. 
Ochsner, K. N., Kosslyn, S., Yee, E., Chrysikou, E. G., & Thompson-Schill, S. L. (2013). Semantic memory. in the oxford handbook of cognitive neuroscience, volume 1: Core topics. Oxford University Press. 
Osgood, C. E., Suci, G. J., & Tannenbaum, P. H. (1978). The measurement of meaning. 1957. Urbana: University of Illinois Press. Peterson, A., & Spirling, A. (n.d.). Classification accuracy as a substantive quantity of interest: Measuring polarization in westminster systems. 
Ponari, M., Norbury, C. F., & Vigliocco, G. (2017). Ac quisition of abstract concepts is influenced by emotional valence. Developmental science. 
Steinhaeuser, K., & Chawla, N. V. (2010). Identifying and evaluating community structure in complex networks. Pat tern Recognition Letters, 31 (5), 413–421. Steyvers, M., Griths, T. L., & Dennis, S. (2006). Prob abilistic inference in human semantic memory. Trends in cognitive sciences, 10 (7), 327–334. 
Thompson-Schill, S. L., Kan, I. P., & Oliver, R. T. (2006). Functional neuroimaging of semantic memory. Handbook of functional neuroimaging of cognition, 2 , 149–190. 
Tourangeau, R. (1992). Context e↵ects on responses to atti tude questions: Attitudes as memory structures. In Con text e↵ects in social and psychological research (pp. 35–47). Springer. 
Tourangeau, R., & Rasinski, K. A. (1988). Cognitive pro cesses underlying context e↵ects in attitude measurement. Psychological bulletin, 103 (3), 299. 
Troyer, A. K., Moscovitch, M., & Winocur, G. (1997). Clus tering and switching as two components of verbal fluency: evidence from younger and older healthy adults. neuropsy chology, 11 (1), 138. 
Vigliocco, G., Meteyard, L., Andrews, M., & Kousta, S. (2009). Toward a theory of semantic representation. Lan guage and Cognition, 1 (2), 219–247. 
Warriner, A. B., Kuperman, V., & Brysbaert, M. (2013). Norms of valence, arousal, and dominance for 13,915 en glish lemmas. Behavior research methods, 45 (4), 1191– 1207. 
Westbury, C., Keith, J., Briesemeister, B. B., Hofmann, M. J., & Jacobs, A. M. (2015). Avoid violence, rioting, and outrage; approach celebration, delight, and strength: Using large text corpora to compute valence, arousal, and the basic emotions. The Quarterly Journal of Experimental Psychology, 68 (8), 1599–1622. 
Yee, E. (2017). Fluid semantics: Semantic knowledge is experience-based and dynamic. The Speech Processing Lexicon: Neurocognitive and Behavioural Approaches, 22 , 236. 
Zaller, J., & Feldman, S. (1992). A simple theory of the sur vey response: Answering questions versus revealing prefer ences. American journal of political science, 579–616. 
450
Human Decisions on Targeted and Non-Targeted Adversarial Samples 
Samuel M. Harding (hardinsm@indiana.edu) 
Prashanth Rajivan (prajivan@andrew.cmu.edu) 
Bennett I. Bertenthal (bbertent@indiana.edu) 
Cleotilde Gonzalez (coty@cmu.edu) 
Abstract 
In a world that relies increasingly on large amounts of data and on powerful Machine Learning (ML) models, the veracity of decisions made by these systems is essential. Adversarial samples are inputs that have been perturbed to mislead the in terpretation of the ML and are a dangerous vulnerability. Our research takes a first step into what can be an important innova tion in cognitive science: we analyzed human’s judgments and decisions when confronted with targeted (inputs constructed to make a ML model purposely misclassify an input as some thing else) and non-targeted (a noisy perturbed input that tries to trick the ML model) adversarial samples. Our findings sug gest that although ML models that produce non-targeted adver sarial samples can be more efficient than targeted samples they result in more incorrect human classifications than those of tar geted samples. In other words, non-targeted samples interfered more with human perception and categorization decisions than targeted samples. 
Keywords: Adversarial Machine Learning; Human Decision Making; Adversarial Samples. 
Introduction 
Machine Learning (ML) models are changing our world: they are part of search engines, recommendation systems, social media sites and new forms of social exchange. Autonomous cars use sensors to “see” the road and use ML models to make accurate decisions. These models learn discriminative features of road signs (e.g., a STOP sign) to select appro priate actions. Although very powerful, ML and particularly deep neural network (DNN) models are also severely vulner able to adversarial samples: inputs crafted with the intention of causing misclassification. The consequence is that slight alterations of the “transfer stimuli” (e.g., a STOP sign with some noise) can readily result in incorrect recognition. 
There are two broad approaches to developing adversarial stimuli that are capable of misleading ML and DNN models: targeted and non-targeted. In targeted attacks, minimal mod ifications are made to the input stimuli (e.g., images) such that they will be misclassified by the ML models as another specific target class (e.g., modify a STOP sign in such a way that the ML model in an autonomous vehicle interprets it as a YIELD sign instead). In non-targeted attacks, modifica tions are made to the input stimuli but there is no specific class intended; the goal is to make the model misclassify the perturbed input to any class/output, different from the actual class. 
Researchers focused on understanding Adversarial Ma chine Learning attempt to deal with the fundamental trade off of designing algorithms that are computationally effi cient while at the same time resist adversarial perturbations (Goodfellow, Shlens, & Szegedy, 2014; Huang, Joseph, Nel son, Rubinstein, & Tygar, 2011; Papernot et al., 2016). An 
important recent finding is that adversarial stimuli targeting one ML model can be successfully transferred to target a different model due to the shared adversarial stimuli space (Szegedy et al., 2013; Liu, Chen, Liu, & Song, 2016). Re cently, the argument has been made that such transfer is also possible between machines and humans (Elsayed et al., 2018). There is, however, limited evidence for this claim given that it is based on a single task with stimulus presenta tion times around 70ms. Thus, it is unclear whether these ef fects will generalize to other cognitive tasks and longer delib eration times. The question of whether humans can recognize a stimulus as adversarial is critical to security of automation systems as it may be possible to allow humans to intervene on predictions made by a compromised ML model in critical situations. Hence, it is critical to test the effect of different adversarial examples generated from different algorithms on human perception and decisions. 
This research extends the comparison of human perfor mance on adversarial samples to a wider range of cognitive tasks than has been studied previously. In Experiment 1, we test adversarial samples generated using JSMA (Jacobian based Saliency Map Attack), a targeted approach proposed by (Papernot et al., 2016); in Experiment 2 we test adver sarial samples generated using FGSM (Fast Gradient Sign Method), a non-targeted approach proposed by (Goodfellow et al., 2014). Each experiment involved human classifica tion, discrimination, and similarity decisions with targeted and non-targeted adversarial samples. In the Discussion, we compare and discuss the results from the two experiments and their implications for the transferability between machine and human systems. 
Machine Learning Models and Adversarial Samples of Handwritten Digits 
The FGSM and JSMA models models were developed and tested to attack a feedforward neural network model that was trained on the MNIST dataset containing images of handwrit ten digits (Yann, Corinna, & Christopher, 1998). These im ages are represented as vectors of 784 features (one for each of the 28x28 = 784 pixels), and each feature corresponding to a pixel intensity normalized to values between 0 and 1. The hidden layer neurons in the network each use logistic sigmoid function as their activation function. Let J(θ, x, y) represent the loss function used to train the neural network in both al gorithms where θ represents the neural network model, x rep resents the input and y represents the label/class for x. We will use these notations to describe the two algorithms. 
The Fast Gradient Sign Method (FGSM) used a simple 451
and efficient method for finding perturbations where, given a source image x, each of the 784 features representing the input is perturbed in the direction of the gradient by mag nitude of ε. ε represents the magnitude of the perturbation. The strength of perturbation at every feature is limited by the same constant parameter ε and the resultant is a adversarial stimuli ˜x of the original input x. With even small ε it is pos sible to mislead such Deep Neural Networks (DNN) with a high success rate. Due to the nature of gradient descent on the loss function, it is not possible for the model to anticipate the outcome and therefore, the goal is to misclassify adversar ial input ˜x as any other class than its correct class (y). Hence, it is a non-targeted form of attack. 
Papernot et al (2016) proposed the Jacobian-based Saliency Map Attack (JSMA) to generate adversarial sam ples to mislead neural network model. This model used an iterative approach to modify a limited and specific set of fea tures (among the 784 features) of the input image (x) for targeted misclassification. In this approach, an adversarial saliency map is calculated for the input image which con tains the scores for each pixel that reflect how the pixel can help in achieving the intended target class ( ˜y) while reducing the probability of achieving any other class. Pixels with high saliency scores are perturbed by ε repeatedly until the model misclassifies the input as the intended target class. Papernot et al. (2016) found that a deep neural network can be fooled with high success (97%) while only requiring small modifi cations (4.02%) of the input features of a sample; while hu mans identified 97.4% of the adversarial samples correctly and classified 95.3% of the adversarial samples correctly. 
Adversarial Image Generation We quantified the amount of perturbation introduced by each algorithm by computing the L1-norm, or pixel-wise (i, j) difference between the un perturbed and adversarial image, which more robust to out liers than a common alternative, the L2-norm, and directly represents the total change in luminance between the images: 
similar at all”) to 10 (“identical”) using a sliding bar. Each trial included a brief instruction reminder, the stimulus im age(s), and a response field. Trials were not time constrained, and responses were recorded when the participant indicated they were ready to move to the next trial by clicking a red arrow button. 
In the tasks requiring a comparison between two images (discrimination, similarity), there were three types of stim uli. In Source-Source pairs, an unperturbed MNIST image was paired against itself, which served as a control condi tion. The remaining comparisons paired images from dif ferent digit classes (0-9) with one another in two ways. In Source-Adversarial pairs, an unperturbed MNIST digit was paired with an adversarially modified version of itself. Fi nally, in Target-Adversarial pairs, an adversarial image was compared against an unmodified image from a different class. In the case of stimuli generated by JSMA, this was the class that was targeted by the algorithm; for FGSM stimuli, the al gorithm operates without targeting a specific output class, so the comparison image chosen was digit class which the DNN reported when classifying the adversarial image. Examples of the three stimulus pairs can be seen in Figure 2. 
In the classification task, only a single image was pre sented, and it was either an unperturbed MNIST digit (taken from Source-Source pairs), or an adversarial image (from Source-Adversarial and Target-Adversarial pairs). 
For each task type, participants completed 70 trials for a total of 210 trials. All participants finished the task within 15 and 30 minutes. 
Dx,x˜ = ∑ i=1 
∑ j=1 
|xi j −x˜i j| (1) 
General Method 
In two experiments, we tested the effect of adversarial im ages from two algorithms (JSMA: Experiment 1; FGSM: Ex periment 2) on human performance within classification, dis crimination, and similarity tasks. The general procedure is outlined below, followed by specific details about the partici pants and stimuli for each experiment individually. 
Procedure Participants were told they would view “images of numbers” and be asked to complete three perceptual tasks, which alternated from trial-to-trial (see Figure 1). In the clas sification task, participants freely reported the identity of a single digit; in the discrimination task, they responded by in dicating whether two images showed the “same” or “differ ent” digits by clicking a corresponding button; finally, in the similarity task, participants rated two images, from 0 (“not 
  

Figure 1: Example images demonstrating the three tasks per formed by the subjects in both experiments: the classification task (top row), the discrimination task (middle row) and the similarity task (bottom row) 
452
  

Figure 2: Examples of the image pairs shown in Experi ments 1 (left columns) and Experiment 2 (right columns). In ‘Source-Source’ pairs, an MNIST digit was compared with itself; ‘Source-Adversarial’ pairs pitted an unperturbed MNIST digit against an adversarially-modified version of it self; finally ‘Target-Adversarial’ pairs compared an adversar ial digit with an MNIST digit from the incorrect class pro duced by the DNN when classifying the adversarial image. 
Experiment 1 
In Experiment 1, we tested human classification, discrimina tion, and similarity judgments over images generated using the JSMA algorithm (targeted attack). 
Method 
Participants We recruited participants via Amazon’s Me chanical Turk, and collected data using Qualtrics (with IRB approval from Carnegie Mellon University). Participants (n = 300; 113 females; mean age = 34.25 years) first provided in formed consent and confirmed normal or corrected-to-normal vision. Monetary compensation was based on performance (base pay rate $4, average bonus: $2.71). 
Stimuli The image pairs used in Experiment 1 were pro duced by the JSMA algorithm, which were selected from a larger database of image pairs provided by the authors of Pa pernot et al.(2016). For each Source-Adversarial and Target Adversarial comparison, we selected images with the largest adversarial distance (see Equation 1). The average distance for stimuli generated by the JSMA algorithm, among those tested in this study was 54 pixels (min = 14; max = 100). 
Design Due to the large number of comparisons, we cre ated three non-overlapping stimulus sets and randomly as signed participants to one of three groups. Within each group, the same stimulus set was used in all three tasks (classi fication, discrimination, and similarity). All three stimu lus sets included Source-Source comparisons for every digit (0/0,1/1,...,9/9). The nine remaining, non-matching com parisons for each digit (e.g. 0/1,0/2,...9/8) were divided between the three participant groups. For example, Group 1 judged pairs of images comparing an unperturbed ‘0’ against 
adversarial images from categories ‘2’, ‘3’, and ‘9’, while Group 2 compared against ‘4’, ‘5’, and ‘9’, and Group 3 saw ‘1’, ‘6’, and ‘7’. Each of these non-matching pairs was tested twice, once as a Source-Adversarial pair and once as a Adversarial-Target pair. 
Experiment 2 
In order to contextualize the results of Experiment 1 within the larger adversarial domain, we measured human judgments on images generated using a different algorithm, “the fast gra dient sign method (FGSM)” proposed by Goodfellow et al. (2014). 
Method 
Participants We recruited a new sample of participants (n = 300; 135 female; mean age = 34.72 years) using the same process as Experiment 1. Average bonus pay was $2.71. 
Stimuli We chose images from FGSM with the largest ad versarial distance. The range of distances among tested stim uli was more limited than in Experiment 1, (mean = 296.1, min = 78.4, max = 313.6). Due to the non-targeted nature of the FGSM algorithm, there were few digit classes that, when perturbed never generated adversarial images that were misclassified as certain other digits. For example, adversarial modifications to images portraying the digit, “1”, were never misclassified as “0”, and the same was true for the pairs, 1/6,4/1,5/1,7/6. In order to prevent biases arising from participants noticing the absence of these comparisons, we substituted these missing pairs with least perturbed images from the JSMA algorithm, and removed responses to these stimuli from all analyses (a total of 5% of the total trials). 
Design We divided the 10×10 stimuli in the same manner as in Experiment 1, though the exact distribution of stimu lus pairs was randomized, such that e.g. Group 1 performed comparisons of digit, ‘1’ against ‘2’, ‘4’, and ‘6’. As before, each group was tested on self-comparisons for all digits and against three non-self comparisons. 
Results 
We first examined participants’ accuracy in the classification task. In Experiment 1, participants correctly reported the pre sented digit on 95.5% of classification trials. In Experiment 2, the average accuracy decreased to 90.2% (see Table 1). A generalized linear, mixed effects model predicting the num ber of errors 1 between unperturbed (Source) and adversar ial (Adversarial, Target) images, and across experiments, re vealed a significant main effect of Perturbation, F(1,1796) = 290.21, p <.001, as well as a significant main effect of Exper iment, F(1,1796) = 25.574, p < .001. These results are con sistent with the human performance data reported in Papernot et al. (2016), which showed that human classification of ad versarial stimuli remains near ceiling, and we generalize this 
1binomial model, link = logit; models fit using MATLAB func tion, fitglme, using the Laplacian fitting method 
453
finding to a novel adversarial algorithm. The difference in ac curacy when comparing across the two algorithms suggests that FGSM was more successful in confusing human judg ments, perhaps due to the larger amount of perturbation, or the more global pattern of pixel changes. 
Table 1: Classification Accuracy 


	Experiment 1 
	Experiment 2
	Unperturbed Adversarial 
	96.8% 
94.2% 
	97.8% 
82.7%
	Total 
	95.5% 
	90.2%
	



We next examined whether participants would correctly identify pairs of images showing the ‘same’ or ‘different’ digits, in spite of the adversarial modifications, in the Dis crimination task. Overall accuracy was at 99.1% in Experi ment 1, and 96.6% in Experiment 2 (see Table 2). A gener alized, linear mixed-effects model over Trial Type (Source Source, Source-Adversarial, Target-Adversarial) and Exper iment (Experiment 1, Experiment 2) showed a significant main effects of Trial Type, F(2,1794) = 71.937, p < .001. There was also a main effect of Experiment, F(1,1794) = 17.76, p < .001, and a significant 2-way interaction, F(2,1794) = 43.818, p < .001. These results were driven primarily by better performance for the adversarial com parisons (Source-Adversarial, Target-Adversarial) in Exper iment 1 than in Experiment 2, with no difference in Source Source trials. This is consistent with the pattern of results found in the classification task, which showed that perfor mance on images produced by the FGSM algorithm tended to be worse than over those generated by JSMA; furthermore, this is a novel demonstration that adversarial images can per turb human judgments in tasks other than Classification. 
Table 2: Discrimination Accuracy 


	Experiment 1 
	Experiment 2
	Source-Source 
Source-Adversarial Target-Adversarial 
	99.9% 
97.9% 
99.7% 
	99.9% 
95.0% 
94.8%
	Total 
	99.1% 
	96.6%
	



In the similarity task, we examined whether there were differences across the Experiments or image Type, using a linear mixed-effects model. Similarity ratings were signifi cantly different across Trial Types; F(2,1794) = 13,881, p < .001. This difference was mostly in the Source-Adversarial and Target-Adversarial comparisons (see Figure 4). There was not a significant main effect of Experiment, F(1,1794) = .712, p > .05, but the interaction between Trial Type and Experiment was significant, F(2,1794) = 46.627, p < .001. This latter effect was due to the reversal in the two adver sarial comparisons: while the ratings in Target-Adversarial 
pairs remained lower than the other comparisons, the addi tional noise introduced by FGSM seems to have made the adversarial image appear more similar to the intended target category than the procedure adopted by JSMA. 
One possible explanation for this finding is that the dis tance between adversarial and source images was larger for FGSM than JSMA, so we followed up by examining the impact of adversarial distance on similarity rating. Due to the limited range of distances of tested stimuli created us ing the FGSM algorithm, we focused the analysis on Source Adversarial pairs generated by JSMA. Adversarial Distance (see equation 1) for Source-Adversarial image pairs did not significantly predict human performance on the classification or discrimination tasks, (both F’s < 3.5, p’s > .05), but there was a significant negative relationship, β = −.021(.002), in the similarity task, F(1,88) = 86.382, p < .001 (see Figure 3). Participants rated images with more distortions as less simi lar than those with fewer. The JSMA algorithm was designed to find the minimal perturbations necessary to produce mis classifications by the deep neural network model (DNN), and thus remain relatively undetected by human observers. This finding is critical because it demonstrates that, while perfor mance on classification would appear to suggest that human observers fail to detect the adversarial changes, these explicit ratings of similarity reveals that, not only do observers notice the changes, their responses are tightly mapped to the amount of change introduced by the algorithm. This more sensitive measure likely provides a better means of evaluating the effi cacy of adversarial models in evading human detection. 
  

Figure 3: The amount of perturbation (Adversarial Distance) was significantly related to participants’ similarity ratings over Source-Adversarial image pairs in Experiment 1 
Finally, in order to assess whether performance on one task (e.g. similarity) could be used to predict performance in the other tasks, we correlated performance across the three tasks within each experiment. In Experiment 1, individual perfor mance in the classification and discrimination tasks was sig nificantly correlated, r(298) = 0.511, p < .001. Due to the 
454
stark differences in ratings as a function of trial type in the similarity task, we ran separate correlations for each stimulus type: Source-Adversarial similarity scores were significantly correlated with classification performance, r(298) = .152, p < .01, and marginally related to discrimination, r(298) = .112, p = .053. Target-Adversarial performance was likewise cor related between similarity, r(298) = -.129, p < .05, and dis crimination, r(298) = -.131, p < .05. Finally, Source-Source similarity judgments were only related to discrimination per formance, r(298) = .272, p < .001. 
In Experiment 2, individual performance in the classifi cation and discrimination tasks was significantly correlated, r(298) = 0.839, p < .001. Separate correlations by stimu lus type in the similarity task showed that Target-Adversarial judgments were significantly negatively correlated with clas sification performance, r(298) = -.328, p < .001, and re lated to discrimination, r(298) = -.471, p < .001. Source Adversarial performance was correlated between similarity, r(298) and discrimination, r(298) = .129, p < .05. 
Together, these results suggest that the different tasks rely on similar perceptual representations, and that individuals’ performance on one task could be used to predict their abil ities in the other domains. If, for example, a subject rates adversarial images as particularly dissimiliar to their unper turbed counterparts, they may be less prone to incorrectly classify the image, and therefore be less vulnerable to these types of perturbations, making the collection of explicit simi larity ratings an important tool for assessing the risk posed by adversarial images. 
  

Figure 4: Mean similarity ratings across Experiments 1 (blue) and 2 (red), separated by the image pair shown to subjects. 
General Discussion 
Current research on AML claims that humans are insensi tive to the perturbations introduced in adversarial samples; however, these claims are not based on evidence from em pirical research. This study represents the systematic attempt to test humans susceptibility to adversarial stimuli, and the results suggest that previous claims may have been over stated. Although adversarial stimuli are very effective in fool 
ing ML models with incorrect classifications hovering be tween 97% and 99.9% (Papernot et al., 2016; Goodfellow et al., 2014), human performance reveals much greater variation depending on task (classification, discrimination, similarity) and model (FGSM, JSMA). The key question emerging from these results is how to interpret this performance. 
Our main point is that lack of sensitivity to adversarial stimuli does not necessarily imply that humans are unable to detect these perturbations. Similarity judgments between stimuli revealed significant differences between unperturbed and perturbed images (source-adversarial, adversarial-target) and the magnitude of these differences was scaled to the cal culated distance between the stimuli. Likewise, participants were very good at discriminating the image of a digit from its adversarial target, even though the adversarial target was classified by humans as representing the same number as the unperturbed image. 
Presumably, machine learning models would also discrim inate between adversarial and unperturbed stimuli, but this is because they would classify the two stimuli as different numbers (i.e., source and adversarial target). By contrast, hu mans discriminate the stimuli not because they classify them differently, but because they detect featural differences cor responding to texture density or contrast or discontinuities in the contour, to name just a few candidates. It is often risky to draw parallels between ML models, such as DNNs and human information processing because we still know so little about how neural networks work. These adversarial examples simply demonstrate the fragility of these ML models. This is why drawing direct comparisons between human cogni tion and neural networks and anthropomorphizing them may be unfair (Gershman, Horvitz, & Tenenbaum, 2015; Chollet, 2017). 
It is noteworthy that we observed a significant difference between the two forms of attack (targeted vs non-targeted) in terms of their ability to produce human recognizable ad versarial images. We found that humans are less accurate in classifying adversarial images generated by FGSM, a non targeted form of attack, compared to human performance on the same task with images generated by JSMA, a targeted form of attack. In other words, non-targeted perturbation of pixel intensities interfered more with human perception and classification decisions. This performance difference was significantly reduced when participants made judgments on adversarial images during the discrimination task. As such, these results demonstrate that the more effective adversarial model results in poorer classification and discrimination by humans, which represents a disadvantage when trying to de tect adversarial stimuli. 
Of course, it is premature to generalize from these prelim inary findings that the FGSM algorithm is more effective in fooling machines than humans, because the conclusions de pend to a large extent on the specific information processing task administered to humans. Although our results revealed that performance in some of these tasks is correlated, the cor 
455
relations were generally very small accounting for no more than 25% of the variance and in most cases much less. We thus conclude that a complete testing of human performance with adversarial stimuli will require a broad range of tasks assessing different perceptual and cognitive skills. 
It should also be noted that these adversarial stimuli were generated with the primary goal of making ML and DNN models to misclassify and do not take into account the hu man in the loop (yet). Does integrating human feedback with ML solve the problem of adversarial perturbations? This is a question for future research. While humans may not be highly susceptible to these specific adversarial samples they may be susceptible to attacks that exploit gaps and limitations in human cognition. For example, we are easily fooled by op tical illusions and easily fooled by spear phishing emails. Re cent work by Elsayed and colleagues have produced a small set of adversarial examples that fool ML and humans alike (Elsayed et al., 2018). However, the limitation of their study is that they restricted exposure time to 70ms followed by a mask, which inhibits much of the higher-level processing that is typically available to humans. Moreover, this study did not explicitly test whether humans would correctly classify the stimuli when the decision space was much greater. Such at tacks that fool both ML and humans alike can have more se vere repercussions. Hence it is critical to study the effect of adversarial algorithms on both ML and humans. 
Much work still needs to be done in studying the interac tion between human and machine intelligence. Our current work is limited to simple, black and white images, in a do main where we all have significant knowledge of the stim uli (i.e., hand-written numbers). We know, however, that adversarial attacks are considerably more difficult to con duct in practice. Images are more naturalistic (color, shape, sizes), distance and movement change the visual view consid erably, and information may be presented in different modes (e.g.vision, voice). Furthermore, context information is avail able in practice. Although current AML research is only in its infancy, the speed at which this is advancing suggests that we need to try to keep pace with malicious applications of this technology in order to understand how to protect our systems from possible attacks. As we continue to progress toward the future, it is safe to assume that the ML models, for example, those used in autonomous cars, will become more sophisti cated and robust than the ones currently available to protect against adversaries. Thus, it is important to best understand the vulnerability of these algorithms as well as how humans can defend against them, because we have observed that even the most sophisticated algorithms can be fooled even with small perturbations. It is equally important to understand the extent to which humans can be fooled with adversarial sam ples before we advocate for supervised learning by humans (Veeramachaneni, Arnaldo, Korrapati, Bassias, & Li, 2016). 
Acknowledgments 
This research was funded by the Army Research Laboratory under Cooperative Agreement Number W911NF-13-2-0045 (ARL Cyber Security CRA). The views and conclusions con tained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government. The authors thank research assistant in the Dynamic Decision Making Laboratory, Carnegie Mellon Univerity, Nalyn Sriwattanakomen, for her help with prepa ration of Qualtrics experimental paradigm and data collec tion. We also thank Nicolas Papernot and Patrick McDaniel for providing the images and results from their algorithms. 
References 
Chollet, F. (2017). The limitations of deep learn ing. Retrieved from https://blog.keras.io/ the-limitations-of-deep-learning.html 
Elsayed, G. F., Shankar, S., Cheung, B., Papernot, N., Ku rakin, A., Goodfellow, I., & Sohl-Dickstein, J. (2018). Ad versarial examples that fool both human and computer vi sion. arXiv preprint arXiv:1802.08195. 
Gershman, S. J., Horvitz, E. J., & Tenenbaum, J. B. (2015). Computational rationality: A converging paradigm for intelligence in brains, minds, and ma chines. Science, 349(6245), 273–278. Retrieved from http://science.sciencemag.org/content/349/6245/273 
doi: 10.1126/science.aac6076 
Goodfellow, I. J., Shlens, J., & Szegedy, C. (2014, Decem ber). Explaining and Harnessing Adversarial Examples. ArXiv e-prints. 
Huang, L., Joseph, A. D., Nelson, B., Rubinstein, B. I., & Tygar, J. (2011). Adversarial machine learning. In Pro ceedings of the 4th acm workshop on security and artificial intelligence (pp. 43–58). 
Liu, Y., Chen, X., Liu, C., & Song, D. (2016). Delving into transferable adversarial examples and black-box attacks. arXiv preprint arXiv:1611.02770. 
Papernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik, Z. B., & Swami, A. (2016). The limitations of deep learn ing in adversarial settings. In Security and privacy (eu ros&p), 2016 ieee european symposium on (pp. 372–387). 
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., & Fergus, R. (2013). Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199. 
Veeramachaneni, K., Arnaldo, I., Korrapati, V., Bassias, C., & Li, K. (2016). Aiˆ 2: training a big data machine to defend. In Ieee conference on big data security (pp. 49–54). 
Yann, L., Corinna, C., & Christopher, J. (1998). The mnist database of handwritten digits. URL http://yhann. lecun. com/exdb/mnist. 
456
Object Recognition when Features Arrive Dynamically Samuel M. Harding (hardinsm@indiana.edu) 
Richard M. Shiffrin (shiffrin@indiana.edu) 
Department of Psychological and Brain Sciences; Cognitive Science Program Indiana University, Bloomington, IN, 47405 
Abstract 
We report a model for object identification based on an exper iment that varies the arrival times of different features of the objects. A single object, a circle with four spokes extending in different directions, is presented and must be classified as either one of four well trained target stimuli, or one of four well trained foil stimuli. The features (spokes) are presented either simultaneously or successively at intervals of 16, 33, or 50 ms., with target diagnostic features arriving first or last. All durations are short enough that the display appears simultane ous. The data show that individual decisions vary with both timing and diagnosticity. We apply a dynamic model based on one reported in (Cox & Shiffrin, 2017) for episodic recognition memory. Our model assumes features are perceived at vary ing times following presentation, possibly in error. At each moment the current features are compared to the well learned memory representations of the eight stimuli, producing a like lihood ratio for target vs foil. A decision is made when the log likelihood first exceeds a target decision boundary or falls be low a foil decision boundary. The model implements a form of Bayesian optimal decision making given the assumptions con cerning feature perception. It predicts the key findings quite well. 
Keywords: response time modeling; dynamic stimuli; visual search; object recognition; feature sampling 
Introduction 
The time course and outcome of recognition decisions are frequently used to inform our understanding of perceptual, memory, and preference judgments. Many successful mod els of classification and recognition treat the choice and its time as the outcome of a dynamic process that samples and integrates stimulus information over time. For example, the drift diffusion (Ratcliff & Rouder, 1998) and Linear Ballistic Accumulator (LBA) (Brown & Heathcote, 2008) models suc cessfully capture response patterns to static stimuli by con tinually sampling evidence at a constant rate, throughout the time that a single decision is being made. It is of consider able theoretical interest to explore the way these kinds of de cisions vary when stimuli change over the course of a single trial, but few studies have explored this domain. In this report we model data from a study that manipulates the timing and amount of evidence available within each decision trial. 
A wealth of existing models have explored how decision processes vary over time according to the influences of in ternal factors. For example primacy and recency effects in perceptual and preference judgments are thought to arise due to lateral inhibition between response options (Usher & Mc Clelland, 2001) or by applying differential weight to evidence arriving early versus late during deliberation (Busemeyer & Townsend, 1993). Shifting attention to consider differ ent aspects of response alternatives can lead to reversals in 
preference within the course of each trial (Diederich & Os wald, 2014). When responses are made under time pressure, choices made with only partial information may be reversed when given sufficient time to sample all relevant features (Lamberts, 1995; Cohen & Nosofsky, 2003). Such feature sampling may also account for response reversals in asso ciative recognition, as initially context-based retrieval is bol stered by item, and finally associative features over time (Cox & Criss, 2017). 
Less work, however, has focused on capturing how exter nal changes in stimuli affect the decision process. Lamberts and Freeman (1999) asked participants to categorize items comprised of several discrete features; on some trials, the en tire item was shown, while other trials presented single fea tures in isolation. They found that responses to individual fea tures could be used to predict those made to the entire object, suggesting that categorization decisions are made, in part, by integrating information across constituent parts. In another study, Holmes, Trueblood, and Heathcote (2016) changed the direction of coherent motion of a cloud of randomly moving dots, before a left/right motion discrimination response was made. Using a version of the LBA model with two distinct rates of processing the authors found evidence that partici pants noticed the change after a short delay and adjusted their decision process in light of the new information. Together, these studies highlight the novel insights provided by dynam ically modifying stimuli during the course of an ongoing de cision decision. 
We present here an alternative dynamic model for ob ject perception and categorization, based on a paradigm that changes the timing and order of the arrival of features of vary ing diagnosticity during a single decision. The data modeled are a subset of a larger experiment (other conditions are re ported in Cousineau and Shiffrin (2004) and in Cousineau, Donkin, and Dumesnil (2015)). The task is one of visual search for well practiced targets and foils that do not change roles over 58 sessions of training. Some conditions in the study presented objects sequentially and others presented fea tures of those objects sequentially. We apply our model to accuracy and reaction time data for three subjects who pro vided sufficient data in the feature sequential conditions that presented a single target or foil for a binary target-foil deci sion. 
The model is an extension of one recently proposed to ac count for recognition decisions via perceptual sampling of features during storage and retrieval (Cox & Shiffrin, 2017). The present model compares the relative evidence in favor of 
457
either response, at each moment, derived from matching the perceptually sampled features against well-learned represen tations of the eight objects stored in memory. The response given, and its time, are determined by the point in time at which the evidence passes one of two decision boundaries. 
Method 
The three participants carried out visual search for 58 sessions approximately an hour in length, over several weeks. Four stimuli were defined as ‘targets’, four others were defined as ’foils’ and the stimuli maintained these roles over the course of the experiment. Following the terminology of (Schneider & Shiffrin, 1977; Shiffrin & Schneider, 1977) such training is termed consistent mapping, or CM. Each trial in the con ditions we model presented a single stimulus to be classified as a ‘target’ or ‘foil’. The four features of this test object ap peared simultaneously, or sequentially with 16, 33, or 50 ms between each onset. This timing was chosen because even the slowest presentation rate was fast enough that the dis plays appeared simultaneous (albeit a bit ‘flickery’) making it unlikely that strategies would differ with presentation rate. Finally, the order of the sequentially presented features varied in diagnosticity, with the most diagnostic features presented either first or last (this is described in more detail below). 
Participants 
The three participants gave informed consent in accord with the Indiana University IRB. Monetary compensation was pro vided, and the participants were instructed to respond as quickly as possible without exceeding 5% error rate. 
  

Figure 1: (a) Stimuli used throughout the course of the ex periment were drawn from this set of eight objects, which were consistently mapped (CM) to either the “Target” or “Foil” category. (b) Each object was formed with four fea tures (spokes) extending radially from a central black circle. Target-diagnostic features are highlighted; these pairs of ob jects only ever appeared, together, on items in the “Target” category. 
Stimuli 
A set of 8 novel objects was employed throughout the ex periment. Stimuli were black circles with lines (spokes) ex 
tending outward from four out of eight potential locations per object (see Figure 1). There were four ‘target’ objects, and four ‘foil’ objects. No single feature distinguished targets from foils, but a particular two features could identify two of the targets uniquely, and a different set of two features could identify the other two targets uniquely. Generally, feature combinations differed in the degree to which they provided diagnostic evidence concerning a target versus foil decision. 
Procedure 
Displays alternated between 1, 2, and 4 objects on any given trial, but the interest here is in the single object test displays, which occurred on a random one third of the trials. At each of the three sequential presentation rates, the order in which features appeared varied by diagnosticity: for targets the two diagnostic features appeared either first or last; for foils, only one of the target-diagnostic features was shown, and it was either the first or the last to appear. Each trial began with the presentation of a fixation cursor in the center of the screen for 1000ms, which was followed by the appearance of a feature less circle for 500ms in the location of the subsequent test ob ject. The features were then added one-by-one and remained visible until the subject made a response. 
Results 
The results of the experiment are broken down by accuracy and median response time in Figures 2 and 3. Of primary interest are the patterns of results when using sequential pre sentation of features, when compared to the simultaneous pre sentation condition (the data shown as grey squares). 
Accuracy 
When a target object was shown, accuracy stayed near ceiling if early-arriving features were strongly diagnostic of targets (diagnostic-first); however, when the first two features were instead diagnostic of foils, subjects were less likely to iden tify the object as a target. This pattern was reversed when the presented object was a foil: if the first feature to appear was highly-diagnostic of a target, participants tended to report that it was a target, while a late-appearing target feature did little to decrease performance1. Use of a generalized linear, mixed-effects model showed a significant two-way interac tion that indicated that the number of correct responses was significantly related to the order of target-diagnostic feature presentation (first versus last) and the identity of the object (target versus foil), F(1,2148) = 138.600, p < .0012. 
The amount of delay between features played a role only when early features provided deleterious information, as the presence of early, useful information did little to improve the already near-ceiling performance. This was revealed via a 3- way interaction of including delay, object identity, and feature 
1Note that foil objects only included one target-diagnostic fea ture. 
2Binomial (link = logit) model, fit using MATLAB function, fit glme with Fixed Effects for Order (First, Last), ISI (16, 33, 50ms) and Object Identity (Target, Foil), and Random Effects for Subject. 
458
  

Figure 2: Mean accuracy data (shapes) and qualitative model fits (lines) for subjects (S1-S3) separated into ‘Hits’ (respond “Target” to targets; top row) ‘and Correct Rejections’ (respond “Foil” to foils; bottom row). Accuracy in the simultaneous condition is shown for reference (grey square), and each of the delay conditions is presented in the colored shapes (dark-to light: 16ms, 33ms, 50ms). Target-diagnostic features appeared were the first (triangles) or last (circles) to appear. 
order, F(2,2148) = 3.4716, p < .05. These results suggest that subjects began to accumulate evidence towards a decision be fore all of the features were presented, and that later-arriving features did not entirely mitigate this effect. 
Response Times 
Median response times were analyzed using a similar gener alized linear mixed-effects model. Overall, response times were slowed when the features arrived sequentially. This makes sense, given that the amount of information available to the decision process was limited by the number of avail able features; this was supported by a significant main effect of ISI, F(2,2148) = 42.015, p < .001. The order of feature appearance was also important: response times were faster when early-arriving features aligned with the eventual ob ject identity. For example, target-diagnostic features arriving first on a target object led to faster “target” responses than when these crucial features were the last to appear. Again, this pattern was reversed for foil objects. The 2-way interac tion between stimulus and order was significant, F(1,2148) = 36.982, p < .001. 
It is difficult to draw strong conclusions from the accu racy and response time data when analyzed separately, but 
these patterns suggest that the decision process operated con tinuously as features were sampled, and that early-arriving features tended to bias the eventual response. We therefore turn to a dynamic process model to account for both response times and accuracy as they vary with timing and order of ap pearance of diagnostic features. 
The Model 
The proposed model is based on one proposed to account for accuracy and response data in episodic recognition memory and reported in (Cox & Shiffrin, 2017). The model captures variability in response times and response proportions via a feature-sampling process that unfolds stochastically over time: as time passes following presentation of the test stim ulus, the subject extracts features, which are entered into a probe and used to search long-term memory. At each mo ment this comparison yields a relative likelihood that the test stimulus matches objects from ‘target’ and ‘’foil’ categories. A response is generated when the relative likelihood exceeds a “target” response criterion or drops below a “foil” response criterion. A key to the model’s predictions is the differential arrival times of different feature types into the probe. 
459
  

Figure 3: Median empirical (shapes) and predicted (lines) response times for correct responses made to Targets (top row) and Foils (bottom row). Responses were longer when the amount of delay between features was increased (timing conditions: 16, 33, or 50ms). When informative information appeared early (Targets, diagnostic-first; Foils, diagnostic-last), responses were faster than when this information was withheld until later in the trial. Error bars show the 25th and 75th quantile of the response time distributions. 
Feature Sampling 
The present model treats stimuli as comprised of discrete fea tures, assumed to be the eight spokes of the stimuli. The fea tures are not perceived immediately upon presentation, but are sampled probabilistically over time. Note that both the presence and absence of features provide information about the identity of the object: the presence of single diagnostic feature merely suggests a “Target” response. When paired with a second diagnostic feature, the two provide definitive evidence; the absence of this second feature conversely iden tifies the item as a foil. At each moment, the current belief about each feature is in one of three possible states: ‘Present’, ‘Absent’, or ‘Unknown’. Each of the 8 features begins in the ‘Unknown’ state, and beliefs are updated via the accumula tion of noisy perceptual evidence, modeled using a diffusion process (Ratcliff & Rouder, 1998), approximated via the ma trix method given in Diederich and Busemeyer (2003). This feature sampling process is notably different than that pro posed by Lamberts (1995), which assumes features are absent until sampled; our ternary framing is necessary when missing features actively contribute to the decision. 
The diffusion process operates by integrating perceptual information about the stimulus over time. When a feature is absent, the mean rate of accumulation is controlled by the mean drift rate, µAbsent; similarly, when a feature is added to the display, the rate of accumulation switches to a different rate, µPresent. Unlike the standard diffusion process, which terminates as soon as the evidence crosses a decision thresh old, we utilize non-absorbing boundaries, which allows the process to continue monitoring for changes in the visible fea tures. A delayed feature that is identified as “Absent” early in the trial can therefore be recognized as “Present,” with suffi cient information. The parameter that controls how much ev idence is needed to move out of the “Unknown” state is gov erned by a parameter θ, which is a proportion of the total dis tance from the starting point of the process (assumed to be 0) to either non-absorbing boundary: smaller values correspond to more conservative identification. Regardless of timing of presentation, different features are perceived independently; the joint probability of any combination of ‘Present/Absent’ judgments, φ(t), is the product of the individual probabilities. 
460
Memory Retrieval 
At each moment in time the currently beliefs about the fea tures, φ(t), are compared to the well-learned object repre sentations stored in long-term memory. The probability that the presented object is a “target” is given by dividing the number of matching traces from the target class by the total number of retrieved items from either class, each count be ing augmented by a small constant to add some noise to the comparison process. Pr(Target|φ) = nT+ε 
nT+nF+ε. This fram ing highlights the role of feature diagnosticity; if a pair of target-diagnostic features have been perceived and sampled into φ(t), only traces from the target class will be retrieved from long-term memory, the small amount of noise excepted and thus Pr(Target|φ) = nT 
nT = 1. 
Decision 
In this framing, evidence is grounded in relative, rather than absolute terms (Cox & Shiffrin, 2012), and can be tracked using a log-likelihood ratio. Two decision boundaries are es tablished such that decisions occur when the log-likelihood favoring one of the responses first crosses one of the bound aries. We generate a log-likelihood for each collection of fea tures, which expresses the relative evidence in favor of the current object being a target versus a foil: 
the boundary: Pr(r = R) = Φ(BR,µ(t),σ(t)). Because fea ture sampling is independent across time, the Gaussians rep resenting the distribution of evidence over time are also in dependent; the probability of first-passage time is thus given by the probability of having not yet crossed either boundary by time, t, times the immediate probability of crossing. The duration of the decision process is then added to a residual non-decision component, which is estimated separately for Target and Foil responses, allowing for e.g. greater response inhibition for Foils (NDTT , NDTF). 
Model Fitting 
There are three parameters associated with this perceptual feature sampling process: the two drift rates (µPresent, µAbsent), and the proportion of the decision space that corresponds to being in the ‘Unknown’ state (determined by θ). The deci sion process yielded an additional four parameters, the initial starting bias (b), the distance to the response boundaries (A), and the two non-decision components (NDTT ,NDTF). 
In spite of the relatively small number of parameters in the model, we were able to produce a high degree of match between the empirical data across three subjects providing somewhat heterogeneous data. Bayesian posterior estimates for the parameters were found using Differential Evolution 
βTarget(φ) = log
" 
Pr(Target|φ) Pr(Foil|φ) 
# 
(1) 
(Turner & Sederberg, 2012). We fit, jointly, the response pro portions and the complete response time distributions for cor rect responses, but only the response proportions for incorrect 
These log-likelihoods do not depend directly on time, only on the collection of features in short-term memory, φ, which changes as time passes. In order to find the distribution of evidence over time, βTarget(t) we compute a weighted sum of log-likelihoods from each collection, according to the cor responding probability of having each such collection at that time. We utilize the method presented in Cox and Shiffrin (2017), to approximate this distribution as Gaussian, with mean and variance given by3: 
µ(t) = log(β)Pr(φ,t) (2) 
σ(t) = log(β)TΣ(t)log(β) (3) 
Responses are generated when the log-odds sufficiently fa vor one result, which is instantiated by tracking the process until it crosses one of two response boundaries. The two boundaries, are estimated to lie at a distance (A/2) away from the starting point (b), and correspond to target and foil re sponses (BTarget,BFoil). Computing the probability of cross ing either boundary at time, t, is easily accomplished using the standard normal cumulative density function, evaluated at 
3The approximation utilizes the fact that the distribution over feature combinations, φ(t) is a multinomial distribution that must sum to 1. Multiplying by a weight vector (here, the βs for each state), projects the distribution onto a univariate subspace of famil iarity, which is approximately normal for large N. 
choices, as there were insufficient trials to estimate the shape of the response time distribution. Table 1 shows the priors for each parameter, as well as the posterior mode for each subject (across the columns). 
The model successfully captured the patterns of all three subjects, with the minor exception of predicting too-few Cor rect Rejections for Subject 3. The parameters associated with feature sampling show the expected result, namely that visible features provide more evidence for a feature being ‘Present’ than when they were not yet visible in the dis play (µPresent > µAbsent). For subject 1, missing features pro vided evidence against its being in the stimulus, as expected; however, for subjects 2 and 3, missing features provided weak, positive evidence, though this was offset by requiring more evidence before deciding whether a feature was present (smaller θ values). One possible interpretation is that this ‘head-start’ allows for more rapid detection of new features when added to the display, which was consistent with their overall faster responding. 
For all participants, the non-decision component associated with “Target” responses was lower than for “Foil” responses, which is justified within the context of the visual search task from which these conditions were drawn, in that rejecting a display and indicating that a target was “Absent” likely re quires greater evidence than finding a target and responding “Present.” 
461
Name 
	Prior Distribution 
	Posterior Mode
	µPresent 
µAbsent 
θ 
b 
A 
NDTT 
NDTF 
	∼ N(0,5) 
∼ N(0,5) 
∼ U(0,0.5) 
∼ U(0,0.5) 
∼ N(20,10) 
∼ U(0,300) 
∼ U(0,300) 
	1.96 3.22 3.76 -0.82 0.77 0.99 0.3 0.08 0.04 0.51 0.52 0.52 10.3 10.05 9.98 
123 3 40 187 44 58
	



Table 1: Prior distributions and posterior modes (columns corresponding to individual subjects) of model parameters. 
Discussion 
Aside from technical details the model we employ is concep tually simple and coherent: features are sampled and accumu late as time passes. At each moment the collection of current features is matched to the well learned set of eight stimuli. The matching process produces a likelihood that the current collection matches one of the targets versus one of the foils. When this likelihood exceeds a target boundary or falls below a foil boundary a corresponding response is made. 
It is clear from the data that a model like this is needed, because the timing and diagnosticity of features changes the pattern of results, for both accuracy and response time. We plan to pursue comparisons of our proposed model with ex isting approaches aimed at similar problems, such as EGCM VS (Guest et al., 2017) and EBRW-PE (Cohen & Nosofsky, 2003), as well as extending the model to the conditions in which objects but not features arrive sequentially. 
References 
Brown, S. D., & Heathcote, A. (2008). The simplest com plete model of choice response time: Linear ballistic ac cumulation. Cognitive Psychology, 57(3), 153–178. doi: 10.1016/j.cogpsych.2007.12.002 
Busemeyer, J. R., & Townsend, J. T. (1993). Decision field theory: A dynamic-cognitive approach to decision mak ing in an uncertain environment. Psychological Review, 100(3), 432–459. doi: 10.1037/0033-295X.100.3.432 
Cohen, A. L., & Nosofsky, R. M. (2003). An extension of the exemplar-based random-walk model to separable dimension stimuli. Journal of Mathematical Psychology, 47(2), 150–165. doi: 10.1016/S0022-2496(02)00031-7 
Cousineau, D., Donkin, C., & Dumesnil, E. (2015). Uniti zation of features following extended training in a visual search task. Cognitive Modeling in Perception and Mem ory: a Festschrift for Richard M. Shiffrin, 3–15. 
Cousineau, D., & Shiffrin, R. M. (2004). Termination of a visual search with large display size effects. Spatial Vision, 17(4-5), 327–352. 
doi: 10.1163/1568568041920104 
Cox, G. E., & Criss, A. H. (2017). Parallel interactive retrieval of item and associative information from event memory. Cognitive Psychology, 97, 31–61. 
doi: 10.1016/j.cogpsych.2017.05.004 
Cox, G. E., & Shiffrin, R. M. (2012). Criterion Setting and the Dynamics of Recognition Memory. Topics in Cognitive Science, 4(1), 135–150. 
doi: 10.1111/j.1756-8765.2011.01177.x 
Cox, G. E., & Shiffrin, R. M. (2017). A dynamic approach to recognition memory. Psychological Review, 124(6), 795– 860. doi: 10.1037/rev0000076 
Diederich, A., & Busemeyer, J. R. (2003). Simple matrix methods for analyzing diffusion models of choice prob ability, choice response time, and simple response time. Journal of Mathematical Psychology, 47(3), 304–322. doi: 10.1016/S0022-2496(03)00003-8 
Diederich, A., & Oswald, P. (2014). Sequential sampling model for multiattribute choice alternatives with random attention time and processing order. Frontiers in Human Neuroscience, 8(September), 1–13. 
doi: 10.3389/fnhum.2014.00697 
Guest, D., Kent, C., Adelman, J. S., Guest, D., Kent, C., & Adelman, J. S. (2017). Journal of Experimental Psychol ogy : Learning , Memory , and Cognition The Relative Im portance of Perceptual and Memory Absolute Identification Determining the Time Course of Absolute Identification. 
Holmes, W. R., Trueblood, J. S., & Heathcote, A. (2016). A new framework for modeling decisions about changing information: The Piecewise Linear Ballistic Accumulator model. Cognitive Psychology, 85, 1–29. 
doi: 10.1016/j.cogpsych.2015.11.002 
Lamberts, K. (1995). Categorization Under Time Pressure. Journal of Experimental Psychology: General, 124(2), 161–180. doi: 10.1037/0096-3445.124.2.161 
Lamberts, K., & Freeman, R. P. (1999). Building ob ject representations from parts: Tests of a stochastic sam pling model. Journal of Experimental Psychology: Hu man Perception and Performance, 25(4), 904–926. doi: 10.1037/0096-1523.25.4.904 
Ratcliff, R., & Rouder, J. N. (1998). Modeling Response Times for Two-Choice Decisions. Psychological Science, 9(5), 347–356. doi: 10.1111/1467-9280.00067 
Schneider, W., & Shiffrin, R. M. (1977). Controlled and auto matic human information processing: I. Detection, search, and attention. Psychological Review, 84(1), 1–66. doi: 10.1037/0033-295X.84.1.1 
Shiffrin, R. M., & Schneider, W. (1977). Controlled and automatic human information processing: II. Perceptual learning, automatic attending and a general theory. Psy chological Review, 84(2), 127–190. doi: 10.1037/0033- 295X.84.2.127 
Turner, B. M., & Sederberg, P. B. (2012). Approximate Bayesian computation with differential evolution. Journal of Mathematical Psychology, 56(5), 375–385. 
doi: 10.1016/j.jmp.2012.06.004 
Usher, M., & McClelland, J. L. (2001). The time course of perceptual choice: The leaky, competing accumulator model. Psychological Review, 108(3), 550–592. 
doi: 10.1037//0033-295X.108.3.550 
462
Emerging abstractions: Lexical conventions are shaped by communicative context 
Robert X.D. Hawkins 1, Michael Franke2, Kenny Smith3, Noah D. Goodman1 
1Department of Psychology, Stanford University ({rxdh,ngoodman}@stanford.edu) 
2Department of Linguistics, University of Tubingen (mchfranke@gmail.com) ¨ 
3Centre for Language Evolution, University of Edinburgh (Kenny.Smith@ed.ac.uk) 
Abstract 
Words exist for referring at many levels of specificity: from the broadest (thing) to the most specific (Fido). What drives the emergence of these taxonomies of reference? Recent com putational theories of language evolution suggest that commu nicative demands of the environment may play a deciding role. Here, we investigate local pragmatic mechanisms of lexical adaptation that may undergird global emergence by manipulat ing context in a repeated reference game where pairs of partic ipants interactively coordinate on an artificial communication system. We hypothesize that pairs should converge on specific names (e.g. Fido) when the context requires frequently mak ing fine distinctions between entities; conversely, they should converge on a more compressed system of conventions for ab stract categories (e.g. dog) in coarser contexts, even if a finer mapping would be sufficient. We show differences in the lev els of abstraction that emerged in different environments and introduce a statistical approach to probe the dynamics of emer gence. 
Keywords: conventions; pragmatics; communication; interac tion 
Introduction 
Natural languages provide speakers with remarkable flexibil ity in the labels they may use to refer to things (Brown, 1958). On top of an abundance of expressions made available by syntactic combination and semantic compositionality (Partee, 1995), we have a number of overlapping and nested terms in our lexicon. Fido, Dalmatian, dog, and animal can all reason ably be used to talk about the same entity at different levels of abstraction. How these overlapping meanings are learned, and why speakers choose different levels of specificity in different contexts, is increasingly well-understood (e.g. Xu & Tenenbaum, 2007; Graf, Degen, Hawkins, & Goodman, 2016) but there remains a more fundamental question about the structure of our lexicon: why and how do different levels of abstraction become lexicalized in the first place? 
One functional answer is suggested by recent computa tional approaches to language evolution, which have argued that the lexical conventions of languages balance simplicity, or learnability, with the communicative needs of their users. This optimal expressivity hypothesis accounts well for the lexical distributions found in natural languages across seman tic domains like color words and kinship categories (Regier, Kemp, & Kay, 2015; Gibson et al., 2017), as well as the com positional systems that emerge under iterated learning with communication in the lab (Winters, Kirby, & Smith, 2014; Kirby, Tamariz, Cornish, & Smith, 2015). A key prediction is that the lexicon of a group should be sensitive to the prag matic demands of their environment. For example, languages in warm regions ought to be more likely to collapse the dis tinction between ice and snow into a single word, simply be 
cause there are fewer occasions that require distinguishing be tween the two (Regier, Carstensen, & Kemp, 2016). Still, there are several limitations to the current evidence for this hypothesis. First, much of the relevant evidence is ob servational, aggregated at the level of overall language statis tics, not by directly manipulating the contextual conditions of individual language users. Second, previous experimen tal studies have largely focused on the functional outcomes of an iterated learning process, but have not grounded the re sults of this process in a cognitive, mechanistic account of lexical adaptation and convention-formation among individ ual agents. Finally, the phenomenon of reference taxonomies poses a further theoretical challenge: why do languages have hierarchies of terms instead of flatly partitioning the space into category labels as previous work has assumed? While globally shared conventions of a language are shaped over the multi-generational timescales of cultural evo lution, contextual pressures operate on the shorter timescales of dyadic interaction. In a matter of minutes, communica tion partners coordinate on efficient but informative local con ventions, or conceptual pacts, for the task at hand (Clark & Wilkes-Gibbs, 1986; Hawkins, Frank, & Goodman, 2017). To understand how languages are globally shaped by com municative constraints, it may therefore be valuable to under stand the local conventions rapidly formed by adaptive agents over extended interactions. 
Under the logic of a local efficiency/informativity trade off, we make two predictions about the emergence of abstrac tions in dyads. First, we expect that communicative pressures for informativity should lead to the lexicalization of specific names when fine distinctions must be drawn. Second, ab stractions should become lexicalized precisely when the rel evant distinctions are at coarser levels of the conceptual hi erarchy. For example, we are often called upon to make fine distinctions between people in our social circles, hence lex icalizing efficient names for each individual; when referring to green beans or paper towels, however, we can get away without such specific terms – we are rarely called upon to disambiguate between entities. 
Here, we develop an experimental paradigm and analytic approach to examine the causal factors driving the emergence of lexical conventions in real-time. We manipulated context in a repeated reference game where pairs of participants in teractively coordinated on an artificial language from scratch. Even though a complete communication system containing a distinct word for each object is feasible and sufficient for all contexts, we find that abstractions begin to emerge when fine-grained distinctions are not necessary. 
463
  

Figure 1: (A) Example of fine context where one of the distractors belongs to the same fine-grained branch of the hierarchy as the target (i.e. another striped circle), so any abstract label would be insufficient to disambiguate them. The target is highlighted for the speaker with a black square. (B) Drag-and-drop chat box interface. (C) Hierarchical organization of stimuli. 
Experiment: Repeated reference game Participants We recruited 278 participants from Amazon Mechanical Turk to play an interactive, multi-player game us ing the framework described in Hawkins (2015). Pairs were randomly assigned to one of three different conditions, yield ing between n = 36 and n = 53 dyads per condition, after excluding participants who disconnected before completion.1 
Procedure & Stimuli Participants were paired over the web and placed in a shared environment containing an ar ray of objects (Fig. 1A) and a ‘chatbox’ to send messages from a randomly generated vocabulary (Fig. 1B). On each of 96 trials, one player (the ‘speaker’) was privately shown a highlighted target object and allowed to send a single word to communicate the identity of this object to their partner (the ‘listener’), who subsequently made a selection from the array. Players were given full feedback, swapped roles each trial, and both received bonus payment for each correct response. 
The objects that served as referents were designed to clus ter in a fixed three-level hierarchy with shape at the top most level, color/texture at the intermediate levels, and fre quency/intensity at the finest levels (see Fig. 1C). Each com municative context contained four objects. Distractors could differ from the target at various level of the hierarchy, creat ing different types of contexts defined by the finest distinction that had to be drawn. We focus on two: fine trials, where the closest distractor belongs to the same fine-grained subordi nate category (e.g. another striped circle; see Fig. 1A), and coarse trials, where the closest distractor belongs to a coarser level of the conceptual hierarchy (e.g. dotted circle instead of striped circle).2 Fixed arrays of 16 utterances (enough to al 
1All materials and data are available at https://github.com/ hawkrobe/conventionalizing hierarchies; planned sample sizes, exclusion criteria, and behavioral analysis plan were pre registered at https://osf.io/2hkjc/. 
2Even coarser trials with super-ordinate distractors (e.g. a circle target among three square distractors) were logically possible but would have introduced several experimental confounds; we opted to leave these trial types out of our design and conduct the minimal 
low the potential for full expressibility) were randomly gener ated for each pair (and held constant across trials) by stringing together consonant-vowel pairs into pronounceable 2-syllable words (see Fig. 1B). 
Critically, we manipulated the statistics of the context in a between-subjects design to test the effect of communica tive relevance on lexicalization. In the pure fine and coarse conditions, all targets appeared in fine or coarse contexts, respectively; in the mixed condition, the two context types were equally likelySequences of trials were constructed by randomly shuffling targets and trial types within blocks and ensuring no target appeared more than once in a row. 
In addition to behavioral responses collected over the course of the game, we designed a post-test to explicitly probe players’ final lexica. For all sixteen words, we asked players to select all objects that a word can refer to (if any), and for each object, we asked players to select all words that can re fer to it (if any). Using a bidirectional measure allows us to check the internal validity of the lexica reported. 
Results 
Partners successfully learn to communicate Although participants in all conditions began with no common basis for label meanings, performing near chance on the first trial (pro portion correct = 0.19, 95% CI = [0.13,0.27]), most pairs were nonetheless able to coordinate on a successful com munication system over repeated interaction (see Fig. 2). A mixed-effects logistic regression on listener responses with trial number as a fixed effect, and including by-pair random slopes and intercepts, showed a significant improvement in accuracy overall, z = 14.4, p < 0.001. Accuracy also differed significantly across conditions (Fig. 2): adding an additional main effect of condition to our logistic model provided a sig nificantly better fit, χ2(2) = 10.8, p = 0.004. Qualitatively, the coarse condition was easiest for participants, the fine con dition was hardest, and the mixed condition was roughly in 
manipulation. 
464
t c
e
r
r
o
c
 
n
o
it
r
o
p
o
r
p
1.00 0.75 0.50 0.25 0.00 
coarse 
mixed 
fine 
0 25 50 75 100 trial number 
Contextual pressures shape the lexicon We predicted that in contexts regularly requiring speakers to make fine distinc tions among objects at subordinate levels of the hierarchy, we would find lexicalization of specific terms for each object (indeed, a one-to-one mapping may be the most obvious solu tion in a task with only 8 objects). Conversely, when no such distinctions were required, we expected participants to adap tively lexicalize more abstract terms. One coarse signature of this prediction lies in the efficiency of the resulting lexicon: lexicalizing abstract terms should require participants to use fewer terms overall. 
To test this prediction, we counted the number of words in each participant’s reported lexicon (i.e. the words for which 
Figure 2: Players learn to coordinate on a successful commu nication system. Each point is the mean proportion of correct responses by listeners; curves are nonparametric fits. 
between. Finally, the (log) response time taken by the speaker to choose an utterance also decreased significantly over the course of the game, t = −19.7, p < 0.001, indicating that lex ical mappings became increasingly established or accessible. Partners converge on similar lexica Another indicator of successful learning is convergence or alignment of lexica across partners in a dyad. Before using post-test responses to compute similarity across partners, however, we exam ine the internal consistency within an individual’s post-test responses. For each participant, we counted the number of mismatches between the two directions of the lexicon ques tion (e.g. if they clicked the word ‘mawa’ when we showed them one of the blue squares, but failed to click that same blue square when we showed ‘mawa’). In general, partici pants were quite consistent: out of 128 cells in the lexicon matrix (16 words × 8 objects), the median number of mis matches was 2 (98% agreement), though the distribution has a long tail (mean = 7.3). We therefore conservatively take a participant’s final lexicon to be the intersection of their word to-object and object-to-word responses. 
Using these estimates of each participant’s lexicon, we compute the overlap between partners. For most pairs, part ners aligned strongly by the end, with a median post-test over lap of 97.6% (125 out of 128 entries). Because these matri ces were extremely sparse, however, just a a few mismatches could have a large impact on performance. Overall accuracy in the game is strongly correlated with alignment: partners who reported more similar lexica at the end tended to per form better at the task (r = 0.77). 
Despite these markers of success at the group level, in dividual performance was bimodal: a subpopulation of 29 games (11% of coarse games, 18% of mixed, and 39% of fine) still showed relatively poor performance, sometimes at chance, by the end of the game. For the subsequent analyses focusing on the content of the lexicon, we exclude games with fourth-quartile accuracy below the pre-registered criterion of 75% to ensure we are examining only successful lexica. 
they marked at least one object in the post-test). We found that participants in the coarse condition reported significantly smaller, more efficient lexica (m = 4.9 words) than partici pants in the mixed and fine conditions (m = 7.4,t = 10.3, p < 0.001 and m = 7.6,t = 9.5, p < 0.001, respectively; see Fig. 3A). At the same time, the smaller lexicon provided equiva lent coverage of objects: the median number of objects where participants agreed on the same word or words was 7, 6.5, and 7, respectively. 
If participants in the coarse condition can get away with fewer words in their lexicon, what are the meanings of the words they do have? We counted the numbers of ‘specific’ terms (e.g. words that refer to only one object) and ‘abstract’ terms (e.g. words that refer to two objects) in the post-test. We found that the likelihood of lexicalizing abstractions differed systematically across conditions (see Fig. 3A). Participants in the fine condition reported lexica containing exclusively spe cific terms, while participants in the coarse condition reported significantly more abstract terms (m = 2.5, p < 0.001). 
These data also reveal an interesting asymmetry in lexi con content across conditions: while abstractions are entirely absent from the fine condition, participants in the other con ditions often reported a mixture of terms (see Fig. 3B). In the coarse condition, for instance, participants could in principle perform optimally with only four abstract terms and no spe cific terms. While this was the modal system that emerged (reported in the post-test by nearly 1/3 of participants), the average proportion of abstract (vs. specific) terms within each participant’s lexicon in the coarse condition (m = 0.56) was significantly higher than in the other conditions (p < 0.001, exploratory). 
Model-based Analysis 
Our post-test provides some insight into the end-result of lex icalization under different communicative contexts, but un derstanding the dynamics of lexicalization requires a more detailed analysis of behavioral trajectories. How do lexica shift and develop over the course of interaction? 
In this section, we present a statistical model of this pro gression. We assume that on any given trial, speakers and listeners are rationally producing and interpreting utterances given some internal lexicon, and we use a Bayesian statis 
465
A B coarse mixed fine 
16 
word meanings not used 
6 
h 
ti
w
 
s
dr
o
w
 
#
g 
n
in
a
e
m
 
t
s
e
t
-
t
s
o
p
12 
8 
4 
0 
coarse mixed fine condition 
count one object 
g 
s10 20 30 40 
n
i
t
r
c
r
4 
e
e
j
f
b
e
r
o
 
 
s
o
d
2 
two objects 
w
r
t
 
o
o
w
t
  
#
0 
0 2 4 6 8 10 0 2 4 6 8 10 0 2 4 6 8 10 
# words in lexicon referring to single object 
Figure 3: Pragmatic demands of context shape the formation of abstractions. (A) Mean number of words participants reported with specific meanings (applying to 1 object) or abstract meanings (applying to 2 objects). (B) Diversity of terms within reported lexica: many participants in the coarse condition reported a mixture of abstract and specific terms. 
tical model to infer their lexicon from their behavior. First, this analysis validates our post-test measures of lexical mean ing against actual behavioral usage — if participant reports are internally consistent, the model’s posterior near the end of the game should predict their post-test responses. Second, we can examine the time-course of lexical emergence by in specting lexica inferred from early behavior in the game. 
Generative model 
We begin with a generative model of how agents use their underlying lexicon to produce and interpret language. This model provides a linking function assigning a likelihood to the speaker utterances and listener choices we observe on each trial, given any latent lexicon. We adopt the probabilis tic Rational Speech Act (RSA) framework, which has been successful in recent years at capturing a broad array of prag matic phenomena in language use (Goodman & Frank, 2016; Franke & Jager, 2016). This framework captures the Gricean ¨ assumption of cooperativity: a pragmatic speaker S1 attempts to be informative in context while a pragmatic listener L1 in verts their model of the speaker to infer the intended target. The chain of recursive social reasoning grounds out in a lit eral listener L0, which directly soft-maximizes its lexicon, Lt(w,o), to interpret a given utterance. This model can be formally specified as follows: 
L0(oi|w,Lt) ∝ exp{Lt(w,oi)} 
S1(w|oi,Lt) ∝ exp{lnL0(oi|w,Lt)} 
L1(oi|w,Lt) ∝ S1(w|oi,Lt)P(oi) 
where oiis a chosen object and w an uttered word. We use these pragmatic speaker and listener likelihood functions to link latent lexica, represented as a matrix of real values `tw,o ∈ R, to behavior. This allows us to then use Bayesian inference to back out each participant’s effective lexicon from their trial-by-trial behavior. Because each trial has only a single choice for each player, we pool statistics within k epochs of the data (we choose k = 6 such that each target appears exactly twice in each epoch). For each epoch, 
we sample lexical entries from independent Gaussian priors: `ko,w ∼ N (0,5) 
This prior is intended to regularize lexicon entries to be rela tively close to 0, inducing a bias toward sparsity. 
We approximate the posterior of this model separately for each pair using mean-field variational inference, imple mented in the probabilistic programming language WebPPL (Goodman & Stuhlmuller, electronic; Ritchie, Horsfall, & ¨ Goodman, 2016). The approximating family for each ran dom variable is Gaussian. We approximate the joint posterior over all lexical entries used in each epoch by each participant. 
Validating post-test responses 
We begin by showing that the lexical entries we infer for each participant accurately predict their post-test responses. We constructed a logistic classifier from our posterior on each epoch: for each object-word pair (o,w) in the post-test re sponse matrix, we computed the marginal posterior proba bility P(`o,w > 0.5|θo,w), where θo,w are the corresponding variational parameters (i.e. the mean and variance of the ap proximating Gaussian). This gives the posterior probability that word w applies to object o. We evaluated the perfor mance of this classifier by constructing an ROC curve that shows the tradeoff between hits and false alarms as the dis crimination criterion is varied. We found that the classifier based on the final epoch predicts post-test responses with ex cellent accuracy (AUC: 0.98; see Fig. 4A). This indicates that the post-test lexicon is indeed linked to behavior as predicted by RSA, validating both the post-test measure and the results of our Bayesian analysis. 
Furthermore, we found that the corresponding posterior predictives from earlier epochs predicted final post-test re sponses less well, even though they were learned from the same number and type of behavioral observations (Fig. 4A). Still, even the classifier based on the earliest epoch performs above chance, indicating that some information about the fi nal lexicon is available from the earliest trials. These patterns are suggestive of a path-dependent process where the lexicon gradually coalesces from initially arbitrary associations over 
466
A B 01234 
1.0 
0.9 
epoch 
1 2 3 4 5 6 
m
i x
ed 
C 
y 
p 
o 
r
t
h c
o
p
0.05 
coarse 
C 
UA
0.8 
0.7 
0.6 
0.5 
1 2 3 4 5 6 
epoch 
y 
t
is
n
e
d
01234 
abstract 
e
n
 
c
e
s
o
 
u
a
n
i
o
r
 
i
s
.
f
v
e 
f
i
e
r 
d
01234 
 
p
 
n
specific 
a
m
f
e
i
o
n
r
e 
f
m
 
0 1 2 3 
entropy of word 
0.00 −0.05 −0.10 
mixed 
fine 
1-2 2-3 3-4 4-5 5-6 epoch 
Figure 4: Model-based results. (A) A logistic classifier based on inferred lexical entries accurately predicts post-test responses. (B) Entropy of posterior word extensions show coalescence across epochs for each condition. (C) Mean change in entropy at the word level from trial to trial (error bars are ±1 SE) 
the course of interaction. We next turn to the earliest stages of this process. 
Examining early time course 
One advantage of the statistical approach we develop here is the ability to make descriptive inferences about the meanings being used in settings where we don’t ask participants for ex plicit judgements—in particular, in early trials of our games. 
Our primary measure of interest is the entropy of the ex tension of words over the eight objects. The entropy of a particular word is near zero when its meaning is peaked on a single object, and is maximized when could apply equally to all objects (e.g. for a novel word that has not yet been used). We expect abstract terms to lie in between these extremes. We obtain the extension distribution for each word by running it through our L0 model, essentially asking how likely it is to refer to each of the eight objects.3 We use the MAP estimate of the lexicon. The resulting distribution of estimated word entropies, aggregated for each epoch and condition, is shown in Fig. 4B. Abstract terms begin to form early (epoch 2) in the coarse condition, and remain stable throughout the game. In contrast, specific terms are relatively slow-forming (epoch 4-5) in the other two conditions. The peak near an entropy of 3 reflects the inferred ambiguity of words that were not used or used randomly. 
Because these distributions are aggregated across words, however, they leave open the possibility that lexica are not stabilizing or coalescing but simply cycling through different words each epoch. We address these dynamics more thor oughly at the word level by computing the difference in each word’s entropy from epoch to epoch (Fig. 4C). For all condi tions, we found that the entropy of individual words changed less over later epochs (i.e. the difference scores approached 
3Using L0, rather than L1 or L, gives us a notion of word exten sion that is close to the underlying lexicon while influenced by non identifiability of parameters. For instance, L has an overall scaling per row that doesn’t influence behavior. 
zero), indicating that meanings gradually stabilized. There are also differences across conditions: words in the mixed and fine condition began with high entropy reduction (becom ing more specific) which continued through the final epochs, while words in the coarse condition actually seemed to in crease in entropy across the game on average. 
These preliminary results, then, may reflect a combination of narrowing and broadening depending on condition. Un known words can initially refer to any of the objects and only acquire more informative meanings as agents learn through interaction. Yet in the coarse condition where agents are quick to adopt meanings, the rest of the game may be spent paring down the lexicon instead. 
Discussion 
How and why do abstractions emerge in local interactions? We hypothesized that although communicative contexts re quiring fine distinctions would favor one-to-one object-word mappings, pressures for efficiency would allow abstractions to emerge in coarser contexts. By manipulating context statis tics in a real-time experiment, we found evidence for these pragmatic influences on interactive convention formation. 
Our results may help to illuminate the relationship between our concepts and words, which are often treated interchange ably. While our mental taxonomies are adaptive to the natural perceptual structure of the world (Mervis & Rosch, 1981) it is far from inevitable that all levels of these conceptual hier archies become conventionalized as lexical items. There are many perfectly natural concepts that are not represented by distinct words in the English language: for instance, we do not have words for each tree in our yards, or for ad-hoc con cepts (Barsalou, 1983). Indeed, English speakers are often fascinated by foreign words like the Danish “hygge” (a spe cific notion of coziness) or Scottish “tartle” (hesitating when introducing someone because you’ve forgotten their name) that are difficult to express in English. Our results highlight communicative needs to distinguish, in context, as a force be 
467
hind the choice to lexicalize some fine-grained concepts. A related direction for future work is to explore the relationship between communicative need and basic-level structure. 
While we showed how abstract words emerge from effi ciency even in a task requiring only reference to individual objects, there are other clear functional advantages to having abstract terms in the lexicon. For one, they allow speakers to efficiently refer to large, potentially infinite, sets of things, and make generalizations about categories, e.g. “Dogs bark” (Tessler & Goodman, 2016). Future work should explore this as an additional pressure toward abstract, nested nouns. Sim ilarly, the option to refer to more specific concepts with com pound terms (e.g. “spotted dog”), which was not available in our experiment, may impact final conventions. We expect that labels will become lexicalized when the cost incurred by fre quently using a compositional construction exceeds the cost of adding an additional word to the lexicon. Future work should also explore these hypotheses about how lexicaliza tion of nominal terms trades off with compositionality. 
Finally, although we implemented a purely statistical Bayesian data analysis model to infer lexica, it is also possi ble to consider a cognitive model of participants’ own lexical inferences. Indeed, our findings are consistent with a recent cognitive model of convention formation which explained the rapid coordination on efficient but informative lexical terms as a process of mutual lexical learning (Hawkins et al., 2017). In this model, each agent assumes their partner is rationally producing cooperative utterances under some latent lexicon; given initial uncertainty over the contents of that lexicon, agents can invert their model of their partner to infer their lexicon from observable behavior. The different dynamics we observed across conditions, then, may be the consequence of different lexical inferences in different local contexts. Fur ther, while we used RSA as a linking function in our statisti cal model, a cognitive model would allow us to test to what extent pragmatic reasoning is necessary to explain behavior. 
Our shared lexical conventions are richly structured sys tems with meanings at multiple levels of abstraction. There is now abundant evidence that languages adapt to the needs of their users, and the context-sensitive emergence of abstrac tions demonstrated in this paper suggests that the driver of this adaptation may lie in the remarkably rapid adaptability of agents themselves. We are constantly supplementing our existing language with local conventions as we need them. Our separate minds may organize the world into meaningful conceptual hierarchies but our shared language only evolves to reflect this structure when it is communicatively relevant. 
Acknowledgments 
This work was supported by ONR grant N000141310341 and a Sloan Foundation fellowship to NDG. RXDH was supported by the Stanford Graduate Fellowship and the National Science Foundation Graduate Research Fellowship under Grant No. DGE-114747. This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and in novation programme (grant agreement 681942, held by KS). 
References 
Barsalou, L. W. (1983). Ad hoc categories. Memory & cog nition, 11(3), 211–227. 
Brown, R. (1958). How shall a thing be called? Psychologi cal review, 65(1), 14. 
Clark, H. H., & Wilkes-Gibbs, D. (1986). Referring as a collaborative process. Cognition, 22(1), 1–39. 
Franke, M., & Jager, G. (2016). Probabilistic pragmatics, ¨ or why bayes’ rule is probably important for pragmatics. Zeitschrift fur Sprachwissenschaft ¨ , 35(1), 3–44. 
Gibson, E., Futrell, R., Jara-Ettinger, J., Mahowald, K., Bergen, L., Ratnasingam, S., . . . Conway, B. R. (2017). Color naming across languages reflects color use. Proceed ings of the National Academy of Sciences, 114(40), 10785- 10790. 
Goodman, N. D., & Frank, M. C. (2016). Pragmatic lan guage interpretation as probabilistic inference. Trends in Cognitive Sciences, 20(11), 818 - 829. 
Goodman, N. D., & Stuhlmuller, A. (electronic). ¨ The de sign and implementation of probabilistic programming lan guages. 
Graf, C., Degen, J., Hawkins, R. X. D., & Goodman, N. D. (2016). Animal, dog, or dalmatian? level of abstraction in nominal referring expressions. In Proceedings of the 38th annual conference of the Cognitive Science Society. 
Hawkins, R. X. D. (2015). Conducting real-time multi player experiments on the web. Behavior Research Meth ods, 47(4), 966-976. 
Hawkins, R. X. D., Frank, M. C., & Goodman, N. D. (2017). Convention-formation in iterated reference games. In Pro ceedings of the 39th annual meeting of the cognitive sci ence society. 
Kirby, S., Tamariz, M., Cornish, H., & Smith, K. (2015). Compression and communication in the cultural evolution of linguistic structure. Cognition, 141, 87–102. 
Mervis, C. B., & Rosch, E. (1981). Categorization of natural objects. Annual Review of Psychology, 32(1), 89-115. Partee, B. (1995). Lexical semantics and compositionality. In An invitation to cognitive science, part i: Language. Cam bridge, MA: MIT Press. 
Regier, T., Carstensen, A., & Kemp, C. (2016). Languages support efficient communication about the environment: Words for snow revisited. PloS one, 11(4), e0151138. 
Regier, T., Kemp, C., & Kay, P. (2015). Word meanings across languages support efficient communication. The handbook of language emergence, 237–263. 
Ritchie, D., Horsfall, P., & Goodman, N. D. (2016). Deep amortized inference for probabilistic programs. arXiv:1610.05735. 
Tessler, M. H., & Goodman, N. D. (2016). A pragmatic the ory of generic language. arXiv preprint arXiv:1608.02926. Winters, J., Kirby, S., & Smith, K. (2014). Languages adapt 
to their contextual niche. Language and Cognition, 1–35. Xu, F., & Tenenbaum, J. B. (2007). Word learning as bayesian inference. Psychological review, 114(2), 245. 
468
Towards a Pedagogical Conversational Agent for Collaborative Learning: A Model Based on Gaze Recurrence and Information Overlap 
Yugo Hayashi12 (y-hayashi@acm.org) 
1College of Comprehensive Psychology, Ritsumeikan University 
2-150 Iwakura-cho, Ibaraki, Osaka, 567-8570, Japan 
2Human-Computer Interaction Institute, Carnegie Mellon University 
5000 Forbes Avenue, Pittsburgh, PA,15213, U.S.A. 
Abstract 
This study focuses on collaborative learning involving a knowledge integration activity, whereby learner dyads explain each other’s expert knowledge. It was hypothesized that learn ing gain can be determined by the degree to which learn ers synchronize their gaze (gaze recurrence) and use overlap ping language (information overlap) during their interaction. Thirty-four learners participated in a laboratory-based eye tracking experiment, wherein learners’ gazes and oral dialogs were analyzed. Multiple regression analysis was conducted, wherein learning performance was regressed on the two inde pendent variables. Then, a simulation was conducted to view how the model predicts performance based on the collabora tive process. The results showed that both gaze recurrence and lexical overlap significantly predicted learning performance in the current task. Furthermore, the suggested model success fully predicted learning performance in the simulation. These results indicate that the two variables might be useful for de veloping detection modules that enable a better understanding of learner-learner collaborative learning. 
Keywords: Collaborative Learning; Pedagogical Conversa tional Agent; Information Overlap; Gaze Recurrence 
Introduction 
Inspired by the social-constructive approach by Vygotsky (1980), numerous studies in the fields of cognitive science and learning science have investigated the mechanisms of how learners gain knowledge through social interactions with collaborative partners. In those studies, “knowledge integra tion” through constructive interactions, especially with part ners who have different knowledge, provides opportunities for individuals to explain to others while reflectively consid ering their own perspectives (Aronson & Patnoe, 1997). Stud ies have pointed out that such learning helps in externalizing knowledge (Shirouzu, Miyake, & Masukawa, 2002; Miyake, 1986), facilitates meta-cognition during explanations (Chi, Leeuw, Chiu, & Lavancher, 1994) and perspective change (Hayashi, 2018). 
Based on this prior literature, I focus on collaborative learning during a simple knowledge intergeneration task, wherein learners’ task is to explain certain content to another learner with a different perspective on the topic. Particularly, I wanted to experimentally investigate the cognitive process of this interaction and develop a predictive model of learn ing performance. The ultimate goal is to apply such a model to develop automated learning support systems that can de tect learners’ activity and facilitate better peer collaborative learning. 
469
Designing a Pedagogical Conversational Agent(PCA) for learner-learner collaboration 
In the past several decades, numerous studies have been con ducted with the aim of developing learning support systems in the field of intelligent tutoring systems (Koedinger, An derson, Hadley, & Mark, 1997; Leelawong & Biswas, 2008). These investigations have illuminated how tutoring systems can be used to facilitate cognitive processes such as self explanations (Aleven & Koedinger, 2002) and self-regulated learning (Graesser & McNamara, 2010). Some studies have examined how efficiently cognitive tutors can facilitate learn ing (Koedinger & Aleven, 2007) and investigations involv ing practical use of these systems have determine the types of knowledge that these systems best support (Koedinger, Booth, & Klahr, 2013). 
Recently, an application called a Pedagogical Conversa tional Agent (PCA) has been introduced to tutoring systems, the result of rapid advances in artificial intelligence and net work technology (Heidig & Clarebout, 2011). According to research on the development of such systems, learners can directly interact with virtual and autonomous tutors though a computer screen, providing learning prompts and meta cognitive suggestions much like human tutors. PCAs have been demonstrated to be effective in the area of collaborative problem solving, such as for prompting achievement goals (Holmes, 2007) and providing periodic initiation opportuni ties (Kumar & Rose, 2011). A number of studies investigat ing the influence of PCA functional design have been con ducted as well, focusing mainly on knowledge explanation tasks (Hayashi, 2012, 2014, 2016a, in press). 
Despite the emerging body of literature on the design and development of PCAs for collaborative learning, a number of problems in this area remain unresolved, particularly the issue of how to detect learners’ cognitive state and provide adequate feedback based on it. Investigations based on the methods used in cognitive science, such as modeling the cog nitive states and interactions of collaborative learners, might be of use for designing efficient intelligent tutoring systems. Accordingly, the current study modeled the interactive pro cess of learners during collaborative learning, with the aim of using the results to predict learners’ cognitive states and incorporating this into PCA for automatic facilitation. 
Capturing cognitive interaction using gaze recurrence and information overlap 
Collaborative learning requires a communicative process such as establishment of a common ground (Clark & Wilkes Gibbs, 1986), which plays an important role during expla nation activities (Miyake, 1986). Taking this into account, I focus on two particular cognitive processes related to com munication: learners’ gaze behavior and language use. 
Gaze Recurrence During conversations between speakers, it is important that the conversing parties refer to the same spatial referents (Schober, 1993) and use the same syntactic structures (Branigan, Pickering, Pearson, McLean, & Brown, 2011); it is also often important for them to physically syn chronize in terms of gestures (Condon & Ogston, 1971) and simultaneously refer to the same referents (Richardson & Dale, 2005). Previous studies, such as (Richardson & Dale, 2005), have suggested that the degree of gaze recurrence be tween dyads (i.e., speaker-listener) is correlated with collab orative performance such as understanding and establishing common ground. They investigated how pairs engage in con versation through looking at a shared picture presented on a computer monitor. The researchers discovered that dur ing these conversations, both speaker and listener physically gazed at the same area on the monitor. Moreover, using a technique called “cross recurrence analysis,” they found that the gaze patterns of the dyad members synchronized over time. They also showed that the degree of synchronization correlates with comprehension tests such as memory retrieval and understanding. Studies on computer-supported collabo rative learning (CSCL) involving gaze recurrence and tech niques such as real-time mutual gaze perception have shown that gaze synchronization helps produce better collaborative learning (Schneider & Pea, 2014). The present study there fore focuses on gaze recurrence as one of the predictors of learners’ performance because it aids learners in establishing common ground and making more efficient explanations dur ing a collaborative task. 
Information Overlap in Conversation Sociology research has conceived several basic principles for successful collabo ration essentially, speakers should act cooperatively and mu tually accept one another to be understood (Grice, 1975). In conversation, speakers implicitly adopt the same language as others in order to establish common ground, a process called lexical entrainment. This helps reduce ambiguity and ensure maximum clarity of reference between speaker and listener. The precise ways in which speakers agree on how a referent is conceptualized are called “conceptual packs” (Brennan & Clark, 1996). Taken together, these past studies have shown that to develop successful understanding through interaction, it is important for speakers to share the same knowledge. In fact, research has shown that speakers select common words when they believe that the others to whom they are speaking share their knowledge. Branigan et al. (2011) used a sim ple referential naming game, and found that speakers tend 
470
to try to align with each other even when interacting with a computer. However, another study investigating information overlap between speakers revealed that the more information people share, they less fluently they may communicate (Wu & Keysar, 2007). In any case, to successfully understand each other, speakers must exchange information on what they know and use that same knowledge to communicate. There fore, in this study, when speakers refer to the same knowledge during their explanations, I considered them to be taking the other’s perspective and trying to effectively coordinate with each other. 
Goal and Hypothesis 
The goal of this study was to understand the collaborative learning process of learner-learner dyads in order to automat ically detect how successfully learners interact in a knowl edge integration task by utilizing a PCA. To this end, I will focus on two indicators considered important for understand ing communication in cognitive science: gaze recurrence and information overlap. First, I hypothesized that the learning process during a knowledge explanation task can be captured by the degree to which learners’ attend to the same physi cal content (gaze recurrence) and their rate of overlapping knowledge (information overlap) during conversation. Sec ond, I hypothesized that the efficiency of knowledge explana tion (i.e., learning performance) can be predicted by a model of interaction that includes these two indices. To test these hypotheses, I conducted an experiment involving a learner learner collaborative explanation task and analyzed the col lected gaze and verbal data. 
Method 
Participants and conditions 
Thirty-four (female: 15, male: 19, Mage: 20.79, SD: 1.84) Japanese university students majoring in psychology partic ipated in this study for course credit. This study was con ducted only after ethical review and approval from the ethical review committee of the author’s university. 
Procedure 
Upon participants’ arrival to the experiment room, the exper imenter thanked them for their participation and introduced them to their partner. The experimenter gave instructions on the task, which they were told was a scientific explanation task using technical concepts to explain human mental pro cesses. Before the main task, they began with a free recall test about the concepts to check if they did not know any of the concepts that would be referred to in the task. Subse quently, they completed the main explanation task in about 10 minutes. After the main task, they completed another free recall test. Upon completing the entire experiment, they were debriefed. 
Task 
For the task, the dyad had to explain a topic in cognitive science (e.g., human information processing on language 
perception) using two technical concepts (e.g., “top-down processing” or “bottom-up processing”). As in the jigsaw method, which is often studied in the field of learning sci ence and is a popular method of knowledge building in class rooms, I set up a situation wherein the learners did not know each other’s concepts. In other words, the experimenter pro vided only one of the concepts to each learner. Thus, to be able to sufficiently explain the topic using the two concepts, they would have to exchange knowledge by explaining the concepts to each other. 
First, the learners had to explain each concept given to them to their partner. Information on this concept was pro vided to each learner before the task began. On starting the task, they were asked to first read the description and then explain what it meant to their partner. Learners were free to ask questions and discuss the concept with their partners. When one learner had finished explaining the concept, they switched roles and the other learner explained the given con cept. The dyads were also instructed before the task that they would have to explain each other’s concept so that they could explain the topic using both technical concepts at the end of the task. 
Experimental System 
A redeveloped version of the system designed in a previous study was used(Hayashi, 2012, 2014, 2016a). Learners sat in front of a computer display and communicated with each other orally. The experimental system was developed in the Java language and worked on an in-house server-client net work platform. The two learners’ computers were connected through a local area network, and task execution was con trolled by a program on the server. Our version of the system also featured a PCA that provided meta-cognitive suggestions to facilitate their explanations. 
During the task, the learners were not able to see each other and were instructed to look at the computer display while conversing with their partner. A brief explanation on the learner’s assigned concept was presented on each screen; their partner’s concept was covered so that they could not simply read and proceed individually. Accordingly, I ex pected that while one partner explained their concept by look ing at the related screen area (Learner B), the listener (Learner A) would look at the same area. Two eye-trackers (Tobii X2- 30) were used to collect gaze data. Furthermore, a micro phone was placed next to each learner to record his/her voice through a two-channel mixer. All these audio data were tran scribed manually. 
An embodied PCA was presented in the center of the screen, which physically moved when it spoke. Below the PCA, there was a text box that showed messages. The ex perimenter sat aside in the experiment room and manually signaled the PCA to provide meta-cognitive suggestions. The timing of the suggestions was based on the following criteria: (1) whenever there was a gap in conversation, and (2) only once per minute. Five types of meta-cognitive suggestions are used, such as reminding learners to achieve the task goal 
471
(Azevedo & Cromley, 2004) and facilitating metacognition (Hayashi, 2014). 
Masseurs 
Performance The results of the pre- and post-task free re call test, wherein participants explained the topic using the two concepts, were used as the main performance data in this study. I coded the collected data according to how well the learners were able to explain their own concept as a result of successful coordination. The coding system was as follows: 0 = wrong, 1 = naive but correct explanation, 2 = concrete explanation based on presented materials, and 3 = concrete explanation based on the presented materials and using exam ples and metacognitive interpretations. While I also analyzed the partner’s explanation and the integrated explanation of the two concepts, these results will be mentioned in a later paper due to the length restrictions of this one. 
(1) Gaze recurrence: Gaze recurrent analysis Gaze syn chronization is important for successful communication. In the current task, the more the learners looked at the same area on their screen, the more attention they were considered to pay to each other (joint-attention). This can thus be inter preted as an index of perspective taking during the collabora tion process. 
Using the gaze data, I investigated the degree that each learner looked at the same area on the screen. The area was categorized according to the areas of interest, where the (1) area 1 = left frame box(self/other concept), (2) area 2 = right frame box(self/other concept), (3) area 3 = middle frame box(PCA), and (4) other. I labeled the fixation coordinates from each participant’s gaze log files from the eye tracking system corresponding to these areas. 
Next, based on Richardson and Dale (2005), learners’ la belled data were analyzed using recurrence analysis, to cap ture the proportion of fixations at the same location for both learners in a typical time state. The analysis was conducted using R. The recurrence of phi observed between the two time-series (Learners A and B) is calculated for a specific time k. The phi(k) coefficient increases with the frequency of matching recurrence at the same time (k; k) and decreases with the frequency of mismatching. Based on the procedure used by Richardson and Dale (2005), I adopted a time lag of 3 s. For each pair, I calculated the recurrence during each time lag, used the maximum value as the representative index for each learner and for further statistical analysis. 
(2) Information overlap: Adopting an epistemic net work analysis In the experimental task, learners must ex plain their own concept and try to understand their partner’s through conversation. Therefore, the more they use the same types of words during conversation, the more likely the learn ers are to be taking their partner’s perspectives to facilitate un derstanding. To analyze this, I used epistemic network anal ysis, a method of understanding the relations between coded data by representing them in a dynamic network. I have pre 
viously used this method to analyze the frequent use of im portant words during dyadic conversations (Hayashi, 2016b). The advantage of using epistemic network analysis in con versational analysis is that it can standardize the complexity of word types and enables comparison of word relations be tween speakers. For comparison, I calculated the similarity of word networks among learners and investigated to what extent they used the same words during the task. 
(a) Development of a dictionary database The first stage of the analysis involved developing a dictionary database and collecting frequent words used during learners’ conver sations. I conducted a morphological analysis using the Rme cab package of R, extracting all nouns used more than twice. There were a total of 13,565 morphemes. Non-technical words (e.g., “I”, “you”) were deleted from the list manually. 
Next, I developed a dictionary database of the extracted keywords for each dyad (Learner A and B). The average number of keywords for each pair was 34.70. An ex ample dictionary was “processing,” “perception,” “knowl edge,” “experience,” “instinct,” “top-down,” “bottom-up,” “language,” “friends,” “relationship,” “information,” “expla nation,” “problem,” “objective,” “important,” “occasionally,” “elements,” “current,” “cognition,” “input,” “event.” Subse quently, I generated a bipartite graph for each learner based on dictionary database of that pair, and calculated the degree of knowledge (i.e., word) overlap between the learners in the network. 
(b) Network analysis For each learner, I developed a net work based on the bipartite graph (i.e., n key words for X each learner’s n utterances). Each node represents the lexical category of a keyword frequently used in each participant’s explanation. The target of this analysis is to capture this de gree of overlap and understand to what extent each dyad used the same knowledge during the task. 
(c) Calculating the knowledge overlap In the next phase, I calculated the degree of knowledge overlap between the two networks. For this, I used the matching rate k, where l indi cates the number of nodes used in both networks (Learner A and Learner B) and n stands for all the nodes in the network. 
k =ln(1) 
The closer the value of k is to 1, the greater the proportion of the same words used by learners during their conversation. This calculation was conducted for all pairs and used as an index of information overlap in the statistical analysis. 
Results 
Regression analysis of human data 
Figure 2 shows the results of the Pearson’s correlation anal ysis between each learner’s performance (horizontal-axis) and the two main variables phi and k (vertical-axis). There 
472
was a significant correlation between performance and gaze recurrence phi (r=.385, p=.012) and a marginally signifi cant correlation with information overlap k (r=.235, p=.090). Next, I conducted a multiple regression analysis where learn ing performance (dependent variable) was regressed on phi and k (independent variables). The regression coefficient R2 was .439 and the F-value from an analysis of variance (ANOVA) was 3.709, indicating significance for both vari ables (p=.036). Thus, the two independent variables predicted learning performance(y). The regression equation is shown below: 
Figure 1: Relation between learning performance and each independent variable and phi and k. 
y = −0.071+ (2.235 ∗ phii) + (5.395 ∗ ki) (2) Using this model, I next wanted to see if it was possible to predict the learning performance of human learners via a simulation. 
Predicting learning performance using suggested model 
Next, using phi and k as parameters, I ran a simulation to see if the model can predict learning performance. This was conducted to see how accurately the system would detect the learning performance if the suggested model was imple mented in the current experiment. Figure 2 shows the results of this simulation. 
Discussion 
To investigate the first hypothesis, learning process was cap tured through analysis of gaze recurrence and information overlap (phi and k). I found that learning performance was significantly predicted by using these two variables, thus sup porting our first hypothesis. These findings suggest that the predictive regression model could function as a model of the learning process. However, the correlation between learning performance and k has rather weak (r=.235, p=.090). It there fore might be necessary to use a better lexical network anal ysis method, such as natural language processing techniques (e.g., correspondence analysis). 
A weakness of this study relates to the collaborative learn ing setting, particularly, learners’ lack of gaze awareness of 
Figure 2: Predicting learning performance using the sug gested model. 
their learning partners during the task. As pointed out in (Tomasello, 1995), social partners must demonstrate aware ness of what their partners are attending to, suggesting the need for an indicator of partners’ gaze. Schneider and Pea (2014) investigated the influence of gaze on learning perfor mance by using a colored dot to represent the partner’s gaze. I have conducted a similar experiment, the initial results of which indicate that awareness of gaze might change interac tive behaviors. The use of other sensor technologies could be applied to generate greater awareness of partners; however, this goes beyond the topic of this paper, so I would like to leave it for future studies. 
The results of this study also reveal that gaze recurrence and information overlap can be used for developing a PCA that can automatically detect the learning process. The next step towards developing such a PCA would be dealing with some issues on real-time automation. Nevertheless, the PCA would require gaze analysis modules and voice recognition technology coupled with lexical network analysis, which is necessary to detect information overlap. Recently, tech nologies such as facial recognition have been used in intel ligent tutoring systems to detect learners’ emotional states (D’Mello, Olney, Williams, & Hays, 2012), and these could be used to detect the synchronized behavior of the learners. Combining these methods might enable the development of an efficient PCA for collaborative learning in the future. 
Conclusions 
Towards developing a PCA for collaborative learning, this study quantitatively captured the collaborative learning pro cess of dyads. One difficulty in developing PCAs is to au tomatically detect to what extent learners are successfully interacting during a collaborative learning task. This study examined whether gaze recurrence and information overlap can capture the efficiency of coordination and thereby predict learning gains (in terms of their performance on an explana tion task). The results showed that gaze recurrence and in formation overlap did indeed capture learning performance. Moreover, a simulation conducted using index data from ac tual learners as input information was able to reproduce the 
473
actual learner’s performance. I discussed how these methods can be used for developing PCAs that can detect learners’ cognitive interactive behavior and provide adequate facilita tion prompts based on an evaluation of the system. This study might contribute to research on collaborative learning in cog nitive science and has methodological implications on the de sign of PCAs for collaborative learning. 
Acknowledgments 
This work was supported by the Grant-in-Aid for Scientific Research (KAKENHI), No. 16K00219. I would like to thank Professor Kenneth R. Koedinger (Carnegie Mellon Univer sity) for his comments on this study. 
References 
Aleven, V. A., & Koedinger, K. R. (2002). An effective metacognitive strategy: learning by doing and explaining with a computer-based cognitive tutor. Cognitive Science, 26(2), 147–179. 
Aronson, E., & Patnoe, S. (1997). The jigsaw classroom(2nd ed.): Building cooperation in the classroom. New York: Addison Wesley Longman. 
Azevedo, R., & Cromley, J. (2004). Does training on selfreg ulated learning facilitate students’ learning with hyperme dia? Journal of Educational Psychology, 96(3), 523-535. 
Branigan, H. P., Pickering, M. J., Pearson, J., McLean, J. F., & Brown, A. (2011). The role of beliefs in lexical alignment: Evidence from dialogs with humans and computers. Cognition, 121(1), 41 - 57. doi: https://doi.org/10.1016/j.cognition.2011.05.011 
Brennan, S. E., & Clark, H. H. (1996). Conceptual pacts and lexical choice in conversation. Journal of Experimental Psychology: Learning, Memory, and Cognition, 22, 1482- 1493. 
Chi, M., Leeuw, N., Chiu, M., & Lavancher, C. (1994). Elic iting self-explanations improves understanding. Cognitive Science, 18(3), 439-477. 
Clark, H. H., & Wilkes-Gibbs, D. (1986). Referring as a collaborative process. Cognition, 22(1), 1-39. 
Condon, W., & Ogston, W. (1971). The perception of lan guage. In D. Horton & J. Jenkins J (Eds.), (p. 150-184). Columbus, OH: Charles E. Merrill. 
D’Mello, S., Olney, A., Williams, C., & Hays, P. (2012). Gaze tutor: a gaze-reactive intelligent tutoring system. In ternational Journal of Human-Computer Studies, 70(5), 377-398. 
Graesser, A., & McNamara, D. (2010). Self-regulated learn ing in learning environments with pedagogical agents that interact in natural language. Educational Psychologist, 45(4), 234-244. 
Grice, H. P. (1975). Logic and conversation. In P. Cole & J. L. Morgan (Eds.), Syntax and semantics: Vol. 3: Speech acts (p. 41-58). New York: Academic Press. 
Hayashi, Y. (2012). On pedagogical effects of learner-support agents in collaborative interaction. In Proceeding of the 
11th international conference on intelligent tutoring sys tems(its2012) (p. 22-32). 
Hayashi, Y. (2014). Togetherness: Multiple pedagogical con versational agents as companions in collaborative learning. In Proceeding of the 12th international conference on in telligent tutoring systems(its2014) (p. 114-123). 
Hayashi, Y. (2016a). Coordinating knowledge integration with pedagogical agents:effects of agent gaze gestures and dyad synchronization. In Proceeding of the 13th interna tional conference on intelligent tutoring systems(its2016) (p. 254-259). 
Hayashi, Y. (2016b). Lexical network analysis on an online explanation task: Effects of affect and embodiment of a pedagogical agent. IEICE Transactions on Information and Systems, E99.D(6), 1455-1461. 
Hayashi, Y. (2018). The power of a ”maverick” in collabora tive problem solving: An experimental investigation of in dividual perspective-taking within a group. Cognitive Sci ence, 42(S1), 69-104. doi: 10.1111/cogs.12587 
Hayashi, Y. (in press). Multiple pedagogical conversational agents to support learner-learner collaborative learning: Ef fects of splitting suggestion types. Cognitive Systems Re search. doi: 10.1016/j.cogsys.2018.04.005 
Heidig, S., & Clarebout, G. (2011). Do pedagogical agents make a difference to student motivation and learning? Ed ucational Research Review, 6(1), 27-54. 
Holmes, J. (2007). Designing agents to support learning by explaining. Computers & Education, 48(4), 523-547. Koedinger, K. R., & Aleven, V. (2007, Sep 01). Explor ing the assistance dilemma in experiments with cognitive tutors. Educational Psychology Review, 19(3), 239–264. doi: 10.1007/s10648-007-9049-0 
Koedinger, K. R., Anderson, J. R., Hadley, W. H., & Mark, M. A. (1997). Intelligent Tutoring Goes To School in the Big City. International Journal of Artificial Intelligence in Education (IJAIED), 8, 30-43. 
Koedinger, K. R., Booth, J. L., & Klahr, D. (2013). Instruc tional complexity and the science to constrain it. Science, 342(6161), 935–937. doi: 10.1126/science.1238056 
Kumar, R., & Rose, C. (2011). Architecture for building con versational architecture for building conversational agents that support collaborative learning. IEEE Transactions on Learning Technologies, 4(1), 21-34. 
Leelawong, K., & Biswas, G. (2008). Designing learning by teaching agents: The betty’s brain system. International Journal of Artificial Intelligence in Education, 18(3), 181- 208. 
Miyake, N. (1986). Constructive interaction and the inter active process of understanding. Cognitive Science, 10(2), 151-177. 
Richardson, D. C., & Dale, R. (2005). Looking to under stand: The coupling between speakers’ and listeners’eye movements and its relationship to discourse comprehen sion. Cognitive Science, 29(6), 1045-1060. 
Schneider, B., & Pea, R. (2014). Toward collaboration sens 474
ing. International Journal of Computer-Supported Collab orative learning, 4(9), 5-17. 
Schober, F., M. (1993). Spatial perspective-taking in conver sation. Cognition, 47(1), 1-24. 
Shirouzu, H., Miyake, N., & Masukawa, H. (2002). Cogni tively active externalization for situated reflection. Cogni tive Science, 26(4), 469-501. 
Tomasello, M. (1995). Joint attention as social cognition. In C. Moore & J. Dunham P (Eds.), (p. 103-130). Hillsdale, NJ: Lawrence Erlbaum Associates Inc. 
Vygotsky, L. S. (1980). The development of higher psycho logical processes. Harverd University Press. 
Wu, S., & Keysar, B. (2007). The effect of information over lap on communication effectiveness. Cognitive Science, 31(1), 169-181. 
Abstract 
Noisy Time Preference 
Lisheng He (hlisheng@sas.upenn.edu) 
University of Pennsylvania, Philadelphia, PA 
Sudeep Bhatia (bhatiasu@sas.upenn.edu) 
University of Pennsylvania, Philadelphia, PA 
complex-- psychological assumptions about explicit biases in  
People’s desire to be patient or impatient can fluctuate from  moment to moment, yet little is known about the effects of  variability in time preference on intertemporal choice  behavior. We examine this issue through the lens of an  exponential discounting model with noisy discount factors. We  show that such a model can generate decreasing patience over  time, accounting for behavioral patterns typically attributed to  hyperbolic discounting, while also making reasonable  predictions regarding violations of intertemporal dominance.  Additionally, two experiments reveal that many participants do  display noise in their discount factors, and that a noisy discount  factor model outperforms hyperbolic models in terms of  quantitative fit. Ultimately the majority of participants are best  described by some type of exponential discounting model  (with or without noisy discount factors). These results indicate  that it may not be necessary to assume alternate forms of non exponential discounting, as long as the discount factors in an  exponential model are permitted to vary at random. These  results also highlight the importance of allowing for different  sources of noise in choice modeling. 
Keywords: decision making; intertemporal choice; noise;  variability; computational modeling 
Introduction 
Random noise plays a central theoretical role in  psychological research on high-level cognition. The  assumption of noise not only explains variability in  individuals’ responses across multiple identical trials; When  allowed to interact with attention, memory, and valuation,  unsystematic noise is also capable of generating a systematic  effect on behavior. In recent years, this type of unsystematic  noise has been shown to account for error and response time  patterns in perceptual and lexical choice, biases in probability  judgment and social judgment, paradoxes in risky decision  making, and the appearance of inconsistent or intransitive  preferences (Bhatia & Loomes, 2017; Brown & Heathcote,  2008; Costello & Watts, 2014; Denrell, 2015; Erev, Wallsten  & Budescu, 1994; Hilbert, 2012; Howes et al., 2016; Ratcliff  & Rouder, 1998; Regenwetter, Dana & Davis-Stober, 2011;  Tsetsos et al., 2016). In many of these cases unsystematic  noise is enough, by itself, to provide a full account of  observed behavioral patterns, making additional --more  
 
1 In a choice set consisting of many different payoffs with different time  delays, the payoff with the highest discounted utility, according to Equation  1, is the one that is chosen. When each option offers multiple payoffs (each  with a different time delay), the payoffs are individually discounted based  on their time delay, and aggregated into a single utility measure. Note that it  
475
the judgment or decision process unnecessary.  In this paper we provide a formal characterization and  analysis of the role of noise in intertemporal choice, that is,  choice between payoffs occurring at different points in time.  Our approach is motivated by the recent theoretical claims of  Bhatia and Loomes (2017), who suggest that there are two  key sources of noise in the preferential choice process. The  first involves noise in response generation, with decision  makers occasionally making mistakes in translating their  preferences into choices. The second involves noise in the  preferences themselves, with the parameters that characterize  these preferences fluctuating from trial to trial. Bhatia and  Loomes (2017) apply both sources of noise within a  “rational” expected utility theory framework in a set of risky  choice tasks, and show that the resulting model can predict  (seemingly irrational) violations of EUT, such as choice  patterns commonly seen to support Prospect Theory accounts  of risk taking.  
In intertemporal choice, it is exponential discounting that  is considered to be the rational or normative model. In this  paper, we propose an exponential discounting model that  allows for trial-to-trial variability in discount factors, as well  as random mistakes in generating responses, and then  examine the properties of this model with both simulations  and experiments. Our analysis tests the descriptive  boundaries of the exponential discounting model and  evaluates when it is and is not necessary to deviate from this  rational theory to describe irrational patterns in intertemporal  choice data. By performing these tests, we hope to obtain a  deeper understanding of the effects of noise in intertemporal  choice, complementing the rich existing theoretical literature  on variability in cognition and behavior.  
Intertemporal Discounting 
The simplest intertemporal choice task requires a decision  maker to evaluate an option X offering a payoff x with a time  delay of t. Discounting models of intertemporal choice  assume that these evaluations involve the calculation of a  discounted utility, which weighs the payoff based on the  magnitude of the time delay. Thus, for a discount function  d(·), the utility of X is given by: 1 
is sometimes assumed that payoffs are transformed non-linearly according  to a value function, prior to being discounted. However, for expositional  clarity, we will avoid this assumption for the purposes of this paper. Our  results should not vary with more complex assumptions regarding payoff  valuation. 
�(�) = �(�) ∙ � (1) 
Exponential discounting, initially introduced by Samuelson  (1937), involves a particularly parsimonious discount  function. For a discount factor δ, it assumes that the  discounted weight on the payoff value is simply given by: 
 � � = �, (2) 
where 0 ≤ δ ≤ 1. Smaller values of δ correspond to increased  discounting and lead to smaller weights on later payoffs  relative to sooner payoffs. δ = 1 corresponds to a complete  absence of discounting of delayed payoffs. This one  parameter discount function is the most commonly used  discounting function in economics, as well in various  applications in psychology, such as reinforcement learning. 
Exponential discounting is, however, limited from a  descriptive perspective. Notably it is unable to account for  observed patterns of decreasing impatience for intertemporal choices. Consider, for example, a choice between an option  XP offering $5 immediately and option YP offering $10 in one  month (proximal choice), as well as between option XR offering $5 in one month and option YR offering $10 in two  
 There is also, however, another source of randomness in  choice: preference noise. The parameters of utility-based  models often provide a formal representation of decision  makers’ preferences. These preferences may not be constant  over the time course of an experiment; that is, they may  themselves fluctuate in a noisy manner (Becker, DeGroot &  Marschak, 1963; Loomes & Sugden, 1995; Regenwetter &  Marley, 2001). Within an exponential discounting model, this  type of variability would correspond a distribution of  discount factors described by a probability density function  f(δ). δ varies from trial to trial, according to f, causing the  discount function and thus the option utilities to vary from  trial to trial. In a given trial, the option with the higher utility  contingent on the sampled δ would be chosen. In a choice  between option X offering payoff x with delay t, and option Y offering payoff y with delay s, the probability of choosing X  is given by: 
Pr � chosen = � �, � � � � �� 
1 �� �,� > �A� 
months (remote choice). As both the payoffs and the  difference in time delays are the same for the proximal and  
 � �, � � = 
0.5 �� �,� = �A� 0 �� �,� < �A� 
 (4) 
remote choice, exponential discounting, with a fixed discount  factor δ, predicts that participants should either select the  sooner payoff in both choices or the later payoff in both  choices. However, some studies suggest that participants  typically select the sooner payoff in the proximal choice, but  the later payoff in the remote choice (Green, Fristoe, &  Myerson, 1994; Kirby & Herrnstein, 1995; Thaler, 1981; see  Frederick, Loewenstein, & O’Donoghue, 2002 for a review).  In response to such violations, researchers have suggested  that the shape of the discounting function is not exponential  but hyperbolic (e.g., Laibson, 1997; Loewenstein & Prelec,  1992; Mazur, 1987; see Table 1 for a representative list of  hyperbolic models). 
Noise in Intertemporal Choice 
One critical issue with the above models is their inability  to account for stochasticity inherent in human behavior. In  order to use discounting models to describe stochastic choice  data, discounting models need to be recast in probabilistic  terms. Exponential discounting is generally modelled  alongside some assumption of response noise, typically in the  form of a logistic choice rule that transforms discounted  utilities into choice probabilities (McFadden, 1973). Here, for  options X and Y offering payoffs x and y with time delays t and s respectively, the probability of selecting X over Y is  given by:  
Pr[� chosen] = 7 
78 9:; {=> ? , ∙@ = ?(A)∙B } (3) 
where θ ≥ 0 is a parameter that determines the extent of noise  in the choice process. Smaller values of θ correspond to  noisier choices, with θ = 0 generating completely random  choice (i.e. X and Y equally likely to be chosen, regardless of  underlying payoffs and time delays).  
476
In such a formulation, E[δ] is the expected discount factor,  and can be seen as characterizing the decision makers’  underlying time preference. Although this underlying time  preference is stable, trial-to-trial variability in δ could alters  decision makers’ utilities when they are exposed to the same  decision problems repeatedly, thus leading to occasional  mistakes in choice.  
Although both response and preference noise do generate  stochastic behavior, they are unable, by themselves, to  account for violations of exponential discounting, such as  decreasing impatience. This is because both types of noise,  when applied individually, generate modal choice predictions  that are in the direction of the prediction of the corresponding  noiseless exponential model.  
Of course both preference and response noise can influence  intertemporal choice simultaneously. In this setting we would  have both variability in discount factors for generating  utilities, as well as variability in translating utilities into  choice. Choice probabilities with such a model can be  obtained by integrating Pr[X chosen] as defined in Equation  3, over the range of feasible values of δ, weighted by their  respective probabilities. Thus, in a choice between option X  offering payoff x with delay t, and option Y offering payoff y with delay s, the probability of choosing X when both types  of noise are present would be given by: 
Pr � chosen = � �, � � � � �� 
 � �, � � = 7 
78 9:; {=> PQ∙@ = PR∙B } (5) 
These choice probabilities can deviate from the predictions  of the corresponding deterministic exponential model (with discount factor of E[δ]). The reason for this is that the utility  
difference between X and Y, δt·x – δs·y is non-linear in δ. This  means that variability in δ distorts the expected differences in  utility between the two options, so that the expectation of the  utility difference between X and Y, E[U(X) – U(Y)], is not the  same as the utility difference of these options, U(X) – U(Y) 
for E[δ]. When preference noise is applied by itself (as in  Equation 4), this distortion does not alter modal choice, as the  choice rule is based only on whether δt·x > δs·y or δt·x < δs·y,  and not on the magnitude of δt·x – δs·y. However, when these  utility differences are combined with response noise (as in  Equation 5) the distorted expected utility differences leads to  distorted choice probabilities. Note that this can happen even  if the distribution δ is symmetric around E[δ]. 
Properties 
The exponential discounting model, with both response and  preference noise, can account for violations of exponential  discounting, such as decreasing impatience. As an illustration  of this, consider again the proximal and remote choices in the  decreasing impatience example above. If we only allowed for  the response noise, and set θ = 1 in Equation 3, we would  obtain Pr[XP chosen] < 0.5 in the proximal choice and Pr[XR chosen] < 0.5 in the remote choice for all values of δ < 0.5,  and Pr[XP chosen] > 0.5 in the proximal choice and Pr[XR 
chosen] > 0.5 in the remote choice for all values of δ > 0.5.  This is shown in Figure 1a. 
Figure 1: The probability of X chosen as a function of    (mean) discount factor in proximal and remote choices. (a)  Only response noise (θ = 1) is assumed. (b) Both response  (θ = 1) and preference noise (η = 0.25) are assumed. 
Now consider adding preference noise to this formulation,  with δ ~ Uniform[δ* - 0.25, δ* + 0.25]. Note that E[δ] = δ*. In  this setting, we find that Pr[XP chosen] < 0.5 in the proximal  choice and Pr[XR chosen] < 0.5 in the remote choice for δ* <  0.457, and Pr[XP chosen] > 0.5 in the proximal choice and  Pr[XR chosen] > 0.5 in the remote choice for δ* > 0.50. For δ*  in the range (0.457,0.500) we obtain both Pr[XP chosen] < 0.5  and Pr[XR chosen] > 0.5, consistent with the finding of  decreasing impatience. Note that this asymmetry emerges  despite preference noise being unsystematic (i.e. δ distributed  symmetrically around δ*). This is shown in Figure 1b. 
A combination of response and preference noise is also  necessary for making reasonable predictions for  intertemporal dominance. Consider, for example, a choice  between an option X offering $10 immediately and option  YND offering $15 in one month (non-dominance choice), as  
477
well as another choice between option X, and option YD offering $7.50 immediately (dominance choice). For a  decision maker with δ = 0.5 we have U(X) - U(YND) = U(X) - U(YD) = 2.50. As the difference in utilities is the same  between X and YND and between X and YD¸ an exponential  choice model with only response noise (as in Equation 3)  would predict the same choice probability of X in both cases.  In other words, the decision maker would be equally likely to  make a mistake and select the less desirable option in the non 
dominance choice as in the dominance choice.  In reality decision makers can detect dominance. Although  they do occasionally choose dominated options, the  likelihood of doing so is much lower than that typically  predicted by models equipped with only response noise (e.g., Busemeyer & Townsend, 1993; Loomes & Sugden, 1998). In order to provide an adequate account of intertemporal  dominance, we once again need both response and preference  noise. For example, if we allow for θ = 1, as well as δ ~  Uniform[δ* - 0.25, δ* + 0.25], with δ* = 0.5, we obtain Pr[X chosen] = 80.0% in the non-dominance choice, but Pr[X chosen] = 92.4% in the dominance choice. Thus even though  the difference in utilities between X and YND and between X  and YD is the same under E[δ] = δ*, the probability of choosing  X is higher when it dominates its competitor.  
The intuition for the above choice patterns is  straightforward: Response noise generates mistakes based on  the utility differences between options, implying that  dominance is violated too frequently when response noise is  applied by itself. Preference noise, in contrast, never violates  dominance. When a given δ is applied to the two options in a  dominance trial, the utility for the dominating option is  always greater than that for the dominated option, leading to  a choice probability of 0% for the dominated option. The  combination of response and preference noise results in an  averaging of these two extreme predictions. Thus, in a model  with both response and preference noise, it is possible to  choose a dominated option, but the probability of this is  smaller than the probability of choosing an equally desirable  non-dominated option. 
Experiments 
We ran two experiments to further test the explanatory score of the exponential model with both response and preference  noise.  
Methods and Materials  
A total of 89 undergraduate students from a university in  United States participated in our two experiments: 44  participants (31 female; aged 20.26 ± 1.25) in Experiment 1  and 45 participants (25 female; aged 19.84 ± 1.49) in  Experiment 2.  
Experiment 1 involved hypothetical binary choices  between an option X offering a payoff of x after a time delay  t, and an option Y offering a payoff of y after a time delay s =  t + k. We set x = $100 in all trials and chose t from the set  {today, 3 months, 6 months, 9 months} and k from the set {3  months, 6 months, 9 months}. y was determined by applying  
annual interest rate from the set {-50%, 50%, 100%, 500%,  1000%} to the corresponding time delays. This generated a  total of 60 unique choice pairs. Note that the use of a negative  interest rate implied that 12 of these choice pairs involved a  dominated option (offering a smaller reward with a larger  time delay than its competitor).  
As shown below, Experiment 1 involved fairly high choice  probabilities for the delayed option Y. Although this should  not alter our key conclusions, we wished to replicate our tests  with stimuli generating roughly equivalent choice  probabilities for X and Y. Thus we ran a second experiment,  with stimuli generated using the methods above, but with  annual interest rates in the set {-25%, 25%, 75%, 125%,  175%}. By using smaller interest rates we obtained smaller  values of y for corresponding values of t and k, leading to  higher choice proportions for X.  
We excluded data from five participants in Experiment 1  and three participants in Experiment 2 because they  constantly chose either X or Y in all the non-dominance  choices. This left 39 participants in Experiment 1 and 42  participants in Experiment 2 for our analysis.  
Model Fitting 
The main goal of the two experiments was to test whether the  exponential discounting model with both response and  preference noise is able to provide a good quantitative  account of choice data. For this purpose, we fit the core  exponential discounting model embedded in a logistic choice  rule for response noise, with a variable discount factor δ ~  Uniform[δ* - η, δ* + η] (Equation 5), with δ* - η ≥ 0 and δ* + η ≤ 1. We refer to this model as the noisy exponential model 
for the remainder of this paper. 
Table 1: Alternate hyperbolic discounting functions. 
Function Name Function Form Domain Mazur-1 hyperbolic � � = 1 + �� =7 α > 0 Mazur-2 hyperbolic � � = 1 + ��U =7 α, τ > 0  LP hyperbolic � � = 1 + �� =V W α, β > 0 
within the logistic choice function (Equation 4) to allow for  response noise.  
Results 
Summary of Choice Data Among the non-dominance  choices, we found that option Y was chosen 66.9% of the time  in Experiment 1, and 48.6 % of the time in Experiment 2. The  frequency of choosing the dominated options was  comparable across the two experiments: Participants in  Experiment 1 and Experiment 2 chose the dominated option  (option Y offering both the smaller and the more delayed  reward) 3.9% of the time and 4.7% of the time respectively.  
We also tested for decreasing impatience in the two  experiments. In the non-dominance trials of Experiment 1,  participants chose option Y (offering the payoff with the  larger delay) 65.9% when t = 0 months, 66.0% when t = 3  months, 67.7% when t = 6 months, and 67.8% when t = 9  months. Formally, the probability of choosing Y in the non 
dominance trials increased with t in a mixed-effect logistic  regression model with random intercepts for participants  intervals between options and the implied interest rates in the  trial (β = 0.028, z = 2.397, p = .017). A similar test applied  individually to each participant found that the effect of t on  the choice probability of Y was positive and significant (p <  0.05) for six participants, positive and non-significant (p >  0.05) for 13 participants, negative and significant for one  participant, and negative and non-significant for 19  participants. These results provide some evidence of  decreasing impatience on the aggregate level, but also  suggest substantial heterogeneity in this choice pattern across  participants, with roughly half the sample showing  significant or non-significant decreasing impatience, and the  other half showing significant or non-significant increasing  impatience. 
We obtained more ambiguous results in Experiment 2. In  this experiment we observed a choice frequency for option Y of 47.6% when t = 0 months, 49.0% when t = 3 months, and  48.9% when t = 6 months, and 49.1% when t = 9 months.  
Quasi-hyperbolic � � = 1, when � = 0 ��,, when � > 0 
0 ≤ β ≤ 1 0 ≤ δ ≤ 1 
Although this choice frequency is increasing in t, it did not  reach statistical significance in a mixed-effect model (β =  0.015, z = 1.447, p = 0.148). On the individual level, the effect  
We also wished to contrast the predictions of the noisy  exponential model with the various hyperbolic models  proposed in prior work. We considered the one parameter  hyperbolic discounting model proposed by Mazur (1987),  which we refer to as Mazur-1 hyperbolic, as well as the two  parameter hyperbolic model proposed by Mazur (1987),  which we refer to as Mazur-2 hyperbolic. We also considered  the two-parameter generalized hyperbolic discounting model  proposed by Lowenstein and Prelec 1992), which we refer to  as the LP hyperbolic. Finally, we considered the quasi hyperbolic model proposed by Laibson (1997). All these  hyperbolic discounting models are presented in Table 1. We  also tested the predictive power of the baseline exponential  model (Equation 2). All hyperbolic discounting models and  the baseline exponential discounting model were embedded  
478
of t on the choice probability of Y was positive and significant  for three participants, positive and non-significant for 21  participants, negative and significant for one participant, and  negative and non-significant for 17 participants. This time, a  little bit more than half sample showed significant or non 
significant decreasing impatience, and the other half showed  significant or non-significant increasing impatience. Overall, the findings of Experiments 1 and 2 suggest that  decreasing impatience is not as robust as is widely held, but is consistent with some recent experiments with similar  inconclusive effects for decreasing impatience (e.g., Kable &  Glimcher, 2010; Read, 2001). 
Best-fit Parameters. Of key interest to the tests in this paper  is the preference noise η in the noisy exponential model. Figure 2 shows the individual-level and group-level estimates  
of η, as well as the estimates of δ* from the noisy exponential  model. As the baseline exponential model is nested in the  noisy exponential model and η is the additional specification, we can evaluate the statistical significance of η using the  likelihood ratio test. The variable being evaluated here is the  ratio between the maximum likelihood values of the two  models. This ratio has a chi-square distribution with a degree  of freedom of 1. The likelihood ratio test reveals that the  noisy exponential model has a significantly higher likelihood  than the baseline exponential model (implying that restricting  η = 0 results in significantly worse fits) on the group level  (χ2(1) > 3.84, p < .05) as well as for 46% participants for  Experiment 1, and for 42.9% of participants for Experiment  2. 
Figure 2: Distributions of individual-level (black dots) and    group-level (blue triangles) estimates of η and δ* from the  noisy exponential model. (a) Estimates of Experiment 1.(b) Estimates of Experiment 2.  
We can also perform a similar set of tests for the quasi hyperbolic model and the LP hyperbolic model, both of  which embed the baseline exponential discounting model. However, on an individual level only 17.9% of participants  in Experiment 1 and 19.0% of participants in Experiment 2  are better fit by the quasi-hyperbolic model than the  exponential model. These proportions are 15.4% and 38.1%  for LP hyperbolic model in Experiments 1 and 2 respectively.  
Model Comparisons The likelihood ratio tests shown above  suggest that the noisy exponential model does better than the  quasi-hyperbolic and LP hyperbolic models, as it provides a  greater improvement over the baseline exponential model  both on the group and on the individual level. However this  test cannot be used to directly compare the noisy exponential  
model with the quasi-hyperbolic and LP hyperbolic models,  as these models are not nested within each other. To perform  such a test, we thus need to use the Bayes Information Criterion, calculated as BIC = -2 ln(L) + k × ln(n), where L is  the maximum likelihood value, k is the number of free  parameters and n is the number of data points. Lower values  of BIC indicate better fits, controlling for model flexibility  (quantified by the total number of free parameters). Table 2 
shows the BIC values for the models fit to the group-level  data in Experiments 1 and 2. Group-level fits impose the  same parameters to all participants, not allowing for  individual differences. It also shows aggregate BIC values for  individual-level fits. As can be seen in Table 2, the best  performing model according to the group-level BIC and the  aggregate individual-level BIC, for both experiments, is the  noisy exponential model. We can also examine the proportion  of participants best fit by each of the models when all models  are compared simultaneously. Here we find that the best  performing model is the baseline exponential model, which  has the lowest BIC values for 41.0% and 46.5% of  participants in Experiments 1 and 2. This is followed by the  noisy exponential model, which provides the lowest BIC  values for 30.8% and 23.3% of participants in the two  experiments. Thus, around 70% of the participants in our two  experiments are best fit by either the baseline exponential or  the noisy exponential model, according to BIC.  
Summary and Discussion 
This paper has examined the role of noise in intertemporal  decision making through the lens of the exponential  discounting model. We propose a modification to this model  that allows for time preference to vary from moment to  moment. Formally, this involves distribution over the  discount factors that quantify time preference. We have  shown how this noisy exponential model can be used to  predict seemingly irrational patterns of behavior, such as  decreasing impatience. A noisy exponential model also  provides a better account of violations of intertemporal  dominance. 
Empirically, we have tested the quantitative properties of  our proposed model and the model fitting exercise has revealed a number of novel insights regarding the effect of noise in intertemporal discounting. Firstly, an examination of best fitting parameters has shown that the level of noise in 
Table 2: Summary of model fit in Experiments 1 and 2. 
Model Name Group-level BIC Aggregate Individual level BIC Percentage of Best Fits Exp 1 Exp 2 Exp 1 Exp 2 Exp 1 Exp 2 
Mazur-1 hyperbolic 9423 11633 6425 7357 17.9% 20.9% Mazur-2 hyperbolic 9410 11560 6118 6772 2.6% 2.3% LP hyperbolic 9408 11546 6043 6690 5.1% 0.0% Quasi-hyperbolic 9413 11547 6068 6706 2.6% 7.0% Exponential 9414 11556 5973 6610 41.0% 46.5% Noisy exponential 8996 11382 5867 6587 30.8% 23.3% 
479
discount factors, on the group level, is significantly greater  than zero. This is also the case on the individual level for  more than 40% of our participants, across the two  experiments (we observe positive noise for most the  remaining participants, but this does not reach statistical  significance). We also find that the noisy exponential model  outperforms the four hyperbolic models considered in this  paper, in terms of quantitative fit. This emerges on both on  the individual and the group level, for both the experiments.  Conversely, on the individual level, hyperbolic discounting  models do not provide better fits even relative to the baseline  exponential model, although the data provide some evidence  for decreasing impatience. This suggests that hyperbolic  discounting models either over predict decreasing impatience  or, if flexible enough to reduce to exponential discounting,  the premium in model fits does not overcome the penalty of  model complexity in model selection. 
The results of this paper indicate that a rational model of  intertemporal decision making that permits (unsystematic)  variability in the degree of time preference has tremendous  explanatory power. By doing so, it complements a rich  existing literature in psychology on the descriptive role of  random noise in cognition and behavior. Noise is not just  useful only for accommodating observed variability in  peoples’ behavior. Rather, it occupies a central theoretical  position of our understanding of this behavior.  
Acknowledgments 
We thank Wenjia Zhao and Zijin Zhang for their assistance  for data collection. 
References  
Becker, G. M., DeGroot, M. H., & Marschak, J. (1963). An  experimental study of some stochastic models for  wagers. Systems Research and Behavioral Science, 8(3),  199-202. 
Bhatia, S. & Loomes, G. (2017). Noisy preferences in risky  choice: A cautionary note. Psychological Review,124(5),  678-687.  
Brown, S. D., & Heathcote, A. (2008). The simplest complete  model of choice response time: Linear ballistic  accumulation. Cognitive Psychology, 57(3), 153-178. 
Busemeyer, J. R., & Townsend, J. T. (1993). Decision field  theory: A dynamic-cognitive approach to decision making  in an uncertain environment. Psychological  Review, 100(3), 432-459. 
Costello, F., & Watts, P. (2014). Surprisingly rational: Probability theory plus noise explains biases in judgment. Psychological Review, 121(3), 463-480. 
Denrell, J. C. (2015). Reference-dependent risk sensitivity as  rational inference. Psychological Review, 122(3), 461- 484. 
Erev, I., Wallsten, T. S., & Budescu, D. V. (1994).  Simultaneous over-and under-confidence: The role of error  in judgment processes. Psychological Review, 101(3), 519- 527. 
480
Frederick, S., Loewenstein, G., & O’Donoghue, T. (2002).  Time discounting and time preference: A critical review.  Journal of Economic Literature, 40(2), 351–401. 
Green, L., Fristoe, N., & Myerson, J. (1994). Temporal  discounting and preference reversals in choice between  delayed outcomes. Psychonomic Bulletin & Review, 1(3),  383-389. 
Hilbert, M. (2012). Toward a synthesis of cognitive biases:  how noisy information processing can bias human decision  making. Psychological Bulletin, 138(2), 211- 237. 
Howes, A., Warren, P. A., Farmer, G., El-Deredy, W., &  Lewis, R. L. (2016). Why contextual preference reversals  maximize expected value. Psychological review, 123(4),  368-391. 
Kable, J. W., & Glimcher, P. W. (2010). An “as soon as  possible” effect in human intertemporal decision making:  behavioral evidence and neural mechanisms. Journal of  Neurophysiology, 103(5), 2513-2531. 
Kirby, K. N. & Herrnstein, R. J. (1995). Preference reversals  due to myopic discounting of delayed reward.  Psychological Science, 6(2), 83–89. 
Laibson, D. (1997). Golden eggs and hyperbolic discounting.  Quarterly Journal of Economics, 112(2), 443–477. Loewenstein, G. & Prelec, D. (1992). Anomalies in  intertemporal choice: evidence and an interpretation.  Quarterly Journal of Economics, 107(2), 573-597. Loomes, G., & Sugden, R. (1995). Incorporating a stochastic  element into decision theories. European Economic  Review, 39, 641-648.  
Loomes, G., & Sugden, R. (1998). Testing different  stochastic specifications of risky choice. Economica, 65,  581-598.  
Mazur, J. E. (1987). An adjusting procedure for studying  delayed reinforcement. In M. L. Commons, J. E. Mazur, J.  A. Nevin, & H. Rachlin (Eds.), The Effect of Delay and of  Intervening Events on Reinforcement Value: Quantitative  Analyses of Behavior (pp. 55-73). Hillsdale, NJ: Lawrence  Erlbaum. 
Ratcliff, R., & Rouder, J. N. (1998). Modeling response times  for two-choice decisions. Psychological Science, 9(5),  347-356. 
Read, D. (2001). Is time-discounting hyperbolic or  subadditive? Journal of Risk and Uncertainty, 23(1), 5-32. Regenwetter, M., & Marley, A. A. J. (2001). Random  relations, random utilities, and random functions. Journal  of Mathematical Psychology, 45(6), 864-912. 
Regenwetter, M., Dana, J., & Davis-Stober, C. P. (2011).  Transitivity of preferences. Psychological Review, 118(1),  42-56. 
Samuelson, P. A. (1937). A note on measurement of utility.  The Review of Economic Studies, 4(2), 155-161. Thaler, R. H. (1981). Some empirical evidence on dynamic  inconsistency. Economics Letters, 8(3), 201-207. Tsetsos, K., Moran, R., Moreland, J., Chater, N., Usher, M.,  & Summerfield, C. (2016). Economic irrationality is  optimal during noisy decision making. Proceedings of the  National Academy of Sciences, 113(11), 3102-3107. 
Modelling reference production using the simultaneity approach:  A new look at referential success 
Daphna Heller (daphna.heller@utoronto.ca) 
Department of Linguistics 
University of Toronto, Canada 
Suzanne Stevenson (suzanne@cs.toronto.edu) 
Department of Computer Science 
University of Toronto, Canada 
Abstract 
When a speaker produces a referring expression, their overarching goal is to get the addressee to identify a particular  object in the context. This goal leads to the expectation that  speakers will use a referring expression tailored to the  perspective of the addressee. While research in  psycholinguistics has indeed found that speakers tailor their  referring expressions to the addressee’s perspective, they also  find egocentric tendencies; namely, a sensitivity to the  speaker’s own perspective. Mozuraitis, Stevenson and Heller  (2018) make the novel proposal that “mixing” perspectives is  a design feature of the production system, modelling data from  an experiment where knowledge mismatch concerned object  function. Here we further test this model on the more common  knowledge mismatch of visual perspective, modelling data  from Vanlangendonck, Willems, Menenti and Hagoort (2016).  The modelling results shed new light on concept of “referential  success” that has been assumed to guide reference production.  
Keywords: language production; reference; pragmatics;  audience design; computational modeling; common ground;  perspective-taking; probabilistic models. 
Introduction 
Audience design refers to the phenomenon where speakers  design their linguistic utterances to fit their audience, based  on their assessment of their addressee. It seems intuitively  necessary for speakers to engage in audience design if the 
goal of communication is for the addressee to recover the  message they encode in their utterance. But psycholinguistic  research on audience design, which focuses mainly on the  forms of referring expressions, has produced mixed results. 
While much research indeed demonstrates that speakers  adapt to their addressee in choosing the form of their  utterances (Nadig & Sedivy, 2002; Heller, Gorman &  Tanenhaus, 2012; Yoon, Koh, & Brown-Schmidt, 2012;  Gorman, Gegg-Harrison, Marsh & Tanenhaus, 2013), other work argues that speakers are egocentric, tailoring linguistic  forms to their own perspective (Brown & Dell, 1987; Horton  & Keysar, 1996; Wardlow Lane & Ferreira, 2008). 
A closer look at the referring expressions produced across  the different studies suggests that speakers’ behavior might  be better characterized as a “mixture” of two perspectives,  namely some adaptation to the addressee, along with some  egocentric tendencies. Indeed, Mozuraitis, Stevenson and  Heller (2018) were the first to propose that “mixing” is a  design feature of the system. Specifically, they propose that  in the tailoring of referring expressions, speakers  
481
simultaneously consider their own (egocentric) perspective  and their addressee’s perspective (see Heller, Parisien &  Stevenson, 2016, for a similar proposal about the  comprehension of referring expressions). 
The simultaneity approach has an interesting property  where it does not encode a global consideration of referential  success. Because in this approach referring expressions are  evaluated relative to each of the perspectives separately, and  a referring expression is selected based on “mixing” 
perspectives, the referring expression selected is not directly  evaluated as to whether it would allow the addressee to  identify the intended object. This aspect of the simultaneity  approach seems non-intuitive given that the goal of referring  is to get the addressee to identify a certain object. 
Referential success has been seen as a central goal for  referring expressions at least since philosopher Keith  Donnellan (1966) who coined the term referential for those  uses of descriptions where the goal is for the addressee to  choose an object intended by the speaker. Indeed, considerations of referential success have led Clark and  Marshall (1981) to propose that referring expressions are  tailored relative to shared knowledge. Even approaches that  argue that referring expressions are tailored to the egocentric  perspective alone (e.g., Horton & Keysar, 1996) include a  second step of “monitoring-and-adjustment” that checks  whether the resulting referring expression would allow the  addressee to identify the intended object. Thus, the  simultaneity approach contrasts with other approaches to the  production of referring expression. 
The goal of the current paper is to test this aspect of the  simultaneity approach by modelling production data from  Vanlangendonck, Willems, Menenti and Hagoort (2016). We  chose to model this study because it contains an explicit  manipulation that tests the role of referential success, namely,  a case of audience design where egocentricity could possibly  lead to referential failure, and a second case of audience  design where egocentricity is unlikely to be harmful to referential success. Modelling these conditions allows us to  test this aspect of the simultaneity model directly, as well as  test its generality beyond the original set of data for which it  was developed. 
The Vanlangendonck et al. (2016) study Vanlangendonck et al. (2016) (henceforth VWMH) examine the production of referring expression in a dialogue situation,  
where one participant acts as the speaker and a second  participant acts as the addressee. The speaker and the  addressee each saw an array of objects on their screens, as if  they were sitting on the two sides of a vertical shelving unit 
– see Figure 1. The most important aspect of this setup is that  it allows creating knowledge mismatch. Specifically, some  objects were visible to both participants (the objects with the  white background), while other objects were visible only to  the speaker and hidden from the view of the addressee (the  objects with the dark background) – see Figure 2. Thus, as in  many other studies on audience design (e.g., Horton &  Keysar, 1996; Nadig & Sedivy, 2002; Wardlow Lane et al.,  2006; Wardlow Lane & Ferreria, 2008; Yoon et al., 2012)  knowledge mismatch was established by visual co-presence. 
  
Figure 1: the experimental setup in VWMH 
The critical conditions of VWMH required audience  design: these are situations where the most appropriate  referring expression is different when it is tailored relative to  the speaker’s perspective versus when it is tailored relative to  the addressee’s perspective. VWMH tested two such case: in  the ADVISABLE condition, even if the referring expression is  tailored relative the speaker’s perspective, the addressee is  likely to identify the correct object despite the fact that this is  not the ideal referring expression from their own perspective.  In the OBLIGATORY condition, in contrast, if the referring  expression is tailored to the speaker’s perspective, the  addressee may not be able to identify the intended referent,  leading to referential failure. Let us consider these in order. 
  
Figure 2: VWMH conditions 482
In the ADVISABLE condition (top-left display in Figure 2),  the target objet (marked in red) is a candle, but, crucially, the  speaker also sees a second, bigger candle that is not visible to  the addressee. If the speaker tailors the referring expression  to the perspective of the addressee, they should use an  unmodified expression (e.g., the candle). If, however, they  tailor the referring expression based on their own perspective,  they will produce a modified referring expression (e.g., the  small candle). VWMH label this condition “advisable”  because adaptation can be seen as advisable rather than  necessary, as the use of a modified expression would  nevertheless allow the addressee to choose the intended  object and thus lead to referential success. This critical  condition of audience design was accompanied by two  control conditions: linguistic control with one candle (top middle display in Figure 2), and visual control with two  candles (top-right display in Figure 2).  
Because the research question concerns adaptation to the  addressee’s perspective, the results in this condition focus on  the proportion of trials where speakers produced a bare noun  (e.g., the candle), the expression expected from the  addressee’s perspective; the results are summarized in Figure  3. First, speakers behaved as expected in the control  conditions. In the linguistic control condition, which is  parallel to the addressee’s perspective in the audience design  condition, speakers mostly produced bare nouns (87.6%),  whereas in the visual control condition, which is parallel to  the speaker’s perspective in the audience design condition,  they produced a bare noun very rarely (1%). In the critical  case of audience design, speakers mostly produced bare  nouns (79.8%), exhibiting adaptation to the addressee.  Crucially, however, the adaptation is not complete, because  this proportion is significantly lower than the one in the  linguistic control condition. In this case, the lack of complete  adaptation might be due to the fact that not adapting would  not have a harmful effect on referential success. 
Turning to the OBLIGATORY condition (bottom-left display  in Figure 2), the target objet is a candle and there is a second,  bigger candle visible to both conversational partners, but,  crucially, the speaker can also see a third, smaller candle that  is not visible to the addressee. Here. if the speaker tailors the  referring expression to the addressee’s perspective, they  would say the small candle, whereas if they tailor the  referring expression to their own perspective, they will say  the medium candle (importantly, VWMH used objects in four different sizes, meaning that the expected size adjective could  not be determined by the absolute size of the object). The  OBLIGATORY condition is different from the ADVISABLE  condition in that the two perspectives lead to incompatible referring expressions. Thus, if the speaker fails to adapt to the  addressee in this case, the addressee might not be able to  identify the correct referent, leading to referential failure.  This condition was also accompanied by the two control  conditions: Linguistic control (bottom-middle display in Figure 2) which is parallel to the addressee’s perspective in  the audience design condition (i.e., two candles), and visual  control (bottom-right display in Figure 2), parallel to the  
speaker’s perspective in the audience design condition (i.e.,  three candles). 
Here adaptation would lead to using the adjective small (or  large), and hence the results are presented in term of the  proportion of trials on which speakers produced these  adjectives (e.g., the small candle) – see Figure 3. The control  conditions showed the expected pattern: in the linguistic  control (parallel to addressee’s perspective), speakers mostly  produced the small candle (97.3%), and in the visual control  condition (parallel to the speaker’s perspective) they rarely  produced such modifier (1.4%). In the critical audience  design conditions, speakers again showed adaptation to the  addressee, mostly producing the small candle (89.9%). Here  again, adaptation was not complete, as this value is  significantly lower than in the linguistic control condition. But in this case the lack of complete adaption is surprising, because the lack of adaptation could potentially threaten  referential success. 
One of modelling this pattern using the simultaneity  approach is to test whether the patterns observed here arise  from the same “mixing” behavior. 
  
Figure 3: VWMH experimental results. The dependent  variable plotted is the proportion of that behavior which is  adaptive in the audience design condition: a bare noun in  ADVISABLE and the small N in OBLIGATORY.  
Modelling the production data 
The Mozuraitis et al. (2018) simultaneity proposal is  operationalized in a computational model as: 
P(RE|obj) = ∑d∈DP(RE|obj,d)P(d) (1) 
This formula encodes the observation that a referring  expression depends not just on the referent object alone (i.e., obj) but also on the domain of reference (d), which is the set  of contextually-relevant objects from which the referent  needs to be distinguished in the current context. (Note that  the right hand side of Eqn. 1 is not an application of Bayes  rule.) This captures the fact that the same object may be  called the small vase if it appears with a bigger vase, but will  instead be called the big vase if it appears with a smaller vase. 
The referring expression to be produced, P(RE|obj),  requires summing across the possible domains of reference in  the context: for each domain, it takes into account the  probability of referring expressions for the object in that  
483
domain, P(RE|obj,d), and also the probability of the domain  itself, P(d). In a situation of knowledge mismatch between  the conversational partners, the different perspectives of the  partners constitute two relevant domains of reference: d=s is  the perspective of the speaker and d=a is the perspective of  the addressee. In this situation, where D={s, a}, we can  rewrite (1) as: 
 P(RE|obj) = P(RE|obj,d=s)P(d=s) 
 + P(RE|obj,d=a)P(d=a) (2) Thus, in the simultaneity approach a speaker doesn’t choose  between their own (egocentric) perspective and their  partner’s perspective, but instead uses both simultaneously,  combining the contributions of the two perspectives. While it  has been previously proposed that perspective information is  probabilistic (e.g. Hanna, Tanenhaus & Trueswell, 2003),  Mozuraitis et al., (2018) are the first to propose “mixing”. 
We use this approach to predict the behavior in VWMH  where there is knowledge mismatch, namely in the audience  design conditions (top- and bottom-left displays in Figure 2). P(RE|target, d). The first step is to estimate the probabilities  of referring expressions in the addressee’s perspective, P(RE|target,d=a), and in the speaker’s perspective, P(RE|target,d=s).  
Recall that VWMH’s Linguistic control conditions (top and bottom-middle displays in Figure 2) are equivalent to the  addressee’s perspective in the Audience Design displays.  Therefore, we use the production patterns in this condition to  estimate P(RE|target,d=a); see also Figure 3. 
Recall, further, that VWMH’s Visual control conditions  (top- and bottom-right displays in Figure 2) are equivalent to  the speaker’s perspective in the Audience Design displays.  Therefore, we use the production patterns in this condition to  estimate P(RE|target,d=s); see also the relevant columns in  Figure 3. 
We follow VWMH’s analysis, and use as the dependent  variable that form which would be the adaptive behavior in the audience design condition: for the advisable condition, it  is the N, and for the obligatory condition, it is the small N – see again Figure 3. 
P(d). Since the weighting of the two perspectives is not  directly observable (cf. Mozuraitis et al., 2018), our approach  is to determine the value, or range of values, for the weight  that yields a fit to the behavioral data. The resulting P(d) 
indicates the degree to which speakers engage in audience  design. Because we assume that d can only take on the values  speaker and addressee (see Mozuraitis et al., 2018 for  discussion), the two values exhaust the probability space, and  so P(d=s)+P(d=a)=1, or P(d=a)=1–P(d=s). In other words,  there is only one parameter to consider here, P(d=a), as the  other value can be derived from it; we therefore refer to the  parameter as P(a).  
We evaluate our modelling results by looking at what the  P(a) we obtain tells us about three issue: (1) how different  types of knowledge mismatch affect the weight P(a), (2) the  consistency of the weight for individuals; and (3) the  consistency of this weight across referring situations. 
Question 1: Comparing across situations with  different cues to the mismatched information The first question we address in modelling VWMH is  whether situations with different cues to shared versus  mismatched information lead to different weighing of the two  perspectives, as has been proposed in Heller et al. (2016) and  Mozuraitis et al. (2018). Specifically, in production, the idea  is that the more salient the addressee’s perspective is, the  more influence it will have (i.e., P(a) will be higher), and the  less salient it is, the less influence it will have (i.e., P(a) will  be higher). 
Mozuraitis et al. (2018) created situations in which the  knowledge mismatch between interlocutors concerned  objects’ function. To this end, they used visually-misleading  objects: objects whose function is not consistent with their  appearance, such as a crayon that is shaped like a Lego brick.  The mismatched situation they modeled was such that the  speaker knew the unexpected function of the object (this  function was demonstrated to them by the experimenter), but  the addressee did not (they turned their back to the  experimenter during the demonstration). The modelling  results showed, first, that the pattern of referring expression  used is not consistent with the speaker using only their own  perspective, or only the addressee’s perspective, even when  taking into account reasonable amount of noise in the data.  Instead, this data was successfully accounted for by “mixing”  the two perspectives. The best fit to the human data was  achieved when the two perspectives were weighed about  equally: P(a) =0.48 and P(s)=0.52. (Again, these sum to 1, so  in what follows we only report P(a)). When considering the  95% confidence intervals of the means for the modelled  condition, the range is 0.26 ≤ P(a) ≤ 0.64. 
Our goal here is to model the VWMH data, and compare  the P(a) we obtain from that data to Mozuraitis et al.’s modelling results. What is the prediction with respect to how  these should compare? We predict that in VWMH in  VWMH, where the cues to mismatch information are visual,  the addressee’s perspective will be weighed more than in  Mozuraitis et al. (2018), where the knowledge mismatch  concerned object function. This is because, first, the visual  mismatch in VWMH has a constant perceptual correlate:  objects that are not visible to the addressee have a  background with a different color, whereas in Mozuraitis et  al. (2018) speakers need to rely on their memory of the  experimenter demonstrating the function of the object.  Second, the visual setup in VWMH makes it highly unlikely  that the addressee would nonetheless know what the hidden  objects are. In Mozuiraitis et al. (2018), in contrast, speakers may notice that the visually-misleading object has some  properties that are not consistent with their appearance (e.g.,  noticing that the Lego-crayon is not made of plastic), and may  therefore entertain the possibility that the addressee could  also notice these properties and figure out the that what looks  like a Lego is really a crayon. In other words, this is a  situation where there is more uncertainty about the  addressee’s perspective. Finally, the VWMH setup requires  speakers to attribute absence of knowledge to their  
484
addressees, a level I Theory of Mind mismatch, whereas the  Mozuraitis et al. (2018) setup requires speakers to attribute to  the addressee different knowledge, a level II Theory of Mind  mismatch. As the latter is more complex, it stands to reason  that it will lead to less weight on the addressee’s perspective. 
Modelling, Because the experimental manipulation of  perspectives in Mozuraitis et al. (2018) was between participants, the production patterns in the two perspectives came from different participants than the pattern predicted by  the model. In other words, these results were population-level  modelling. Thus, for the VWMH data, we used the overall  means from the Visual control conditions as the speaker’s  perspective, the overall means from the Linguistic controls  conditions as the addressee’s perspective, and combine them  to achieve the means in the Audience Design conditions. We  then find, based on both sets of control condition, a single  value of P(a) that best predicts the Audience Design behavior  in both conditions. 
Results. The best fit is obtained with P(a) = 0.916, and the  model yields values in the ranges consistent with the 95%  confidence intervals for the two Audience Design conditions  at 0.908 < P(a) < 0.924. This result matches our prediction  that P(a) in VWMH will be higher than the P(a) obtained in  Mozuraitis et al. (2018), where the upper end of their range  was 0.64. 
That is, these modelling results demonstrate that the  addressee’s perspective is weighed far more in the situation  in VWMH in which the cues to the addressee’s perspective  are more salient. This is the first piece of evidence that the  weighing of perspectives depends on situational cues.  
Question 2: Comparing across individuals The claim of the simultaneity approach is that a speaker  weighs the probability of each potential referring expression  in the context of both their own and their addressee’s  perspectives in order to determine the form of the referring  expression to be produced. Above we modelled the data at  the population level. Our second goal is to examine whether  each speaker can be modeled individually as using a single  P(a) across all the trials. Because of the within-participant  design of VWMH, where each participant contributed to both  the Linguistic and Visual control conditions and to the  Audience Design conditions, we can model these data at the  individual level (this was not possible in Mozuraitis et al.,  because the type of knowledge mismatch they employed 
drove them to employ a between-participants design). Modelling. In modelling the subject-level human data from  VWMH, we model data from eighteen participants. Two  additional participants were excluded, because they made  corrections or edited their initial referring expression (e.g., by  adding or correcting the adjective) on more than 40% of the  trials (The remaining 18 participants made such corrections  on 15% of the trials or less). 
We split the trials for each speaker, across all six  conditions, into two equal-sized groups. To avoid order effects, we took every other trial for each of the six  conditions; for ease of reference we’ll call these half1 and  
half2. We then fit P(a) to the Audience Design conditions of  half1 based on the two perspectives, which were derived from  the Linguistic and Visual control conditions for half1. Next,  we combine the two perspectives in half2 (derived from the  Linguistic and Visual control conditions of half2) using the  weight P(a) we got from half1, to predict the pattern of  production in the Audience Design conditions in half2. To  ensure that there is no bias in one half, we also did the reverse:  derive P(a) from half2 and use it to predict half1. 
Results. To examine our model, we examine how well the  predicted response rates correlate with the observed response  rates for each subject in VWMH, for each half of the data.  When predicting half2 based on half1, we find a very high  correlations of r=0.931 (95% CI: 0.819, 0.974) – see Figure  4. We also find a very high correlation when predicting half1  based on half2: r=0.919 (95% CI: 0.791, 0.969) (for space  considerations, this is not plotted here). The fact that the P(a) that was fit from each half of the data can be combined with  the behavior in the control conditions of the other half to  predict the behavior in that Audience Design condition of that  other half demonstrates that each participant is using a  consistent weighing of P(a) throughout the experiment. 
  
Figure 4: the correlation between observed behavior in  half2 and those predicted by the model. 
More generally, this successful individual-level modelling  provides strong support for the claim of the simultaneity  approach that speakers combine their own perspective with  the addressee’s perspective in tailoring referring expressions. 
Question 3: Comparing across referential  situations  
In the modelling above, we show that each participant can be  seen as using a consistent setting of P(a) across all their trials  in the experiment. Note that this result was achieved when  using both Advisable and Obligatory conditions to fit P(a) 
and then predicting both the Advisable and Obligatory  conditions. however, those results do not indicate whether  speakers use the same setting of P(a) across the two referring  conditions, or whether they use one setting of P(a) in the  Advisable condition, and a different setting of P(a) in the  Obligatory condition. 
Recall that the two referring conditions differ in how lack  of adaptation to the addressee would affect referential  
485
success. In the Advisable condition, lack of adaptation (i.e.,  being egocentric and saying the small vase) should  nevertheless allow the addressee to pick the correct object (as  the addressee can see only one vase). But in the obligatory  condition, lack of adaptation (i.e., being egocentric and  saying the medium vase) could possibly confuse the  addressee who can only see two vases, and may therefore lead  to referential failure (recall that VWMH used four different  absolute sizes, and thus the adjective medium did not  correspond to a specific size of objects in their experiment). A different sensitivity to the consideration of referential  success would be reflected in the simultaneity model as a  different setting of P(a), with a higher P(a) when there is a  risk of referential failure. This would be similar to having a  global consideration of referential success, as has been  widely assumed in the literature. The simultaneity approach  does not encode a global consideration of referential success.  Instead, the relative weighing is hypothesized to be affected  by general aspects of the situational context and possibly the  individuals. 
Modelling. To test the model with respect to the potential  influence of referential success, we again fit the model’s  value for P(a) on half the data, and use that setting to combine  the data from the two control conditions in the other half and  predict the values in the Audience Design condition. But here  we split the data into the Advisable and Obligatory  conditions. This enables us to test directly whether subjects  are using a consistent setting of P(a) across the entire  experiment, or whether they instead adapt their weighing of  the addressee’s perspective depending on factors specific to  each of the referring conditions, namely based on their  consideration of which situation will lead to referential  success and which may lead to referential failure. The  simultaneity approach, in contrast, posits that the same P(a) should fit the data either direction, as the weighing is chosen  for a particular situation. 
Results. We fit P(a) based on the control conditions (Visual  control → speaker’s perspective; linguistic control →  addressee’s perspective) in the Obligatory trials for each  subject, and use that P(a), along with the control conditions  for the Advisable condition, to predict the Audience Design  response rate for that subject in the Advisable trials. In other  words, we use the weighing of perspectives derived from the  Obligatory data to predict the Advisable data. In this case, we  find a very high correlation between the predicted values and  the human data: r=.985 (95% confidence interval: .959 to  .994); this is plotted in Figure 5, top panel. 
We also did the reverse, namely, fitting P(a) based on  Advisable conditions, and then using the fit P(a) value, as  well as the Obligatory control conditions, to predict the rates  in the Audience Design condition for that subject in the  Obligatory trials. Here again we find a very high correlation  to the human data: r=.932 (95% CI: .823, .975); see Figure 5,  bottom panel. Using P(a) fit from each condition of the data  achieves an excellent fit to the other condition, supporting the  view that each participant is using a consistent weighing of  P(a) for both the Obligatory and Advisable trials. 
Figure 5: the correlation between observed values and  those predicted by the model. Top: values in ADVISABLE predicted based on P(a) fit from the OBLIGATORY condition. Bottom: values in OBLIGATORY predicted based on P(a) fit  
from the ADVISABLE condition. 
Discussion 
We used the probabilistic model of Mozuraitis et al., (2018)  to model human data from the production experiment of  VWMH. First, the within-participants design of VWMH  allowed for individual-level modelling, demonstrating the  generality of the simultaneity approach and providing  stronger support for it. 
More interestingly, this allowed modelling under a  different knowledge mismatch than Mozuraitis et al., (2018).  Modelling results reveal that speakers in VWMH weighed  the perspective of the addressee more than speakers in the  Mozuraitis study. We attribute this difference to the cues to  the mismatched knowledge available in each of the two  situations: with visual co-presence, this information is  perceptually available, is associated with less uncertainty,  and is a Level I Theory of Mind knowledge mismatch that is  easier to attribute to one’s partner. 
Finally, modelling the two referential situations reveals that speakers are not sensitive to a global consideration of  referential success in determining the weighing of  perspectives. This is a surprising result as the literature on  reference has generally assumed that such a global  consideration plays a crucial role in the production of  referring expressions. This finding is, however, predicted by  the simultaneity approach which assumes the weighing to be  
486
determined by cues in the situational context, and possibly  cues related to the individual speakers. 
Acknowledgments 
We are extremely grateful to Flora Vanlangendonck and her  colleague for sharing their data with us. We acknowledge  support from SSHRC of Canada. to D. Heller and from  NSERC of Canada to S. Stevenson. 
References  
Brown, P., & Dell, G. S. (1987). Adapting production to  comprehension: The explicit mention of instruments.  Cognitive Psychology, 19, 441-472. 
Clark, H. H., & Marshall, C. R. (1981). Definite reference  and mutual knowledge. In A. Joshi, B. Webber, & I. Sag  (Eds.), Elements of discourse understanding (pp. 10-63). 
Donnellan, K. (1966). Reference and definite descriptions.  Philosophical Review, 75, 281-304 
Gorman, K. S., Gegg-Harrison, W., Marsh, C. R., &  Tanenhaus, M. K. (2013). What's learned together stays  together: Speakers' choice of referring expression reflects  shared experience. Journal of Experimental Psychology:  Learning, Memory, and Cognition, 39(3), 843-853. 
Hanna, J. E., Tanenhaus, M. K., & Trueswell, J. C. (2003).  The effects of common ground and perspective on domains  of referential interpretation. Journal of Memory and  Language, 49, 43-61.  
Heller, D., Gorman, K. S. & Tanenhaus, M. K. (2012). “To  name or to describe: shared knowledge affects referential  form”. Topics in Cognitive Science, 4, 290-305. 
Heller, D., Parisien, C. & Stevenson, S. (2016). Perspective taking behavior as the probabilistic weighing of multiple  domains. Cognition, 149, 104–120. 
Horton, W. S. & Keysar, B. (1996). When do speakers take  into account common ground? Cognition, 59, 91-117. Mozuraitis, M., Stevenson, S. & Heller, D. (2018).  Modelling reference production as the probabilistic  combination of multiple perspectives. Cognitive Science. Nadig, A. S., & Sedivy, J. C. (2002). Evidence of  perspective-taking constraints in children’s on-line  reference resolution. Psychological Science, 13, 329–336. Wardlow Lane, L., Groisman, M., & Ferreira, V. (2006).  Don't talk about pink elephants! Psychological Science,  17(4), 273–277. 
Wardlow Lane, L. & Ferreira, V. S. (2008). Speaker-external  versus speaker-internal forces on utterance form: Do  cognitive demands override threats to referential success? Journal of Experimental Psychology: Learning, Memory,  and Cognition, 6, 1466-1481. 
Vanlangendonck, F., Willems, R. M., Menenti, L., &  Hagoort, P. (2016). An early influence of common ground  during speech planning. Language, Cognition and  Neuroscience, 31(6), 741-750. 
Yoon, S. O., Koh, S., & Brown-Schmidt, S. (2012). Influence  of perspective and goals on reference production in  conversation. Psychonomic Bulletin & Review, 19, 699– 707. 
An enhanced model of gemination in spelling:  Evidence from a large corpus of typing errors  
Christopher R. Hepner (chepner3@jhu.edu) 
Department of Neurology, Johns Hopkins University 
1629 Thames Street, Suite 350, Baltimore, MD 21213, USA 
Svetlana Pinet (spinet1@jhmi.edu) 
Department of Neurology, Johns Hopkins University 
1629 Thames Street, Suite 350, Baltimore, MD 21213, USA 
Nazbanou Nozari (nozari@jhu.edu) 
Department of Neurology; Department of Cognitive Science, Johns Hopkins University 1629 Thames Street, Suite 350, Baltimore, MD 21213, USA 
Abstract 
Geminates (or double letters) are a feature of many languages, including English. Studies of the spelling errors produced by individuals with orthographic working memory deficits have  provided evidence that geminates are not produced as two in dependent instances of the same letter. Instead, there must be  a special mechanism in the orthographic system that produces  geminates. Several theories have attempted to model such  mechanisms. However, in most cases, the predictions of such  theories have been tested using data from single-case neuro psychological studies. In the current study, we re-evaluate  these theories using the largest corpus of geminate errors in  typing collected to date, and show that no theory can explain  all the findings. We then propose an enhanced model of gem ination that can.  
Keywords: double letters; geminates; typing; orthographic  working memory; graphemic buffer 
Introduction 
To type a word (e.g., broom) to dictation, a sequence of  phonemes (/bɹum/) must be converted into a sequence of  letters (B-R-O-O-M). In neurotypical adults, this can be  accomplished by either serially mapping phonemes to  graphemes using the sublexical route, or by retrieving the  whole letter sequence in parallel from long-term memory  (LTM) using the lexical route. In the lexical route, hearing a  word activates its representation in phonological LTM, 
which activates the word meaning in the lexical semantic  system. This, in turn, activates the word spelling (BROOM)  in orthographic LTM (O-LTM). Orthographic information is  then processed by orthographic working memory (O-WM,  often referred to as the graphemic buffer), responsible for  maintaining the orthographic representation and selecting its  letters in sequential order to pass them on to effector specific motor plans, e.g., sequences of key presses in typ 
ing. 
In many languages, including English, spellings some times contain double letters or geminates (e.g., O in  BROOM). Findings from neuropsychological studies of  spelling disorders suggest that gemination is more than just  two independent instances of the same letter. For example,  
487
Fischer-Baum and Rapp (2014) describe a patient who pro duced more geminate additions in non-geminate words (e.g.,  MARK → MARRK) after spelling a geminate word (e.g.,  BROOM) than a non-geminate word (e.g., BROAD). The  perseveration of the gemination independently of the letter  identity implies the existence of a special geminate feature (see also Caramazza & Miceli, 1990). Other models, e.g.,  McCloskey et al. (1994), also propose a special mechanism  
for gemination, but without proposing a geminate feature. The majority of the data on which models of gemination  are based come from case studies of individuals with selec tive damage to O-WM. These individuals produce errors  across spelling modalities (writing, typing, and spelling out  loud) which increase in frequency as a function of word  length. While extremely valuable in principle, the utility of  neuropsychological data can be limited by the relatively  small number of errors of interest, as well as individuals’  idiosyncrasies. This is perhaps the reason why, despite sev eral elegant proposals, no consensus has been reached on  this topic. In this study, we have created a large corpus of  geminate errors from 100 neurotypical adults each typing  400 geminate words on two occasions. Using this corpus,  we test current theories of gemination and demonstrate that  none of them is sufficient to explain all of the findings. We  then propose an enhanced model which accounts for both current and previous findings on geminate errors. 
Theoretical Accounts of Gemination 
Any model of gemination must accommodate two basic  assumptions: (a) It must include a representation of letter  order in addition to letter identity; otherwise, words such as  DOG and GOD would be indistinguishable. A full review of  models of segment sequencing in language production is  beyond the scope of this paper, but it is important to note  that chaining models and their variations do not provide a  satisfactory explanation of O-WM errors. (b) It must have a  special mechanism for geminate production, beyond treating  geminates as two independent instances of the same letter. Generally speaking, two classes of models have been pro 
posed: geminate feature models (Caramazza & Miceli,  
1990; Fischer-Baum & Rapp, 2014) which represent the  geminate as an independent feature, and geminate links models (McCloskey et al., 1994) which represent the gemi nate by linking two adjacent slots in the positional frame to  the same letter identity. 
In the current paper, we pick two models that meet both  of the criteria above and are each representative of one class  of gemination models. The first model, McCloskey et al.  (1994), henceforth referred to as M1994, is mentioned  above. The second model, Glasspool and Houghton (2005),  henceforth referred to as G&H2005, combines a geminate  feature model with a competitive queuing mechanism for sequencing segments. This mechanism uses Initiate and End  nodes to dynamically establish a gradient of activation such  that activation is highest for the letter in the current position  and progressively lower for subsequent letters. This gradient  is implemented in an Item layer akin to a positional frame1 
separate from, but connected to, the letter identity represen tations. Letters are selected by a competitive winner-take-all  process (implemented in the model as a competitive filter),  and the produced letter is temporarily inhibited to prevent perseveration. The geminate feature is represented by a separate node, which, like the letter identity nodes, receives  activation from a single slot in the Item layer (i.e., the start ing geminate position; e.g., 3 in BROOM). If the geminate  feature’s activation passes a threshold, it sends a signal to  output production processes to repeat the production of the  last segment, after which it is inhibited just like letter identi ty representations. 
We test the predictions of these two models on our gemi nate error corpus to evaluate whether either, or both, can  account for all the findings. 
Methods 
One hundred native English speakers (56 females, Mage =  34, age range: 18–67 years), who had passed spelling and  typing proficiency pretests, participated for payment  through Amazon Mechanical Turk. Participants completed  two sessions of a timed typing to dictation task. The stimuli  were 600 words, 5-16 letters long, comprising 400 experi 
mental words with a single geminate and 200 filler words  without a geminate. All 600 words were presented auditorily  in each session in randomized order, and participants typed  them before a deadline (300 ms + 180 ms per letter) with an  ITI of 1000 ms, with breaks after every 50 trials. 
Results 
No response was produced on 78 experimental trials. Of the remaining 79,922 responses, 18,865 (23.60%) contained at  least one error. Of those, 3,894 (20.64%) consisted of a  single error affecting the geminate. This “clean” set was used in the analyses. All the error types obtained in this  
 
1 G&H2005 view this layer as also coding some information  about abstract letter identities, although the nature of such infor mation has not been clearly specified. 
488
study, with the exception of Splits (e.g., BOROM; discussed  in the Error Categories section) have also been reported in  handwriting studies, making it unlikely that we are looking  at typing-specific errors. Moreover, both the length effect  (more errors on longer words) and the position effect (more  errors in the middle positions) that are typical of O-WM  deficits were evident in our data; ���� = −51.07, ���� < .001 for  the length effect, and ���� = −4.03, ���� < .001 for the position  effect. We can thus conclude with reasonable confidence  that the errors in our corpus are representative of the same cognitive processes that have been investigated by previous  studies of gemination. 
Error Categories 
Table 1 presents the error types of interest, their definitions,  and examples. 
Geminate Deletions. M1994 explains these errors by as suming that one of the geminate links has been lost and a  repair process has removed the corresponding slot in the  positional frame. G&H2005 explains them by assuming that  the geminate feature has failed to reach the activation  threshold. Thus both accounts explain basic deletions. How ever, a closer look at the data show that the probability of a  geminate deletion is much higher if the target geminate  letter appears in the wrong position than if any other letter  appears in the wrong position. In the set of 1,283 errors  containing letter movements but no letter additions, dele tions (other than the deletion of a single copy of the gemi nate letter), or substitutions, 34.62% of responses (36 out of  104) with the target letter in the wrong position had gemi nate deletions, compared to only 11.37% (134 out of 1,179)  with a non-target letter in the wrong position, ����2 = 42.94, ���� < .001. This finding, which implies interdependence  between the letter identity and the gemination process, is not  expected from either account. 
Geminate Additions. M1994 accounts for additions  through “reloading”, a mechanism by which a degraded  representation can be refreshed by retrieving it again from  O-LTM. If the degraded and the newly-loaded representa 
tions have geminates in different positions, the result is an  addition. According to G&H2005, additions happen when  the geminate feature reaches the activation threshold in  more than one position. We report three empirical findings  regarding geminate additions and evaluate the two models  in light of each. The first is the distribution of geminate  additions around the target geminate position. Figure 1a  plots this distribution for 568 geminate additions in the  current dataset. The probability of geminate additions drops quickly the farther the position gets from the target position. 
In fact, the only position where the rate of geminate addition  is higher than chance is position −1, with 263 errors ob served compared to 107.49 expected, ����2 = 95.38, ���� < .001. Since the process for geminate additions proposed by  M1994 involves a geminate shift, it can account for the  increased likelihood of additions closer to the target position  
Table 1: Error categories, definitions, and examples with their respective error counts. 
Error type 
	Definition 
	Example 
	Count
	Geminate deletions 
	Only one copy of the geminate letter has been produced. 
	BROOM → BROM 
	1,853
	Geminate additions 
	Both the original geminate letter and another letter in the tar get spelling have been doubled.
	BROOM → BRROOM 
	568
	Geminate shifts 
	Another letter in the target spelling was doubled instead of the  original geminate letter.
	BROOM → BRROM 
	846
	Substitutions 
	The geminate has been substituted by two copies of a different  letter, either from within the sequence or from outside.
	BROOM → BRBBM BROOM → BRXXM
	38
	Exchanges 
	The original geminate letter has swapped positions with an other letter. The response contains a double letter, which may  or may not be the same as the geminate letter in the target.
	BROOM → BORRM BROOM → BOORM
	19
	Pseudosubstitutions 
	One of the two copies of the geminate letter has been replaced  by another letter, either from within the sequence or from  outside.
	BROOM → BROBM BROOM → BROXM
	390
	Splits 
	One of the two copies of the geminate letter has exchanged  with an adjacent letter, splitting the geminate.
	BROOM → BOROM 
	180
	



using the same mechanism (see below). G&H2005 predict  that geminate additions should be most likely in positions  adjacent to the target position because, due to the gradient of  activation across positions, adjacent positions have the next  highest activation after the target position, so they activate  the geminate feature more strongly than other non-target  positions. Thus both accounts predict that geminate addi 
tions should occur more often in positions closest to the  
definition, be more than one position away from the target  geminate position. Thus the propensity for geminate addi tions to occur on the same letter identity seems to override  the strong tendency for additions to occur close to the target  position. Since neither M1994 nor G&H2005 have any  mechanisms to bind the geminate feature to letter identity,  neither model can account for this finding. 
target geminate position. 
a 
50% 
The second finding is related to the first one: while the  
s 
40% 
n
probability of a geminate addition at position −1 is signifi 
o
i
30% 
t
i
cantly higher than chance, the same probability at position  
d
20% 
d
A
+1 is significantly lower than chance, with 33 errors ob 
 
f
10% 
o
 
served compared to 88.47 expected, ����2 = 27.34, ���� < .001. 
0% 
%
M1994 does not have a mechanism to account for this. In  G&H2005, the geminate feature gets inhibited after it has  affected production. In order to be activated again, it needs  
b 
to overcome this suppression. When the first production  
40% 
s 
occurs before the target position, the chance of recovering  
t
30% 
f
i
h
from inhibition at the target position is still good, because  
20% 
S
 
f
that position has a link to the geminate feature that can di 
o
10% 
 
%
rectly activate it. However, if the first production occurred 
0% 
at the target position, it is unlikely that the noise alone can  overcome the inhibition enough to produce the geminate  feature again in the next (i.e., +1) position. G&H2005 can  
-9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 Distance 
-9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 Distance 
thus account for this finding. 
Finally, the data suggest that, in words with an additional  copy of the target geminate letter (e.g., COCOON), gemi nate additions are much more likely on that additional copy than on any other letter, e.g., p(COCOON → COOCOON)  > p(COCOON → COCOONN). In the 267 geminate addi tion errors in which there was another copy of the target  letter, 79 (29.59%) of the additions occurred on that addi tional copy compared to 51.07 expected by chance, ����2 = 7.37, ���� = .007. This finding, which suggests a link between  the letter identity and the geminate feature, is especially  intriguing because second copies of the target letter must, by  
489
Figure 1. Distributions of geminate (a) additions and (b)  shifts. Solid lines are observed proportions and dashed lines  are proportions expected by chance. Error bars are 95% CIs. 
Geminate Shifts. According to M1994, these errors occur  when at least one of the two geminate links is detached and  a repair process reconnects the broken links to the wrong  letter. In G&H2005, shifts are caused by the activation of  the geminate feature in the wrong position. We examine the  same three patterns we reported above for geminate addi 
tions, this time for geminate shifts. Figure 1b plots the dis tribution of the 846 geminate shifts in the current dataset.  
Similar to additions, the probability of geminate shifts drops quickly the farther the position gets from the target position. Only positions −1 and +1 around the target geminate show  significantly higher than chance probability of a geminate  shift; position −1: 364 observed vs. 159.33 expected, ����2 = 114.76, ���� < .001; position +1: 215 observed vs. 137.69  expected, ����2 = 20.86, ���� < .001. M1994 predicts this pat tern, because more distant movements require more links to  be broken and reattached. G&H2005 also predicts this find ing because the gradient of activation across positions caus es adjacent positions (and thus the contents of those posi 
tions) to have more activation than distant positions. The strong asymmetry between −1 and +1 positions ob served in geminate additions is not visible here. Both posi tions show higher than chance probability of shifts, a find ing that both M1994 and G&H2005 can account for (see the  explanation of the same finding for geminate additions). Finally, in words with an additional copy of the target gem inate letter (e.g., COCOON), we examined whether a gemi nate is more likely to shift to that additional copy than to any other letter, e.g., p(COCOON → COOCON) >  p(COCOON → COCONN). Unlike geminate additions, this  comparison did not reveal a special status for the additional  copy of the target geminate in geminate shifts. In the 358  geminate shift errors in which there was another copy of the  target letter, 49 (13.69%) occurred on that additional copy  compared to 65.49 expected by chance, ����2 = 2.49, ���� = .943. Thus, both M1994 and G&H2005 can explain these findings. 
Substitutions. Substitutions happen when a letter is re placed by another letter from either within or outside the  target sequence. This is easily explained by both M1994 and  G&H2005 (and any other theory that views letter represen tations as separate from positions). A closer examination of  the data revealed that the target geminate letter is much less  likely to participate in substitutions than any other letter in  the word: out of the 1,204 responses in which the only error  was a substitution, only 3.16% (38 out of 1,204) affected the  target geminate letter, despite geminates accounting for  13.43% (1,204 out of 8,963) of the opportunities for these  errors, ����2 = 103.58, ���� < .0012. In M1994, the double links  between adjacent positions and the letter identity could  provide a mechanism for binding the letter more tightly to  its position. In G&H2005, on the other hand, there is no  mechanism to account for this pattern. 
Exchanges. Two distinct patterns of exchanges are of par ticular interest to us: position-preserving errors, in which  the target geminate letter has been exchanged (i.e., swapped  positions) with another letter, but the original position of the  geminate has been preserved (e.g., BROOM → BORRM) and identity-preserving errors, in which a similar swap be- 
 
2 The effect of gemination is reliable even after position is taken  into account. 
490
tween the target geminate and a non-geminate letter has  happened, but this time, the geminate has remained attached  to the target geminate letter rather than to its target position  (e.g., BROOM → BOORM). Our corpus contains 65 ex 
changes without additional letter insertions, deletions, or  substitutions. In this set, both patterns occur more often than  expected by chance: 23 position-preserving errors compared  to 10.51 expected by chance, ����2 = 5.31, ���� = .011, and 35  identity-preserving errors compared to 12.69 expected,  ����2 = 15.04, ���� < .001. Moreover, identity-preserving errors  were significantly more common than position-preserving errors, ����2 = 3.77, ���� = .026. The propensity for exchanges  to preserve geminate position is predicted by both M1994  and G&H2005, because the representation of the geminate  is connected to position in both of these models. However,  neither model would predict higher than chance probability  of identity-preserving errors or its greater probability than  position-preserving errors, because there is no mechanism  for binding the geminate representation directly to letter  identities in either model. 
Pseudosubstitutions. In M1994, pseudosubstitutions occur  when one of the two links to the target geminate letter is  broken and a repair process fills the empty position with a  different letter. G&H2005, or any theory that proposes a  single slot for the geminate letter in the positional frame,  can only explain pseudosubstitutions as a combination of  two independent errors: a geminate deletion and a letter  insertion adjacent to the geminate. If that were the case,  pseudosubstitutions should be less common than either of  those errors individually. However, there are significantly  more pseudosubstitutions (390) than single-letter insertions  adjacent to the geminate (258) in our set of 18,865 incorrect  geminate word spellings, ����2 = 26.95, ���� < .001, ruling out  the double-error explanation. 
Splits. These errors (N = 180) made up 4.52% of all the  geminate errors in our corpus, which is more than any stud ies of handwriting. We thus suspect that splits might be  specific to typing. In keeping with this assumption, splits  happened more often when the target geminate and the  intruding letter were typed with different hands than the  same hand, both for anticipations (e.g., BROOM → BRO MO) ����2 = 23.19, ���� < .001, and perseverations (e.g.,  BROOM → BOROM) ����2 = 12.05, ���� < .001. We thus  conclude that these errors most likely arise during motor  programming specific to the typed modality, which is out side of the scope of theories discussed in this study. 
Table 2 provides a summary of the empirical results report ed for each error category and indicates whether M1994  and/or G&H2005 can account for that finding. The two  models successfully explain a wide range of empirical find ings on geminate errors, but neither model in its current  form can account for all of the empirical findings. Three  classes of issues can be identified: (a) Cases that can be  accounted for by M1994, but not G&H2005. The common  
Table 2: Comparison of empirical findings to predictions of previous models. 
Finding 
	M1994 
	G&H2005
	1. Basic geminate deletions 
	✔ 
	✔
	a- Higher probability of geminate deletions when target geminate letter moves 
	🗶 
	🗶
	2. Basic geminate additions 
	✔ 
	✔
	a- Positional distribution 
	✔ 
	✔
	b- Suppression in +1 position 
	🗶 
	✔
	c- Higher probability of gemination of another copy of the target geminate letter 
	🗶 
	🗶
	3. Basic geminate shifts 
	✔ 
	✔
	a- Positional distribution 
	✔ 
	✔
	b- Significantly more geminate shifts in both −1 and +1 positions 
	✔ 
	✔
	c- No increased probability of gemination of another copy of the target geminate letter 
	✔ 
	✔
	4. Basic substitutions 
	✔ 
	✔
	a- Lower probability of substitutions affecting the target geminate than other letters 
	✔ 
	🗶
	5. Basic exchanges 
	✔ 
	✔
	a- Higher than chance probability of position-preserving errors 
	✔ 
	✔
	b- Higher than chance probability of identity-preserving errors 
	🗶 
	🗶
	c- Higher probability of identity-preserving than position-preserving errors 
	🗶 
	🗶
	6. Basic pseudosubstitutions 
	✔ 
	🗶
	



origin of these is the fact that M1994 proposes two slots for  the geminate letter in the positional frame, but G&H2005  proposes only one. (b) Cases that can be accounted for by  G&H2005, but not by M1994. The single instance of this  (2b in Table 2) stems from the presence of an inhibition  mechanism on the geminate in G&H2005 after the geminate feature affects production, which is absent in M1994. (c)  Finally, there are cases where both models fail to explain the  finding. The common feature of these cases is that they  suggest an interdependence between the letter identity and  the gemination process that is absent in both M1994 and  G&H2005. In the next section, we propose a model that  integrates these three features into the basic framework of  G&H2005, and show that the enhanced model can account  for all the empirical findings reported here. 
The Enhanced Geminate Model 
We maintain the general architecture of G&H2005 (Fig ure 2), with three modifications: (1) The positional frame  contains two slots instead of one for the geminate letter,  similar to M1994 (feature 1 in Figure 2). (2) While in the  original G&H2005 model the geminate feature affects out put processes (e.g., repeating the motor program for produc ing the last letter), the enhanced model proposes that the  main function of the geminate feature is to block the inhibi tion of letters after their production (feature 2 in Figure 2).  As can be seen in the figure, a single geminate node sends  an inhibitory signal to all the self-inhibitory connections to letter identities; however, only the letter that has been just  produced would have an activated self-inhibitory connec tion. Thus the geminate feature has a focal effect on that  particular letter. The novel mechanism we have proposed  here for the operation of the geminate feature has an im portant advantage over G&H2005: in that model, when the  
491
letter in the geminate position is reached, e.g., the first O in  BROOM, a signal is sent to the output processes to repeat  the production of the O. The geminate feature must then be  suppressed, otherwise more copies of O will be produced.  As acknowledged by the authors, this suppression makes it  hard for the model to account for double geminates, e.g.,  BALLOON, as well as the many adjacent geminate addition  errors, e.g., BRROOM, observed in our data. The enhanced  model, on the other hand, has no problem with double gem inates. When the first L in BALLOON is produced, the  geminate feature inhibits the L’s self-inhibition, thus keep 
ing it activated for re-selection in the next position (i.e., the  second L). The geminate feature is inhibited for the next  selection step so that extra copies of L are not produced, but  it is released from that inhibition afterwards, allowing it to  repeat the process when the first copy of O is selected. Be 
cause there are no words in English with three consecutive identical letters (e.g., BROOOM), this simple rule of “inhib it the geminate feature for one step after it has exerted its  effect” can account for all gemination patterns in English.  Finally, (3) the enhanced model differs from G&H2005 in  that it proposes a link between the target geminate letter and  the geminate feature, such that the letter can activate the  
geminate feature directly (feature 3 in Figure 2). When BROOM is to be produced, the operation of the  system is similar to G&H2005 until the third positional slot  is reached. Unlike G&H2005, not only the slot, but also the  target geminate letter O sends activation to the geminate  feature. This double source of activation ensures that the  geminate feature passes the threshold in most cases where  gemination is required. When O wins the competition  among the letters, it is produced in the third position. Nor mally, it would be immediately inhibited after production,  but the activated geminate feature inhibits this inhibition  process, whereby allowing O to win the competition again 
when it receives activation from the fourth positional slot.  The geminate feature itself undergoes inhibition once it has  exerted its influence. Thus, under normal circumstances, it  would not pass the activation threshold again, even though  the newly activated O in the fourth position tries to reacti 
vate it (this would lead to BROOOM-like errors). Since the  geminate feature is unlikely to pass the threshold here, O  will undergo post-production inhibition and the next letter in  the sequence will win the competition for the next slot. 
Because the enhanced model preserves all the important  basic features of G&H2005, it is expected to account for all  the findings that that model accounts for. These are ex plained earlier in the paper and we will not reiterate them  
feature over the threshold, compared to when it is receiving  no activation from the letter representations. 
Higher probability of identity-preserving exchanges com pared to chance (5b) and compared to position-preserving  exchanges (5c). Both of these findings also point to the fact  that the target geminate letter directly activates the geminate  feature, even when it appears in the wrong position. 
Start End 
Positional 
Frame 
here.3 Instead, we focus on the findings that were not ex 
1 
Gem 
3 
plained by one or both models. Findings 4a and 6 in Table 2  were accounted for M1994, but not G&H2005, whereas 2b  was only explained by G&H2005. The enhanced model can  account for 4a and 6, because it adopts M1994’s assumption  of double slots for the geminate, as well as 2b by virtue of  the post-production inhibition mechanism from G&H2005.  Table 2 shows four additional cases where neither M1994  nor G&H2005 could account for the data (1a, 2c, 5b and  5c). All of these cases point to a connection between the  target geminate letter and the geminate feature, which is  specified in the enhanced model’s new feature 3. Since  these findings and the specific feature in the model that is  proposed to account for them are new, we unpack the mech anism for each one below. 
Higher probability of geminate deletion when the target  geminate letter moves than when non-target letters move (1a). In the enhanced model, in the absence of noise, the  geminate feature can only pass the threshold of activation  necessary for its operation if its input is the summed activa tion of both the positional slot and the target letter. When  the target letter is activated in the wrong position, the gemi nate feature no longer receives the summed activation,  which causes a geminate deletion. During the movement of  non-target letters, the geminate feature still receives the  summed activation, making deletions less likely. It is im portant to note that under noisy circumstances, activation of  either the letter identity or the positional slot may be enough  to push the geminate feature above the threshold, just not as  robustly as when the summed input is received. 
Higher probability of a geminate addition on another copy  of the target geminate letter than on any other letter (2c). Since the enhanced model includes a connection from the  target letter identity to the geminate feature, the geminate  feature will receive activation from the letter whenever it is  selected, even in the non-target position. It would thus be  more likely for noise to push the activation of the geminate   
3 We have verified that the changes in the enhanced model do  not affect its ability to explain the patterns accounted for in the past  models. Due to space constraints, however, we were forced to limit  the discussion to cases not accounted for by other models. 
492
Identity B R O M 2 
Competitive Filter 
Output 
Figure 2: The enhanced geminate model. 
Conclusions 
The enhanced model proposed in this paper integrates key  insights from previous gemination models, but views the  gemination process as primarily consisting of inhibiting the  self-inhibition of the most recently produced letter. This  function, together with a direct link between the target gem 
inate letter and the geminate feature, allows the model to  account for all of the empirical data that, to our knowledge, have been reported on geminate errors. 
References 
Caramazza, A., & Miceli, G. (1990). The structure of gra phemic representations. Cognition, 37(3), 243–297.  Fischer-Baum, S., & Rapp, B. (2014). The analysis of per severations in acquired dysgraphia reveals the internal  structure of orthographic representations. Cognitive Neu ropsychology, 31(3), 237–265. 
Glasspool, D. W., & Houghton, G. (2005). Serial order and  consonant–vowel structure in a graphemic output buffer  model. Brain and language, 94(3), 304-330. 
McCloskey, M., Badecker, W., Goodman-Schulman, R. A.,  & Aliminosa, D. (1994). The structure of graphemic rep resentations in spelling: Evidence from a case of acquired  dysgraphia. Cognitive Neuropsychology, 11(3), 341–392.  
A resource model of phonological working memory 
Christopher R. Hepner (chepner3@jhu.edu) 
Department of Neurology, Johns Hopkins University, 
1629 Thames Street, Suite 350, Baltimore, MD 21231 USA 
Nazbanou Nozari (nozari@jhu.edu) 
Department of Neurology; Department of Cognitive Science, Johns Hopkins University, 1629 Thames Street, Suite 350, Baltimore, MD 21231 USA 
Abstract 
The classic Baddeley and Hitch (1974) model divides working  memory into domain-specific subsystems and a shared, do main-general central executive, which plays a role in allocating  resources to items stored in the subsystems. The nature of this  resource—in particular, its quantization (discrete vs. continu ous) and the flexibility of its allocation—has been studied ex tensively in the visual domain, with evidence from experiments  using continuous response measures providing support for  models with flexibly and continuously divisible resources. It  remains unclear, however, whether similar mechanisms medi ate the division of resources in phonological working memory.  In this paper, we show that, despite representational differences  between visual and auditory processing, continuous measures  can also be employed for studying phonological working  memory. Using such measures, we demonstrate that the prin ciples of resource division in visual and phonological pro cessing are indeed similar, providing evidence for a domain general mechanism for allocating working memory resources. 
Keywords: phonological working memory; cognitive re sources; central executive; domain-generality; resource mod els; slot models 
Introduction 
In the classic model proposed by Baddeley and Hitch (1974),  working memory has both domain-specific and domain-gen eral components: separate subsystems for verbal and visual  information (the phonological loop and visuospatial  sketchpad, respectively), with a shared central executive.  While the domain-specific properties of verbal and visual  working memory have been studied extensively, the function  of the central executive has remained obscure. There is some  consensus that the central executive plays a role in allocating  working memory resources to items stored in the domain 
specific subsystems; however, the exact nature of this opera tion has been hotly debated: Are resources discrete or con tinuous? Is there an upper limit on the number of items to  which resources can be allocated? Are resources divided  equally between items, or can higher-priority items receive a  larger share? These questions have been investigated exten sively in visual working memory, leading to significant sup port for models with flexibly and continuously divisible re sources (i.e., resource models) in the visual domain (Ma, Hu sain, & Bays, 2014). The evidence has come from experi ments in which, instead of binary accuracy, the deviation of  responses from the target has been measured, allowing the  quality of the stored representations to be investigated rather  
493
than just the quantity. For example, instead of probing  memory for colors with a choice between a limited set of dis crete values (e.g., prototypical red, orange, yellow, etc.) and  scoring the response as either correct or incorrect, allowing participants to select any hue on a continuous color wheel and  measuring the distance between that hue and the target. The  question remains: are resources divided the same way in the  verbal domain? This paper investigates this issue. Specifi cally, we examine whether a resource model is appropriate  for phonological working memory. 
Division of Resources 
Generally speaking, two classes of model have been proposed  for the division of resources: discrete and continuous. Slot  models (e.g., Cowan, 2001) propose that working memory re sources are discrete, divided up into a fixed number of slots.  Each slot can store exactly one item. When set size is less  than or equal to the number of slots, all items receive slots  and can be recalled with little or no error. However, when set  size exceeds the number of slots, some items do not receive  slots, and probing one of these items will result in a random  response. Slot models thus make identical predictions for  both error rates and the deviation of responses as a function  of set size: minimal until set size exceeds the number of slots,  then rising steeply. Resource models (e.g., Ma et al., 2014),  on the other hand, propose that resources are continuous and  can be divided between any number of items. The quality of  a stored representation is dependent on the amount of these  resources it receives: items receiving more resources can be  recalled with greater precision, i.e., less deviation from the  target response. Any increase in set size, even from 1 to 2  items (well below the capacity of any slot model), would  stretch the resources a little thinner and thus reduce the qual ity of the stored representations. Binary accuracy measures  may not be sensitive enough to detect this difference when  the set sizes are small (e.g., 1 vs. 2 items), since the quality  of the stored representations may still be sufficient to select  the correct response. This makes binary accuracy measures a suboptimal tool for distinguishing between slot and re 
source models. However, a continuous measure of the devi ation of the response from the target (i.e., the quality or pre cision of the response) can provide the necessary sensitivity.  Using such measures, Ma et al. (2014) showed that, in line  with the predictions of a resource model, the deviation be tween the target color and participants’ responses on a color  
wheel increased monotonically as a function of the number  of colors to be remembered. 
Within the framework of a resource model, the allocation  of resources may be either fixed, meaning that resources are  divided equally between all items to be remembered, or flex ible, meaning that one or more items may receive a larger  share of resources than the others, e.g., due to manipulation  of top-down attention. In keeping with the prediction of a  flexible resource model, experiments in which attentional  cues were manipulated (e.g., Gorgoraptis, Catalao, Bays, &  Husain, 2011) have shown that deviation scores are signifi 
cantly lower for prioritized items, and significantly higher for  those that have been deprioritized, compared to a neutral  baseline. 
In summary, evidence from the visual domain supports a  flexible version of the resource model. 
Visual and Verbal Representations 
In the previous section, we explained why distinguishing be tween slot and resource models requires a continuous meas ure of the deviation of a response from the target. This is easy  to obtain in the visual domain, since many of the features as sociated with visual representations can take any value on a  continuum, rather than a small set of discrete values. Im portantly, observers can often imagine “in-between” values  rather easily. For example, people can imagine a variety of greenish-blues and bluish-greens between the prototypical  colors blue and green. Similarly, people can imagine differ ent orientations between vertical and horizontal. Conse quently, the deviation of a response from the target can be  measured as the distance between the corresponding points  on the continuum. 
The task of identifying similar continua in the phonological  domain is more complex. Though the acoustic properties of  speech sounds also vary continuously, only variation that  crosses category boundaries is relevant to distinguishing be 
tween phonemes. For this reason, people tend to hear a /k/- ish /ɡ/ as either a /k/ or a /ɡ/, but not as something in between  (i.e., categorical perception; Liberman, Harris, Hoffman, &  Griffith, 1957). This tendency is not absolute, though. If  asked to rate a /k/-ish /ɡ/ on a continuous scale between /k/  and /ɡ/, participants are capable of doing so (Massaro & Co hen, 1983). This finding suggests that, despite surface differ ences in how people perceive visual and auditory infor mation, both information types are perceived in enough detail  to allow for fine-grained measurements of the deviation of a  response from the target. 
The Current Study 
In the current study, we used the syllable rating task from  Massaro and Cohen (1983) to obtain measurements of devia tion in phonological working memory. Using this paradigm allowed us to test whether the results in support of resource  models in the visual domain can be extended to auditory per ception. If so, we can conclude that the same domain-general  principles are at work in both visual and verbal domains at  the level of the central executive. If not, domain-specific  
494
models of resource division must be proposed. In Experi ment 1, we manipulated the number of syllables presented in  each trial to determine the relationship between set size and  the deviation of responses. In Experiment 2, we manipulated attentional cues while maintaining a constant set size in order  to determine the flexibility of resource allocation in phono logical working memory. 
Experiment 1 
Participants 
Forty-eight native speakers of American English (31 females,  Mage = 43.2, age range: 24-65 years) participated in an online experiment developed using jsPsych (de Leeuw, 2015) for  payment through Amazon Mechanical Turk (AMT;  https://www.mturk.com). 
Stimuli and Procedures 
Stimuli were 28 syllables, seven from each of four acoustic  continua: /bɑ/–/dɑ/, /kɑ/–/ɡɑ/, /ɹɑ/–/lɑ/, and /sɑ/–/ʃɑ/. The  syllables at the ends of the continua were recordings of a na tive speaker of American English. The five intermediate syl lables on each continuum were created by progressively  changing the acoustic properties of the initial consonant to  create five equally-spaced consonants between the two rec orded syllables while leaving the vowel unchanged. To min imize interference, a different distinctive feature was manip ulated in each continuum: [−coronal] vs. [+coronal], [−voice]  vs. [+voice], [−lateral] vs. [+lateral], and [+anterior] vs. [−an terior], respectively. Each participant completed two ses sions 24–72 hours apart with the same structure but a differ ent trial order. Each session consisted of two phases: a base line phase and a working memory phase. 
Baseline Phase The baseline phase was divided into four  blocks, one for each acoustic continuum. In each block, par ticipants first completed an orientation in which all seven syl lables along the continuum were played in order. As each  syllable was played, its position was shown on a slider visu ally representing the range between the most extreme sylla bles on the continuum (e.g., between the most /bɑ/-like sylla ble at the left end, labelled “B”, and the most /dɑ/-like sylla ble at the right end, labelled “D”). This orientation procedure  was repeated four times to give participants enough opportu nities to learn the relationship between the syllables on the  acoustic continuum and the corresponding positions on the  visual slider. 
Once the orientation was over, participants were tested on  their ability to rate syllables (baseline test). They listened to  the same syllables, presented in a random order, and indicated  the position of each one on the continuum using the slider.  Although there were only seven syllables in each continuum,  participants could adjust the slider continuously. Once par 
ticipants had adjusted the slider to their satisfaction, they  pressed a “submit” button and the position was recorded on a  scale from 1 to 100. Participants did not receive any feedback on their responses. Only one syllable was played in each trial  
and there was no deadline for responding. The baseline test  in each block consisted of 14 practice trials (two per syllable)  followed by 56 experimental trials (eight per syllable). Thus,  across the two sessions, participants completed a total of 448  experimental baseline test trials (16 for each of the seven syl 
lables in each of the four continua). 
Working Memory Phase This phase tested the effect of set  size on the deviation of responses from the target. On each  trial, participants were presented with a sequence of one, two,  or four syllables from different acoustic continua played at 1  s intervals. One second after the final syllable was played,  the slider appeared, and participants rated the relevant sylla 
ble on the slider. As in the baseline test, there was no dead line for responding. Since two syllables from the same con tinuum were never played during the same trial, the labels on  the slider unambiguously indicated which syllable to rate. In  each session, there were 15 practice trials, followed by 12  blocks of 28 experimental trials with pseudorandomized or der, such that no more than two consecutive trials had the  same set size. Across the two sessions, participants com pleted a total of 672 experimental working memory trials  (224 for each of the three set sizes). The design was fully  counterbalanced, so each syllable was probed the same num ber of times (eight) in each set size for each participant.  Within each set size, each syllable appeared the same number  of times in each position. 
Statistical Analyses 
Dependent Variable To measure the magnitude of the error  in the responses (and thus the precision), we obtained a “De viation Score” for each response made in the working  memory phase in three steps: (1) We calculated the median  
Results 
Baseline Test Figure 1 shows the distributions of partici pants’ ratings for the syllables in the /kɑ/–/ɡɑ/ continuum in  the baseline phase; the rating distributions for the other con tinua were similar. To determine whether participants had  been able to rate the syllables continuously, rather than cate gorically, we analyzed the ratings using uninformed mixture  modelling by means of the mclust package (version 5.3; Fra ley & Raftery, 2002) in R. If participants were rating the syl lables continuously, the overall distribution of the ratings  should be a mixture of seven distributions centered on or near  the “correct” rating for each syllable. The differences be tween the means of the model distributions and the correct  ratings were small: the root-mean-square deviation (RMSD)  was only 4.27. To formally test whether seven distributions  provided a better model for the data than two distributions  near the ends of the rating scale (as would be expected if par ticipants were rating categorically), we also fitted a model  with only two distributions and compared the fit of the two  models using the Bayesian information criterion (BIC),  which penalizes for additional parameters. The BIC for the  seven-distribution model (196,329) was much lower than the  BIC for the two-distribution model (201,838; a difference of  5,509), providing very strong evidence against the two-distri bution model. These results indicate that the participants  were able to perceive and rate the syllables continuously.  Next, we tested the effect of set size on ratings for the same  syllables in the working memory phase. 
Syllable 
1 
2 
of the participant’s 16 ratings for the same syllable in the  
y 
t
baseline phase. (2) We then subtracted this baseline median  
i
s
n
from the response. If, for example, the median of the partic e
D
ipant’s ratings was 30, the results for the responses 33 and 29  in the working memory phase would be 33 − 30 = 3 and 29 − 30 = −1, respectively. (3) Finally, we took the absolute value  of the number from (2) to get a Deviation Score for each re sponse. These Deviation Scores were the dependent variable  in both experiments. 
3 
4 
5 
6 
7 
0 50 100 Rating 
Statistical Models The main analyses in this study were car ried out with linear mixed-effects modeling (LMEM) using  the lme4 package (version 1.1-14; Bates, Mächler, Bolker, &  Walker, 2015) in R (version 3.4.2; R Core Team, 2017). We  strove to include the maximal random effects structure toler ated by the model. All numeric variables were centered and  scaled, and the dependent variable (the Deviation Score) was log-transformed to approximate a normal distribution. The  p-values were calculated based on Satterthwaite approxima tions using the lmerTest package (version 2.0-33; Kuz netsova, Brockhoff, & Christensen, 2016). 
495
Figure 1: Distribution of ratings for syllables in the /kɑ/– /ɡɑ/ continuum in the baseline test phase of Experiment 1. 
Working Memory Figure 2 shows the relationship between  set size and the Deviation Score at each position. Before an alyzing the effect of set size on deviation scores, we first es tablished that the bow-shaped serial position effect character istic of working memory performance was present in our  data: better performance at the beginning (primacy effect)  and end (recency effect) of the sequence compared to the  middle. We fitted a model to data from trials with set size  four (set sizes one and two are too small to allow for clear  testing of position effects) to test (a) whether the canonical  
position effects were obtained, and (b) which covariates  needed to be included in subsequent models to control for the  effects of nuisance variables. 
This model included position as a set of polynomial con trasts, with separate fixed effects for linear and quadratic (i.e.,  bow-shaped) serial position effects, along with random  
.001, (b) M = 0.57, 95% CI = [0.18, 0.97], p = .009, (c) M =  1.68, 95% CI = [1.05, 2.31], p < .001 and (d) M = 0.80, 95%  CI = [0.19, 1.44], p = .018. 
17 
16 
slopes for each of these by subject and item. The model also  
e 
r
included fixed effects for several nuisance variables: (a)  
o
c
15 
baseline median, which was the absolute value of the distance  
S
 
n
between the participant’s baseline rating for the syllable and  
o
i
14 
t
a
the center of the rating scale, to account for the reduction in  
i
v
variability at the ends of the scale (see the ratings for syllables  
e
13 
D
1 and 7 vs. syllable 4 in Figure 1); (b) baseline variability,  which was the standard deviation of the participant’s baseline  rating for the syllable; and (c) session, which was coded as a  contrast between the first and second sessions. We also in 
Set Size 
1 
2 
4 
1 2 3 4 
Position 
cluded random intercepts for participants and items, i.e., syl lables. Critically, there was a significant linear effect of po sition (t = −2.70, p = .012), indicating a decrease in Deviation  Score for more recent syllables, and a significant quadratic  effect of position (t = −4.46, p < .001), indicating higher De viation Scores in the middle of the sequence than at the ends.  There were also main effects of baseline median (t = −10.33,  p < .001), corresponding to a decrease in the Deviation Scores  closer to the ends of the rating scale, and baseline variability  (t = 10.17, p < .001), but not session (t = −0.66, p = .509).  These findings suggest that (a) Deviation Scores do indeed  reflect working memory performance, and (b) both baseline  median and baseline variability have significant influence on  Deviation Scores. We thus included these covariates in all  subsequent analyses. We also included serial position and its  interaction with set size because serial position alone could potentially have created spurious set size effects. 
The main prediction of the resource model investigated in  this experiment—that the Deviation Score would increase as  a function of set size—was tested using an LMEM with fixed  effects for set size, serial position, and the interaction be 
tween the two, along with the baseline median and baseline  variability as covariates. The random effect structure in cluded random intercepts for participants and syllables and  random slopes for set size, position, and the interaction be tween the two by participant and syllable. The model re vealed a significant main effect of set size (t = 5.32, p < .001),  with the Deviation Score increasing as a function of set size. 
There was no interaction between set size and position; however, to confirm that the effect of set size was robust  across positions, we conducted additional post-hoc analyses.  Four post-hoc tests compared: (a) the initial positions of set  sizes one and two, (b) the final positions of set sizes one and  two, (c) the initial positions of set sizes two and four, and (d)  the final positions of set sizes two and four. For each test, we  compared the mean difference in Deviation Scores between  
the two set sizes across participants to the distribution gener ated by a Monte Carlo simulation with 1,000,000 permuta tions, resampling within participants. After Bonferroni cor rection for multiple comparisons, the set size effect was sig nificant in all cases: (a) M = 0.76, 95% CI = [0.35, 1.18], p <  
496
Figure 2: Deviation Score as a function of set size and serial  position in Experiment 1. Error bars are 95% CIs. 
Discussion 
Analysis of the baseline phase confirmed that participants  were able to perceive and rate the syllables continuously, as  previously reported by Massaro and Cohen (1983). This  finding makes these materials appropriate for testing the pre 
dictions of a resource model. If such a model is appropriate  for phonological working memory, then the Deviation Scores  of the syllable ratings should increase as set size increases.  This increase in the Deviation Scores should be visible for  any increase in set size, even from one to two syllables, which  is well below the capacity limit proposed by any slot model.  The results of Experiment 1 confirmed this prediction. Devi 
ation Scores increased significantly as a function of set size.  This effect was robust in post-hoc analyses which took pri macy and recency effects into account by restricting compar isons between set sizes to matching positions. Thus, despite  the clear differences between visual and verbal stimuli, the  results of Experiment 1 closely resembled those found in the  visual domain (e.g., Bays, Catalao, & Husain, 2009; Wilken  & Ma, 2004), and were in full accord with the predictions of  a resource model. Experiment 2 tested whether a fixed or a  flexible version of the resource model is more appropriate for phonological working memory. 
Experiment 2 
Participants 
Forty-eight native speakers of American English (27 females,  Mage = 36.6, age range: 21–58 years) participated for payment  through AMT. 
Materials and Procedures 
The materials were the same as Experiment 1. The same two session design as Experiment 1 was used, with a similar ses sion structure. The baseline phase was unchanged. In the  working memory phase, the presence (or absence) and valid ity of cues appearing before the presentation of the syllables  
was manipulated. A fixed set size of four was used in all  trials. In one third of the trials, no cue was presented. These  no-cue trials were identical to the trials with set size four in  Experiment 1. On the other two thirds of the trials, a cue (a  number between 1 and 4) was presented at the beginning of  the trial. This cue indicated that the syllable in the corre 
sponding position (1 through 4) had a 50% chance of being  probed (one third of all trials; valid-cue trials). The other  syllables each has a 16.7% chance of being probed (one third of all trials; invalid-cue trials). 
On cued trials, the cue appeared for 1 s, after which the four  syllables were presented and one of them was probed in the  same way as in Experiment 1. Participants completed three  blocks of trials in the working memory phase: one block with  no-cue trials and two blocks with a mixture of valid- and in 
valid-cue trials, in counterbalanced order. Each block con sisted of 14 practice trials and 112 experimental trials, with  breaks between sets of 28 trials, for a total of 224 trials in  each cue condition (no cue, valid-cue, and invalid-cue) across  both sessions. Each syllable was probed exactly once in each  position in each block, resulting in a total of 8 samples for  each syllable in each cue condition from each participant. 
Results 
Deviation Scores were calculated in the same manner as be fore. Figure 3 shows the Deviation Scores as a function of  attentional cueing. 
19 
−4.52, p < .001). Interestingly, there was also a reliable in teraction between the valid-cue condition and position (t =  2.14, p = .037), but no such interaction between the invalid 
cue condition and position (t = −0.35, p = .729). To further explore the influence of cueing on serial position  effects, we fitted separate models for the valid-cue and no cue conditions with polynomial position contrasts for posi tion to test for both linear and quadratic (bow-shaped) serial  position effects. In the no-cue model, there were both signif icant linear (t = −5.02, p < .001) and quadratic (t = −2.38, p =  .023) effects of position, whereas only the linear effect was  significant (t = −3.34, p = .001) in the valid-cue model. 
Discussion 
The results of Experiment 2 closely mirrored those reported  in visual working memory (Gorgoraptis et al., 2011). Manip ulation of attention led to a significant decrease in Deviation  Scores for valid-cue trials (i.e., cued syllables), with a corre sponding decrease in precision for invalid-cue trials (i.e., the  uncued syllables presented in the same sequence as a cued  syllable), relative to no-cue trials. Cueing also reduced the  effect of serial position on recall, as evidenced by the signif icant interaction between the valid-cue condition and both  linear and quadratic effects of position. In particular, the pro totypical decrease in accuracy (or in this case precision) for  items in the middle of a list, which was present in the no-cue condition for this experiment, was eliminated by cueing. In  summary, the results of Experiment 2 showed that the distri bution of resources is flexible and can be influenced by atten tion. 
e 
r
o
c
S 
n
o
it
a
i
v
e
D
17 
15 
13 
1 2 3 4 Position 
Condition No cue 
Valid cue 
Invalid cue 
General Discussion 
In the visual domain, evidence from experiments measuring  the deviation of responses from the target instead of binary  accuracy has provided support for a resource model of work ing memory. Moreover, the division of such resources has  been shown to be flexible and subject to regulation through  top-down attention. Through the use of similar continuous measures in a phonological working memory task, we were  able to (a) verify the predictions of a resource model for pho 
Figure 3: Deviation Score as a function of cue condition and  serial position in Experiment 2. Error bars are 95% CIs. 
To test for effects of cue condition, we used an LMEM with  fixed effects for cue condition (contrast-coded as valid-cue  vs. no-cue and invalid-cue vs. no-cue), position, the interac tion between cue condition and position, and the same covari ates as Experiment 1. The random effect structure included  random intercepts for participants and syllables, along with  random slopes for cue condition, position, and the interaction  between the two by participant and syllable. Valid-cue trials  had significantly lower Deviation Scores (t = −3.34, p = .002)  and invalid-cue trials had significantly higher Deviation  Scores (t = 2.76, p = 0.008) compared to no-cue trials. There  was also a significant main effect of serial position, with De viation Scores decreasing for more recent syllables (t =  
497
nological materials, and (b) to show that, as in the visual do main, resources can be flexibly allocated, such that items pri oritized by top-down attentional cues will receive more re sources at the cost of those that are deprioritized. These re sults are consistent with a flexible resource model of phono logical working memory and, more generally, with a domain general mechanism of resource allocation operating on both  
visual and verbal domains, as proposed in Baddeley and  Hitch’s (1974) central executive. 
A key advantage of a resource model is its biological plau sibility: it has been proposed that the quality of the represen tations stored in working memory is proportional to the gain (amplitude of neural activity) of the populations of neurons  
encoding those representations (van den Berg, Shin, Chou,  George, & Ma, 2012). Due to the energy cost of maintaining  high gain, there is an upper bound on the total activation  across populations. Given these constraints, the optimal  
strategy for a resource allocation mechanism is to divide the  available gain between items in proportion to their relative  importance (e.g., based on attentional cues)—which would  produce precisely the pattern of performance observed in the  experiments described in this paper and previous experiments  in the visual domain. The identification of working memory  resources with neural gain also provides a natural explanation  for the relationship between working memory and attention,  in that attention has been shown to modulate neural gain (e.g.,  McAdams & Maunsell, 1999). This is consistent with the  claim that working memory is the subset of long-term  memory currently within the focus of attention (e.g., Cowan,  2001). 
One might object that the manipulation employed in the  current study does not reflect how people process language  in everyday life. While it is certainly true that the processing  of phonemes in the context of words and sentences involves  additional operations, the main point that these experiments 
make is that similar principles can explain the division of re sources in clearly separate domains of vision and auditory  verbal processing. Moreover, we have demonstrated that, as  in the visual domain, a continuous deviation score can be ob tained and used to measure the performance of phonological  working memory. Future work can take advantage of these  results and further explore the degree to which low-level in formation is retained in phonological vs. visual working  memory during processing of larger units like words and sen tences. 
Finally, while the current results do not speak directly to  another critical debate regarding the effect of temporal delay  vs. interference on working memory performance (e.g.,  Oberauer & Lewandowsky, 2008), the paradigm used in this  study easily lends itself to manipulations of time delay and  the similarity between stimuli that can help distinguish be 
tween decay and interference models. 
In conclusion, the results of this work shed light on the na ture of the central executive proposed by Baddeley and Hitch  (1974) by specifying a clearly defined, empirically falsifia ble, and biologically plausible mechanism for its operation:  the central executive divides resources continuously between  domain-specific representations that need to be held in work ing memory, and, in both visual and verbal domains, the par titioning of these resources is determined by the prioritization  of items in the attentional space. 
References  
Baddeley, A. D., & Hitch, G. (1974). Working memory. In  G. H. Bower (Ed.), Psychology of Learning and Motivation (Vol. 8, pp. 47–89). Academic Press. 
Bates, D., Mächler, M., Bolker, B., & Walker, S. (2015). Fit ting linear mixed-effects models using lme4. Journal of  Statistical Software, 67(1), 1–48.  https://doi.org/10.18637/jss.v067.i01 
Bays, P. M., Catalao, R. F. G., & Husain, M. (2009). The pre cision of visual working memory is set by allocation of a  shared resource. Journal of Vision, 9(10), 7–7.  https://doi.org/10.1167/9.10.7 
498
Cowan, N. (2001). The magical number 4 in short-term  memory: A reconsideration of mental storage capacity. Be havioral and Brain Sciences, 24(1), 87–114.  https://doi.org/10.1017/S0140525X01003922 
de Leeuw, J. R. (2015). jsPsych: A JavaScript library for cre ating behavioral experiments in a Web browser. Behavior  Research Methods, 47(1), 1–12.  https://doi.org/10.3758/s13428-014-0458-y 
Fraley, C., & Raftery, A. E. (2002). Model-Based Clustering,  Discriminant Analysis, and Density Estimation. Journal of  the American Statistical Association, 97(458), 611–631.  https://doi.org/10.1198/016214502760047131 
Gorgoraptis, N., Catalao, R. F. G., Bays, P. M., & Husain, M.  (2011). Dynamic updating of working memory resources  for visual objects. The Journal of Neuroscience, 31(23),  8502–8511. https://doi.org/10.1523/JNEUROSCI.0208- 
11.2011 
Kuznetsova, A., Brockhoff, P. B., & Christensen, R. H. B.  (2016). lmerTest: Tests in linear mixed effects models  (Version 2.0-33). 
Liberman, A. M., Harris, K. S., Hoffman, H. S., & Griffith,  B. C. (1957). The discrimination of speech sounds within  and across phoneme boundaries. Journal of Experimental  Psychology, 54(5), 358–368.  https://doi.org/10.1037/h0044417 
Ma, W. J., Husain, M., & Bays, P. M. (2014). Changing con cepts of working memory. Nature Neuroscience, 17(3),  347–356. https://doi.org/10.1038/nn.3655 
Massaro, D. W., & Cohen, M. M. (1983). Categorical or con tinuous speech perception: A new test. Speech Communi cation, 2(1), 15–35. https://doi.org/10.1016/0167- 6393(83)90061-4 
McAdams, C. J., & Maunsell, J. H. R. (1999). Effects of at tention on orientation-tuning functions of single neurons in  macaque cortical area V4. Journal of Neuroscience, 19(1),  431–441. 
Oberauer, K., & Lewandowsky, S. (2008). Forgetting in im mediate serial recall: Decay, temporal distinctiveness, or  interference? Psychological Review, 115(3), 544–576.  https://doi.org/10.1037/0033-295X.115.3.544 
R Core Team. (2017). R: A language and environment for  statistical computing (Version 3.4.1). Vienna, Austria: R  Foundation for Statistical Computing. 
van den Berg, R., Shin, H., Chou, W.-C., George, R., & Ma,  W. J. (2012). Variability in encoding precision accounts for  visual short-term memory limitations. Proceedings of the  National Academy of Sciences, 109(22), 8780–8785.  https://doi.org/10.1073/pnas.1117465109 
Wilken, P., & Ma, W. J. (2004). A detection theory account  of change detection. Journal of Vision, 4(12), 11.  https://doi.org/10.1167/4.12.11 
How to use context to disambiguate overlapping categories: The test case of Japanese vowel length 
Kasia Hitczenko1, Reiko Mazuka2,3, Micha Elsner4 & Naomi H. Feldman1,5 khit@umd.edu, mazuka@brain.riken.jp, melsner@ling.osu.edu, nhf@umd.edu 
1Department of Linguistics, University of Maryland 
2RIKEN Brain Science Institute, Laboratory for Language Development 
3Department of Psychology and Neuroscience, Duke University 
4Department of Linguistics, The Ohio State University 
5Institute for Advanced Computer Studies, University of Maryland 
Abstract 
Infants learn the sound categories of their language and adults 
successfully process the sounds they hear, even though sound 
categories often overlap in their acoustics. Most researchers 
t 
n
agree that listeners use context to disambiguate overlapping cat egories. However, they differ in their ideas about how context 
u
o
C
is used. One idea is that listeners normalize out the systematic 
effects of context from the acoustics of a sound. Another idea 
is that contextual information may itself be an informative cue 
to category membership, due to patterns in the types of contexts 
Long 
Short 
Log Duration 
that particular sounds occur in. We directly contrast these two ways of using context by applying each one to the test case of Japanese vowel length. We find that normalizing out contextual variability from the acoustics does not improve categorization, but using context in a top-down fashion does so substantially. This reveals a limitation of normalization in phonetic acquisi tion and processing and suggests that approaches that make use of top-down contextual information are promising to pursue. 
Keywords: speech perception; phonetic category acquisition 
One of the first tasks infants face when acquiring their native language is learning what its sound categories are, a task that involves grouping sounds that vary continuously into discrete categories. Even once people have learned their language and its sound categories, they still need to be able to map a particular acoustic pronunciation they hear to one of those categories, in order to process speech effectively. These can be difficult tasks because there is often a lot of overlap between categories in terms of how they are acoustically realized (Bion, Miyazawa, Kikuchi, & Mazuka, 2013), and this overlap can mask which sounds should be grouped together. 
A prime example of this is Japanese vowel length, the test case we consider in this paper. In Japanese, vowel length is contrastive: whether a vowel is phonologically short or long can change the meaning of a word (e.g. /biru/ means building, but /bi:ru/ means beer). Short and long vowels are separate sound categories, yet analyses have shown that they overlap substantially in their durations.1 That is, a particular production of a phonologically short vowel can be longer than a particular production of a phonologically long vowel. In fact, because only 9% of Japanese vowels are long, the combined distribution of vowels is unimodal (Figure 1). Cases like this one are problematic for classic distributional learning approaches, which posit that listeners make use of clusters 
1We use vowel length to refer to the phonological status of a vowel and vowel duration to refer to the physical acoustic property of a vowel (i.e. how long the speaker took to produce that sound). It is thought that vowel duration is the main acoustic cue to vowel length. 
499
Figure 1: Acoustic distribution of Japanese vowel duration in spontaneously produced infant-directed speech. Data are from the R-JMIC corpus, as described in the Data section. 
or peaks in the acoustic data they hear to learn and process sounds (Maye, Werker, & Gerken, 2002; Bion et al., 2013). How do infants learn the sound categories of their language and how do adults process the sounds of their language when there is so much overlap in the speech they hear? A large body of work has argued that listeners use context to disambiguate sound categories, but researchers differ in their ideas about how context is used. One idea is that the context a sound occurs in systematically affects how that sound is produced and causes overlap in sound categories. Listeners then factor out the effect of context from the acoustics (‘normalization’ or ‘adaptation’) (e.g. McMurray & Jongman, 2011; Dillon, Dunbar, & Idsardi, 2013; Kleinschmidt & Jaeger, 2015). An other idea, which is not mutually exclusive from the first, is that sound categories differ in the types of contexts or environ ments they are likely to occur in for phonotactic, lexical, and other reasons. Listeners use this top-down information about which sound is most likely to occur in the context they heard to supplement acoustic cues (e.g. Ganong, 1980; Feldman, Griffiths, Goldwater, & Morgan, 2013). These ideas have been studied extensively; however, we have limited knowledge of the extent to which each one is effective on realistic data. In this work, we directly contrast these two ways of using context by testing their relative efficacy in separating over lapping vowel length categories in Japanese. We show that factoring out context from the acoustic cues does not improve category separability, while using context as a direct cue par allel to acoustic cues does - and substantially. This result reveals limitations in the efficacy of factoring out systematic variability, and suggests that approaches that make use of top down contextual information are promising to pursue in future research. 
Context in Phonetic Perception 
This paper contrasts two ideas for how context could be used in the acquisition and processing of overlapping sound categories. The first is based on the idea that contextual factors (broadly construed to include speaker, neighboring sounds, speech rate, etc.) systematically and predictably affect the acoustic real ization of particular sounds, causing the observed overlap between different categories. The idea is that context can be “factored out” of the acoustics, in order to reduce category over lap. Listeners might do this either by learning the structure of the variability and undoing its effect (‘normalization’), or by building a separate model of the mapping between acoustics and categories for each context a sound occurs in (‘adapta tion’). Both normalization and adaptation have considerable scientific support. A body of experimental work has convinc ingly shown that listeners’ perception of a particular sound can be changed by modifying the speaker (Nearey, 1978), the neighboring sounds (Mann & Repp, 1980), or the speech rate (Fujisaki, Nakamura, & Imoto, 1975) of the surrounding ut terance. This set of findings has generally been interpreted as support for the idea that listeners take into account systematic variability when making categorization decisions - at least on controlled lab or synthetic speech. These findings have been supplemented by computational work, which has found that models that take into account systematic variability achieve better matches with human performance than models that do not, both in adult categorization (McMurray & Jongman, 2011) and sound category acquisition (Dillon et al., 2013). 
The second idea, which is not mutually exclusive from the first, is that sound categories differ in the types of contexts or environments they are likely to occur in for phonotactic, lexi cal, historical and other reasons. Just knowing the context of a target sound, then, could be informative about what category it is likely to be. Listeners might supplement bottom-up acoustic cues with this type of top-down contextual information, when learning and categorizing the sounds they hear. Indeed, exper imental work has shown that participants in speech perception experiments are biased to choose sound categorizations that result in words over non-words (Ganong, 1980), as well as phonotactically legal sequences over sequences that violate phonotactic constraints (Brown & Hildum, 1956). Feldman et al. (2013) showed that a computational model that used information about the word frames that sounds occurred in resulted in an improvement in sound category learning over models that did not incorporate lexical information. 
The literature on these two ideas is extensive, but is not conclusive on what role each of these strategies plays in ac quisition and processing. Many of the studies that are cited as classical evidence for factoring out systematic variability are consistent with using top-down linguistic information, and vice versa. As an example, Port and Dalby (1982) showed that whether participants perceived a particular stimulus as being the word rapid or rabid changed depending on the duration of the vowel that preceded the /p/ or /b/. This finding was origi nally taken as evidence that participants were normalizing the 
acoustics for speech rate. However, it was later considered evidence that participants were using the duration of the vowel as a direct cue to determining the identify of the consonant (Toscano & McMurray, 2012). Because these ideas have been somewhat conflated in the literature, it is hard to evaluate their relative contribution to acquisition and processing. In addi tion, because most of the evidence for these ideas comes from work on synthetic or controlled lab speech, we have limited knowledge about whether they are also effective on more nat uralistic speech. This paper isolates the two ideas and tests their relative efficacy in separating overlapping categories, by applying them to the Japanese vowel length contrast. 
Japanese vowel length is an ideal test case because there is evidence that both of these strategies could be helpful in overcoming the overlap between short and long vowels. On the one hand, research has shown that factors such as vowel quality (Hirata, 2004; Bion et al., 2013), speech rate (Hirata, 2004), prosodic position (Martin, Igarashi, Jincho, & Mazuka, 2016), and neighboring sounds (Hirata & Whiton, 2005) all affect the duration of Japanese vowels. It is possible that these factors could cause overlap between short and long vowels, in which case factoring out the effect of context would be effective. On the other hand, there is also evidence that there are systematic differences between short and long vowels in the types of contexts and environments that they occur in. For example, different vowel qualities (a, e, i, o, u) have different relative proportions of short and long vowels, short and long vowels differ in the types of sounds they co-occur with (Hirata, 2004), and long vowels are less likely to occur phrase-finally in some strata of the Japanese lexicon (Moreton & Amano, 1999). Listeners could exploit these contextual patterns in a top-down process to better process and learn the contrast. 
In what follows, we compare the relative efficacy of these two strategies in separating overlapping categories by testing how well each of them categorizes Japanese vowels as short or long. Analysis 1 tests whether normalization improves cate gorization performance. Following Cole, Linebaugh, Munson, and McMurray (2010) and McMurray and Jongman (2011), and Nearey (1990), we implement the idea of factoring out systematic variability by regressing out contextual variability from acoustic cues. We then test whether a logistic regression categorization model that uses normalized cues outperforms ones that use unnormalized cues. Analysis 2 tests whether us ing top-down contextual information improves categorization performance, by comparing a logistic regression that only uses acoustic cues to ones that also use contextual factors as direct predictors of category membership. 
We choose to implement factoring out systematic variability as normalization rather than adaptation because normalization allows us to isolate the two ways of using context in a way that adaptation does not.2 
2Adaptation builds separate models for each context a sound occurs in, allowing it to make use of top-down information about how likely each category is to occur in a particular context, in addition to factoring out systematic acoustic variability. 
500
Data 
The data we use come from the RIKEN Japanese Mother Infant Conversational Corpus (Mazuka, Igarashi, & Nishikawa, 2006). The data were originally collected by recording the speech of 22 mothers who visited the lab with their 18- to 24-month old children. The mothers first played with their child with picture books. They then played with their child with toys. Speech by the mother in these two sessions was labelled as infant-directed. In the final session, the mothers talked to a female experimenter and the mother’s speech in this session was labelled as adult-directed. The corpus consists of about 14 total hours of speech, and is hand-labelled for both phonetic and prosodic information. 
We extracted information about each of the vowels produced by the mothers, but excluded singing, coughing, devoiced vowels, diphthongs, and any segments that the researchers could not transcribe. This left 92003 total vowels, 30035 of which were in the adult-directed section of the corpus and 61968 of which were in the infant-directed section of the corpus. All of the analyses we report were run on the infant directed part of the corpus, plotted in Figure 1.3 
Acoustic cues 
We extracted acoustic information about each vowel: • Duration: We extracted vowel duration in seconds. • Formants: Although, up to this point, we have only dis 
cussed duration, previous work has shown that spectral in formation can improve categorization performance (Hirata, 2004). As a result, we used the first three formants at the vowel midpoint which were automatically extracted using Praat (Boersma, 2001) by Antetomaso et al. (2017). 
Contextual information 
We also extracted a set of contextual factors about each vowel: • Vowel quality: This was a categorical variable that took one of five values: /a/, /e/, /i/, /o/, /u/. 
• Speaker: This was a categorical variable with one of 22 different possible values. 
• Accented?: This was a binary variable that took a value of 1 if the vowel was accented and 0 if it was not. • Condition: This variable indicated whether the mother uttered the vowel to their child while they were playing with books or with toys. 
• Prosodic position: We extracted a categorical variable that indicated whether the word that the vowel occurred in was at the end of an accentual phrase (AP), at the end of an intonational phrase (IP), at the end of an utterance, or none of the above. We extracted a second categorical variable, which indicated whether the word that the vowel was in was AP-initial, IP-initial, utterance-initial, or none of the above. Finally, we extracted a vector of binary variables, of which the first three elements indicated the position of the vowel in the word (initial, medial, final), the next three its position 
3We also ran these analyses on the adult-directed part of the corpus and found comparable results. 
in its AP, the next three its position in its IP, and the last three its position in the utterance. Unlike the previous two variables, this one marked the position of the vowel itself rather than its containing word. 
• Speech rate: We extracted the average syllable duration of both the word and utterance the vowel was in (pauses were excluded in calculations). We also extracted the duration of the previous (and following) sound. For vowels without immediately preceding (or following) sounds, we used the overall average previous (and following) duration. 
• Neighboring sound: We extracted the quality of the previ ous and following sounds, as well as whether or not they were geminate consonants. 
• Part-of-speech: We coded whether each vowel was in a function or a content word based on part-of-speech annota tions in the corpus. 
Analysis 1: Removing Systematic Variability In this section, we test to what extent normalizing out system atic variability from acoustics can help disambiguate short and long vowels. 
Methods 
We compare categorization models that make use of normal ized acoustic cues to ones that make use of unnormalized acoustic cues. The unnormalized cues are the duration and formants taken directly from corpus annotations. To obtain the normalized acoustic cues, we train a linear regression model to predict each vowel’s acoustic cues (duration and formants) from its context (speech rate, neighboring sounds, etc., as listed under Contextual Information). The model’s predic tion represents what we expect the vowel’s acoustic cues to be based on its context. Once we have these predictions, we calcu late each vowel’s normalized acoustic cues by subtracting that vowel’s predicted acoustic cues from its actual acoustic cues (i.e. by taking the residuals). This step effectively subtracts out the influence the context had on each vowel’s acoustic cues. We vary whether or not we factor out the effect of part of-speech from the acoustics. We want our analyses to apply to both acquisition and adult speech perception and because infants probably do not have access to part-of-speech informa tion when learning about vowel length, we test how effective normalization is both with and without part-of-speech. 
Once we have the normalized cues, we train logistic re gression models to predict each vowel’s length either from its unnormalized or its normalized acoustic cues. These logistic regressions take the input acoustic cues and output the relative probability that the vowel is short or long. The vowel is cate gorized as belonging to the category with higher probability. 
To train the linear and logistic regressions, the data are divided into a training set (90% of the data) and a test set (10% of the data), keeping the relative proportion of short and long vowels constant across the sets. For normalization, the linear regression equation is estimated on the training set, but is used to normalize the acoustic cues in both the training and test set. The logistic regression is trained on the same training set as 
501
Model Accuracy Short Accuracy Long Accuracy BIC Model 1: Unnormalized baseline 91.0 98.8 13.4 28698 Model 2a: Normalized (no part-of-speech) 91.0 99.5 6.1 31552 Model 2b: Normalized (with part-of-speech) 91.0 99.5 5.8 31644 Model 3a: Top-down information (no part-of-speech) 95.1 99.0 60.0 17784 Model 3b: Top-down information (with part-of-speech) 95.3 99.0 61.6 16760 
Table 1: Summary of results from Analysis 1 and 2. Analysis 1 compared the Unnormalized/Baseline model to the two Normalized models. Analysis 2 compared the Unnormalized/Baseline model to the two Top-down information models. 
the linear regression (except that the acoustic cues are now normalized). In order to make sure that the models performed consistently, we randomly split the data into training and test sets 10 separate times, ran each model ten times, and averaged performance across these ten runs. 
We report two types of evaluation metrics for each model we present. First, we report overall categorization accuracy on the unseen test set, which is the percentage of all vowels in the test set that the model categorized correctly, as well as the accuracy on just short vowels and just long vowels. Second, we report the Bayesian Information Criterion (BIC) for each model, which is a common metric used to select between different models. It has the property that it prefers simpler models, all else being equal, and lower values are better. 
Results 
A summary of the results is presented in Table 1. 
Unnormalized Model (Model 1) The baseline model used unnormalized duration and formants as predictors of category membership, without regressing out the effects of context. This logistic regression model reached an overall accuracy of 91.0%. It got 98.8% of short vowels correct and 13.4% of long vowels correct. It had a BIC of 28698. While the overall accuracy and the accuracy on short vowels seem quite impressive, a model could get 90.9% accuracy just by guessing ‘short’ for every vowel token. 
Normalized Models (Models 2a-2b) These two models both used normalized duration and formants as predictors of categorical membership, but we varied whether we regressed out part-of-speech. Without regressing out part-of-speech, the model reached an overall accuracy of 91.0%. It got 99.5% of the short vowels correct, and 6.1% of the long vowels cor rect. It had a BIC of 31552. With part-of-speech regressed out, the model reached an overall accuracy of 91.0%, getting 99.5% of the short vowels correct, and 5.8% of the long vow els correct. It had a BIC of 31644. Both models showed worse performance compared to the unnormalized model, with lower accuracy on long vowels and an increase in BIC. Normalizing out the effect of part-of-speech resulted in worse performance than ignoring its effect. 
Discussion 
Previous work suggests that factoring out systematic variabil ity could be helpful in the acquisition and processing of the Japanese vowel length contrast; however, our current results 
did not support this hypothesis. Our main finding was that normalization did not increase separability between short and long vowels, as evidenced by no improvement in categoriza tion. Given how prevalent this hypothesis has been in the literature, the results are surprisingly bad and suggest that normalization in the duration and formant cues may not be the solution to the Japanese vowel length contrast problem. We return to the question of why we observed these results in the General Discussion. 
Analysis 2: Using Top-down Information 
In this section, we test to what extent making use of top-down contextual information, in addition to bottom-up acoustics, could help in the acquisition and processing of Japanese vowel length. In this analysis, context is used to directly predict the identity of a vowel, rather than to predict its acoustics, as was done in Analysis 1. 
Methods 
As in the previous analysis, we use logistic regression models as our categorization models, but in this analysis, we do not run any linear regressions to regress out context from acoustics. The baseline model was identical to Model 1 in Analysis 1. This was compared against two logistic regression models that predicted vowel length from unnormalized acoustic cues, plus the contextual factors described in the Data section. As before, we varied whether part-of-speech was included. The models were again evaluated using overall, short vowel, and long vowel accuracies, as well as BIC. 
Results 
The results are summarized in Table 1. 
Baseline Model (Model 1) The baseline model was identi cal to the unnormalized model in Analysis 1 (see Table 1). 
Top-Down Information Models (Models 3a-b) In these two models, in addition to using unnormalized duration and formants to predict vowel length, we also used contextual fac tors as direct predictors of vowel length. When we did not include part-of-speech, the model achieved 95.1% accuracy overall, getting 99% of short vowels right, and 60% of long vowels right. It had a BIC of 17784. With part-of-speech, the model had 95.3% overall accuracy, getting 99% of short vowels right and 61.6% of long vowels right. It had a BIC of 16760. Both of these models substantially outperformed the baseline model, while adding part-of-speech as a predictor 
502
additionally improved performance, lowering BIC and increas ing long vowel and overall accuracy. 
Discussion 
We investigated to what extent using contextual information as a direct cue to vowel length, in addition to acoustic cues, helped in categorization. We found that this strategy drastically improved accuracy and lowered BIC scores, suggesting that this method separates short and long vowels quite well. Given the relatively small set of factors we used, it is quite impressive that we achieved this level of performance, and it suggests that this is a promising strategy to pursue in future work. 
General Discussion 
This paper applied two ideas about how listeners might use context in overcoming the overlapping category problem to the test case of Japanese vowel length. Results showed that normalizing the effect of context out of the acoustic cues did not improve short and long vowel separability. On the other hand, using contextual information as a direct cue to cate gory membership resulted in much better separability between categories, as evidenced by an improvement in categorization. 
The fact that normalization was not helpful was surprising, given how well-established this idea is in the field. The mod els we tested were supervised, and were given information on what the vowel categories and relevant contextual factors were, yet were still unable to separate short and long vowels. The problem would be even greater in acquisition, where the learner would need to simultaneously learn what the categories are and how to factor out context from the acoustics. 
Previous work using the same normalization techniques that we employ here found improvements in accuracy. Although it is difficult to directly compare improvement based on accu racy, two past studies reported increases of performance from 28.63% to 54% (Cole et al., 2010) and from 83.3% to 92.9% (McMurray & Jongman, 2011). There are a few reasons why we may have found different normalization performance. 
First, in our case, there were many more short vowels than long vowels, whereas in previous work, the categories were more balanced. Binary classifiers can perform poorly on imbal anced data (He & Garcia, 2009), so it is possible that the bad performance we observe is not due to normalization, but rather to the categorization model. Although this is a legitimate con cern, the categorization model in Analysis 2 achieves much better categorization, which makes this explanation unlikely. What is more likely is that imbalances in the proportion of long and short vowels differed across contexts, and this affected the results. Imbalances in a particular context - precisely the signal that top-down models use - can impede normalization by artificially shortening or lengthening the mean duration of vowels in a context. 
Second, it is possible that we are not factoring out the right set of contextual factors for the Japanese vowel length case. We only considered a small number of contextual factors, and many of the factors we included were quite basic compared to the complicated processes they represent. For example, we 
reduced all of the complexity of pitch accent to a single binary variable. It is possible that representing the full complexity of all of the relevant factors would improve results. However, the fact that we were unable to include more sophisticated factors does not explain the lack of improvement from what was included. The model had access to many factors that have been shown to systematically affect vowel duration in Japanese, yet factoring them out did not improve results. 
Third, it is possible that there was a problem with the par ticular implementation of normalization we used. While this implementation has been effective in other cases (Cole et al., 2010; McMurray & Jongman, 2011), it has not been applied to spontaneous speech before, and it may be unable to capture the structure of the contextual variability. We did not include in teraction terms between factors in the linear regression as this was computationally difficult, yet research has revealed that these interactions exist. For example, vowels are lengthened more in slow speech than in fast speech. Linear regression models also assume that particular contextual factors add or subtract a fixed duration, but it is possible that these factors af fect duration in a different way (e.g. in a multiplicative fashion by, for example, doubling the duration of the vowel). A more complex normalization model might be more successful, by better capturing the relationship between context and acoustics However, as before, the fact that a more complex model could improve performance fails to explain why normalization did not help here. Previous work has shown that individual fac tors such as vowel quality, speech rate, and prosodic position each have systematic effects on acoustics that are evident even when variability in other factors is not controlled for. Our model can normalize out these individual effects, but doing so did not improve performance. 
Fourth, it is possible that factoring out systematic variability is not a strategy that would work for the particular case of the Japanese vowel length contrast. For instance, it may be the case that top-down information is sufficient to distinguish most long/short minimal pairs without attending to the acoustic duration at all, so that in conversational speech, the durational contrast is mostly neutralized. Under this account, factoring out systematic variability would work for contrasts with high functional load, where speakers must produce a perceptible contrast in order to be understood, but might be ineffective for contrasts with low functional load, where speakers might not produce an acoustic contrast all the time. Further research is needed on what cues adult listeners use to distinguish Japanese long and short vowels, and in general, what sorts of problems can be solved by factoring out variability. 
Finally, it is possible that factoring out systematic variability is not effective for spontaneously produced speech. Although a body of work has argued that listeners do factor out sys tematic variability from the acoustics (McMurray & Jongman, 2011; Mann & Repp, 1980), most of this work has studied carefully controlled laboratory speech or synthetic speech, in stead of spontaneous speech, and typically manipulated the influence of one contextual factor at a time. It is possible 
503
that in spontaneous speech, which has been shown to be quite different than laboratory speech (Wagner, Trouvain, & Zim merer, 2015), there are so many individual factors involved and interacting with one another that normalization becomes ineffective. However, this idea is difficult to reconcile with the automatic speech recognition literature, where speaker and speech rate are taken into account in many systems, and are often used when dealing with real, messy speech. Pursuing this possibility would require revisiting some previous work that showed improvements from normalization. 
Overall, we do not yet have enough evidence to make a strong claim about which of these (and other) possibilities is correct. In ongoing and future work, we will test the efficacy of normalization in other test cases of overlapping categories, on Japanese laboratory speech, with more and more sophisticated contextual factors, and we will use neural networks to imple ment more sophisticated normalization techniques. This will allow us to better understand the pattern of results observed here, as well as allow us better delineate when factoring out systematic variability is helpful and when it is not. 
In Analysis 2, we showed that an alternative hypothesis where the factors were used as independent predictors of vowel length resulted in improved categorization performance. Given the small set of factors we included, it is quite impressive that we are able to correctly classify around 99% of the short vowels, as well as over 60% of the long vowels, given just how much they overlap along the duration dimension. At minimum, this should be taken as evidence that it is possible to improve categorization performance substantially, even when the base categorization rate is over 90% due to a high rate of short vowels. More strongly, this suggests a promising alternative to consider in future work. Although our use of supervised models means we cannot draw strong conclusions about acquisition, these results show that there is signal in the data that could be exploited. Ultimately, if we can show that short vowels have quite different distributions than long vowels when we include non-acoustic information, then we can start to study unsupervised versions of this model. 
In this paper, we contrasted two ways of using context to overcome the overlapping categories problem: factoring out systematic variability arising from the context and using con textual information as a direct cue to category membership in a top-down fashion. Our intention is not to imply that the true solution is one or the other, but rather to study the rela tive contribution of each of these hypotheses separately. Our results call into question the idea of factoring out systematic variability on its own. It may still be useful when combined with other ideas, and future research should consider strategies, like adaptation, that integrate both ideas. 
Acknowledgments This work was supported by NSF grants IIS 1421695, IIS-1422987, DGE-1449815, and NSF/JSPS EAPSI grant 1713974. We thank the reviewers, Adam Albright, Stephanie Ante tomaso, Edward Flemming, Bill Idsardi, Chiyuki Ito, Kyoji Iwamoto, Jeff Lidz, Thomas Schatz, Kristine Yu, the RIKEN Lab for Language Development, the MIT Computational Psycholinguistics Lab, the MIT Phonology Circle, the MIT CompLang group, NECPhon 11, and Brown University LingLangLunch for their help and feedback. 
References 
Antetomaso, S., Miyazawa, K., Feldman, N., Elsner, M., Hitczenko, K., & Mazuka, R. (2017). Modeling phonetic category learning from natural acoustic data. In BUCLD 41: Proceedings of the 41st Annual Boston University Conference on Language Development. 
Bion, R. A., Miyazawa, K., Kikuchi, H., & Mazuka, R. (2013). Learning phonemic vowel length from naturalistic recordings of Japanese infant-directed speech. PloS ONE, 8(2), e51594. 
Boersma, P. (2001). Praat: A system for doing phonetics by computer. Glot International, 5(9/10), 341-345. 
Brown, R. W., & Hildum, D. C. (1956). Expectancy and the percep tion of syllables. Language, 32(3), 411-419. 
Cole, J., Linebaugh, G., Munson, C., & McMurray, B. (2010). Unmasking the acoustic effects of vowel-to-vowel coarticulation: A statistical modeling approach. Journal of Phonetics, 38(2), 167-184. 
Dillon, B., Dunbar, E., & Idsardi, W. (2013). A single-stage ap proach to learning phonological categories: Insights from Inuktitut. Cognitive Science, 37(2), 344-377. 
Feldman, N. H., Griffiths, T. L., Goldwater, S., & Morgan, J. L. (2013). A role for the developing lexicon in phonetic category acquisition. Psychological Review, 120(4), 751-778. 
Fujisaki, H., Nakamura, K., & Imoto, T. (1975). Auditory perception of duration of speech and non-speech stimuli. Auditory Analysis and Perception of Speech, 197-219. 
Ganong, W. F. (1980). Phonetic categorization in auditory word per ception. Journal of Experimental Psychology: Human Perception and Performance, 6(1), 110-125. 
He, H., & Garcia, E. A. (2009). Learning from imbalanced data. IEEE Transactions on Knowledge and Data Engineering, 21(9), 1263-1284. 
Hirata, Y. (2004). Effects of speaking rate on the vowel length distinction in Japanese. Journal of Phonetics, 32(4), 565-589. Hirata, Y., & Whiton, J. (2005). Effects of speaking rate on the 
single/geminate stop distinction in Japanese. The Journal of the Acoustical Society of America, 118(3), 1647-1660. 
Kleinschmidt, D. F., & Jaeger, T. F. (2015). Robust speech perception: Recognize the familiar, generalize to the similar, and adapt to the novel. Psychological Review, 122(2), 148-203. 
Mann, V. A., & Repp, B. H. (1980). Influence of vocalic context on perception of the [S]-[s] distinction. Attention, Perception, & Psychophysics, 28(3), 213–228. 
Martin, A., Igarashi, Y., Jincho, N., & Mazuka, R. (2016). Utterances in infant-directed speech are shorter, not slower. Cognition, 156, 52-59. 
Maye, J., Werker, J. F., & Gerken, L. (2002). Infant sensitivity to distributional information can affect phonetic discrimination. Cognition, 82(3), B101-B111. 
Mazuka, R., Igarashi, Y., & Nishikawa, K. (2006). Input for learning Japanese: RIKEN Japanese mother-infant conversation corpus. The Technical Report of the Proceedings of the Institute of Elec tronics, Information and Communication Engineers, 106(165), 11-15. 
McMurray, B., & Jongman, A. (2011). What information is necessary for speech categorization? Harnessing variability in the speech signal by integrating cues computed relative to expectations. Psy chological Review, 118(2), 219-246. 
Moreton, E., & Amano, S. (1999). Phonotactics in the perception of Japanese vowel length: Evidence for long-distance dependencies. In Eurospeech. 
Nearey, T. M. (1978). Vowel space normalization in synthetic stimuli. The Journal of the Acoustical Society of America, 63(1). Nearey, T. M. (1990). The segment as a unit of speech perception. Journal of Phonetics, 18(3), 347-373. 
Port, R. F., & Dalby, J. (1982). Consonant/vowel ratio as a cue for voicing in English. Attention, Perception, & Psychophysics, 32(2), 141-152. 
Toscano, J. C., & McMurray, B. (2012). Cue-integration and context effects in speech: Evidence against speaking-rate normalization. Attention, Perception, & Psychophysics, 74(6), 1284-1301. 
Wagner, P., Trouvain, J., & Zimmerer, F. (2015). In defense of stylistic diversity in speech research. Journal of Phonetics, 48, 1-12. 
504
Effectively Learning from Pedagogical Demonstrations 
Mark K Ho (mark ho@brown.edu) 
Department of Cognitive, Linguistic, and Psychological Sciences, 190 Thayer Street Providence, RI 02906 USA 
Michael L. Littman (mlittman@cs.brown.edu) 
Department of Computer Science, Brown University, 115 Waterman Street Providence, RI 02912 USA 
Fiery Cushman (cushman@fas.harvard.edu) 
Department of Psychology, 1484 William James Hall, 33 Kirkland St. Cambridge, MA 02138 USA 
Joseph L. Austerweil (austerweil@wisc.edu) 
Department of Psychology, University of Wisconsin-Madison, 1202 W Johnson Street Madison, WI 53706 USA 
Abstract 
When observing others’ behavior, people use Theory of Mind to infer unobservable beliefs, desires, and intentions. And when showing what activity one is doing, people will modify their behavior in order to facilitate more accurate interpretation and learning by an observer. Here, we present a novel model of how demonstrators act and observers interpret demonstrations corresponding to different levels of recursive social reasoning (i.e. a cognitive hierarchy) grounded in Theory of Mind. Our model can explain how demonstrators show others how to per form a task and makes predictions about how sophisticated ob servers can reason about communicative intentions. Addition ally, we report an experiment that tests (1) how well an ob server can learn from demonstrations that were produced with the intent to communicate, and (2) how an observer’s interpre tation of demonstrations influences their judgments. Keywords: Theory of Mind; Communicative Intent; Cogni tive Hierarchy; Reinforcement Learning; Bayesian Pedagogy 
Introduction 
People often learn by observing others’ demonstrations. Con sider learning how to tie your shoes. It would be difficult to learn shoe tying through trial-and-error, which is why we usually learn how to do it from others. However, by itself, being in the presence of social others who are adept at ty ing shoes is insufficient: imagine trying to learn to tie your shoes by only examining finished knots or briefly watching as someone ties their shoes before rushing out the door. That would be difficult. Instead, people often engage in teaching interactions in which a demonstrator intentionally communi cates the structure of a task or skill while an observer intently watches, aware of the demonstrator’s pedagogical aims. The demonstrator, to better teach, might modify their behavior to better disambiguate a task, while the observer, to properly learn, might interpret actions in light of these teaching goals to draw better inferences. This form of interaction supports learning in a variety of domains, from learning everyday tasks like shoe tying to complex technical skills to nuanced social norms. Understanding the cognitive processes that support this capacity is thus critical for a painting a complete pic 
ture of folk pedagogy and cultural learning (Tomasello et al., 2005; Boyd, Richerson, & Henrich, 2011). 
We examine learning from demonstration from the per spective of Theory of Mind (Dennett, 1987; Baker, Saxe, & Tenenbaum, 2009) and communication via recursive so cial reasoning (Sperber & Wilson, 1986; Shafto, Goodman, & Griffiths, 2014). Theory of Mind is the capacity to rea son about one’s own or others’ mental states (such as be liefs, desires, and intentions) and interpret behavior in light of these mental states. Previous work focuses on how ob servers reason about agents that are simply doing activities such as pursuing goals (Gergely, Nadasdy, Csibra, & B ´ ´ıro,´ 1995) or interacting with others besides the observer (Heider & Simmel, 1944; Hamlin, Ullman, Tenenbaum, Goodman, & Baker, 2013). However, people often intentionally teach (Csibra & Gergely, 2009) and demonstrators who are show ing how to do a task behave in ways that differ systematically from those simply doing a task (Ho, Littman, MacGlashan, Cushman, & Austerweil, 2016). 
Here, we present a new framework for modeling how peo ple teach by and learn from demonstrations that combines el ements of planning (Puterman, 1994; Sutton & Barto, 1998) and cognitive hierarchy (Camerer, Ho, & Chong, 2004). This has several theoretical advantages and can capture new as pects of data originally reported in Ho et al. (2016). We de velop a model of sophisticated observers who not only rea son about another agent doing a task, but also reason dif ferently about a demonstrator’s communicative versus non communicative goals, thus learning more effectively than a na¨ıve observer who is insensitive to this distinction. Finally, we present the results of an experiment in which participants observed the behavior of another agent doing or showing how to do a task, and participants were told either that they were or were not produced with communicative intent. The model shows a correspondence to peoples’ judgments, providing further support for this framework for modeling teaching with and learning from demonstration. 
505
Modeling Teaching with and Learning from Demonstration 
To model demonstrator behavior and observer inferences, we draw on two approaches. Aspects of Theory of Mind have been modeled as inverse planning (Baker et al., 2009). Mean while, recursive social reasoning has been modeled as a cog nitive hierarchy (Camerer et al., 2004), where inferences and actions result from Bayesian agents modeling one another. This has been applied to domains such as pragmatics (Frank & Goodman, 2012) and strategic games (Wunder, Kaisers, Yaros, & Littman, 2011). Building on these approaches, we introduce a new model of showing as planning in observer belief space and a model of learning from showing. 
Theory of Mind as Inverse Planning 
Markov Decision Processes (MDPs) (Puterman, 1994) can be used to model intentional action (that is, doing a task) 
Figure 1: Cognitive hierarchy levels and model notation. 
update their beliefs in accordance with Bayesian inference: bObs 
t+1(Mi) = P(Mi | st,at,st+1) 
and serve as the generative model for Theory of Mind in ference (Baker et al., 2009). A ground MDP, Mi 2 M is a tuple < S,A,Ti,Ri, g >: a set of ground states S; a set of actions A; a transition function that maps states and ac 
µ P(at,st+1 | st,Mi)P(Mi) 
= P(at | st,Mi)P(st+1 | st,at,Mi)P(Mi) = pDo 
i (at | st)Ti(st+1 | st,at)bObs 
t (Mi). 
(2) 
tions to distributions over next states, Ti : S ⇥ A ! P(S); a reward function that maps state/action/next-state transitions to scalar rewards, Ri : S ⇥ A ⇥ S ! R; and a discount fac tor g 2 [0,1) that captures a preference for earlier rewards or completing a task quickly. Associated with each MDP is an optimal value function, Q⇤i : S ⇥ A ! R. Intuitively, 
That is, at each timestep, an observer’s beliefs is updated based on the prior belief from the previous timestep, bObs 
t , 
the likelihood of the observed state-action transition as given by the optimal policy, pDo 
i , and the transition dynamics, Ti, 
under the MDP Mi. For notational convenience, we define a belief update function BU(st,at,st+1,bObs 
this corresponds to the maximum expected cumulative dis 
this function is bObs 
t ). The output of 
counted reward (i.e. the value) that an agent could expect to receive when taking an action a from a state s and act ing optimally from then on. Formally, it is uniquely deter mined by the fixed-point of the recursive Bellman equations 
t+1, the observer’s belief that the demonstra tor is in each MDP given a state-action transition and previous beliefs bObs 
t . 
Showing as Planning in Observer Belief Space 
Q⇤i (s,a) = Âs0 Ti(s0 | s,a) 
h 
Ri(s,a,s0) +gmaxa0 Q⇤i (s0,a0) 
i 
, for 
An observer can interpret a demonstrator’s behavior as doing 
each state s and action a. Q⇤i represents the value for a per fectly optimal agent. To account for deviations from this, we assume each ”doing” agent uses a soft-max policy, which is defined as: 
i (at | st) = exp{Q⇤i (st,at)/tDo} 
a task using Theory of Mind. But what if the demonstrator is aware that they are being observed and motivated to show what task they are performing? Then they may reason not only about how their actions cause transitions and rewards in the ground state-space, S, but also in the observer’s be lief space, DM . We formulate a showing demonstrator, then, as representing an Observer Belief MDP (OBMDP), MShow 
pDo 
Âa02A(st) exp{Q⇤i (st,a0)/tDo}, (1) 
defined by the tuple < B,A,TShow i ,RShow 
i , 
i , gShow >: a joint 
belief-ground state-space, B = S ⇥ DM ; the original ground actions, A; a belief-ground state transition function, TShow 
where tDo > 0 is a temperature parameter. 
B ⇥A ! B; a showing reward function, RShow 
i : 
Given a generative model of a demonstrator’s actions as produced from possible desires (i.e. reward functions), en vironment knowledge (i.e. states and transitions), and ap proximate rationality (i.e. acting to soft-maximize value), an observing agent can perform Bayesian inference over worlds (i.e. MDPs). Suppose, at time t, an observer has an initial be 
i : B ⇥B ! R; 
and a showing discount rate, gShow 2 [0,1). (Note from now on we will refer to the doing discount as gDo to explicitly dis tinguish it from gShow.) 
We draw attention to two key components of our model of showing as planning in observer belief space: the showing reward function, RShow 
lief over possible MDPs, bObs 
TShow 
i , and the showing transition function, 
t (Mi). As they observe a demon strator take an action and transition to a new state, they will 
i . They are both influenced by the environment and ob server’s belief state. When cooperatively showing what they 506

Figure 2: (a) Example of behavior in doing versus showing demonstration conditions from Ho et al. (2016) and the instructions for each condition. (b) Space of eight possible reward functions with different assignments of safe (0 points) and dangerous (-2 points) to each of the three colors. Labels like “ooo” are used later in this paper to reference specific reward functions. Note that participants never directly saw the reward function and inferred them from instructions on a trial. 
are doing or how to do a task, a demonstrator wants the ob server to increase their belief in the true ground MDP, Mi. However, they are still constrained by the rewards in the en vironment, determined by Ri. That is, they must plan ac tions with both communicative and non-communicative re wards in mind. Thus, we formulate RShow 
i as a combination 
of ground rewards and weighted observer belief changes in the true ground MDP, Mi: 
RShow 
i (st,bObs 
t ,at,st+1,bObs 
t+1) 
= Ri(st,at,st+1) +k(bObs 
t+1(Mi)bObs 
t (Mi)), (3) 
where k controls the degree of a demonstrator’s motivation to show. The showing transition function is similarly deter mined by the ground dynamics, Ti, as well as how an ob server’s beliefs change in response to observed actions: 
TShow 
i (st+1,bObs 
t+1 | st,bObs 
reasons about showing demonstrators. Analogous to Equa tion 2, a sophisticated observer updates a distribution over OBMDPs by reasoning about possible showing agents: 
bS-Obs 
t+1 (Mi) 
µ pShow 
i (at | st,bObs 
t )Ti(st+1 | st,at)bS-Obs 
t (Mi). (5) 
A sophisticated observer recognizes that actions are, in part, pedagogically motivated. For instance, when teaching a child how to tie their shoes, a parent might fold the laces to clearly resemble “bunny ears”. A na¨ıve observer would only be able to attribute those particular actions to task-related goals, whereas a sophisticated observer could also reason about them in relation to communicative goals. 
Summary 
Here, we have developed a framework for modeling demon 
( 
= 
t ,at) 
Ti(st+1 | st,at), if bObs 
t+1 = BU(st,at,st+1,bObs 
t ) 
0, otherwise. (4) 
strator behavior and observer interpretations of behavior based on recursive social reasoning and Theory of Mind. Different types of demonstrators and observers correspond to different “levels” in a cognitive hierarchy, as illustrated 
A showing demonstrator can then be modeled by calculat ing the solution to an OBMDP.1That is, for MShow 
i , we can 
calculate a value function and softmax policy that defines actions to take in different world-belief states: pShow 
in Figure 1, allowing us to simultaneously model actions and inferences about possible tasks as they unfold over time. An implementation of the different models is available at 
st,bObs 
t ) µ exp{QShow 
i (st,bObs 
i (at | 
https://github.com/markkho/demonstration-teach-learn. 
t ,at)/tShow}, where tShow is a 
showing temperature parameter. 
Learning from Showing 
Just as a showing demonstrator has a nested model of an ob serving agent, we can define a sophisticated observer who 
1Although both plan over beliefs, an OBMDP is not equivalent to a Partially Observable Markov Decision Process (POMDP) with the true MDP as the hidden state. Due to Equation 4, an OBMDP value function is not necessarily piecewise, linear, and convex, which is a key property of a POMDP (Kaelbling, Littman, & Cassandra, 1998). We approximate the OBMDP value function using value iteration (Sutton & Barto, 1998) over a discretization of the belief space. 
Modeling Showing as Planning in Observer Belief Space 
Task 
In Ho et al. (2016), we compared how people show a task to how they do a task. Participants were given the gridworld in Figure 2a, where they could move the blue agent up, down, left, or right. Each round began in the same location and ended upon reaching the yellow goal, worth 10 points. Also, on each round, the reward for stepping on the remaining color tiles (orange, purple, and cyan) changed. Each color could be 
507

Figure 3: Tests for Ho et al. (2016) data. See Figure 2 for reward function codes. ⇤ p < .05, ⇤⇤ p < .01, ⇤⇤⇤ p < .001. 
either safe (no points) or dangerous (-2 points), resulting in eight distinct reward functions (Figure 2b). 
In the test phase, participants were given a round with each of the eight reward functions and were assigned to the do ing or showing condition. Those in the doing condition were simply told the reward values of each color. Those in the showing condition were told the reward values, but they were also told that their behavior would be shown to another par ticipant. Critically, this other participant would need to know which colors were safe and dangerous for a separate experi ment. Additional experimental details can be found in Ho et al. (2016). 
Analysis of Ho et al. (2016) Results 
Does the current model explain showing? In an OBMDP, the value of showing comes from the rewards associated with transitions in an observer’s belief space. Thus, a model of doing the task is a subset of showing the task, and we can use a likelihood-ratio test to determine if showing explains behavior. We fit the current model to individual participants and rounds, varying gDo, tDo, gShow, and tShow. Since an OB MDP collapses into the original world MDP when actions are uninformative, we used tDo = 1000 as the null model in a likelihood-ratio test. Using this test on the original data, we found that for the showing condition, seven out of eight reward functions rejected the null model, while in the doing condition, only one out of eight rejected it (all c2(29) > 42.5, p < .05, Figure 3). 
The model of showing as planning in observer belief space thus provides an account of how peoples’ showing demon strations unfold over time. This represents several advances over previous accounts such as that presented in Ho et al. (2016). First, it directly integrates non-communicative re wards, such as losing points for being on certain tile col ors, and communicative goals through RShow (Equation 3). This allows us to model how people balance these motiva tions. Second, we can arbitrarily approximate the entire value function and policy over observer belief space, rather than be constrained to enumerated trajectories. In doing so, we can directly model extended, repetitive behaviors that are non 

Figure 4: Left: Participant demonstrations in doing condition when only orange is dangerous. Right: Showing demonstra tions with an example of extended behavior captured by the model of planning in observer belief space in red. 
Markov in the world state-space, but Markov in the observer belief space. For example, Figure 4 illustrates a person’s showing demonstration from a single round in which specific transitions are revisited and emphasized in a particular se quence. We can also compute teaching policies when the en vironment involves stochastic transitions (Ho, Littman, Cush man, & Austerweil, in prep), which cannot be modeled by se lecting a deterministic trajectory. Finally, by (approximately) computing a compact representation for showing policies, pShow 
i ,i = 1,2,...,n, we can compute the belief states of a sophisticated observer who reasons about a demonstrator’s communicative intentions (bS-Obs). In the next section, we present an experiment designed to compare the predictions of a na¨ıve observer and this sophisticated observer model. 
Experiment: Learning from Showing 
Given that the model accounts for demonstrator behavior, we can investigate how demonstrator intentions and an ob server’s interpretation influence what is ultimately learned. To answer this question, we presented the empirical demon strations obtained from the doing and showing demonstrator conditions originally reported in Ho et al. (2016) to a new set of participants. These participants were additionally placed in either a doing or showing observer condition in which the interpretation of a demonstration (whether it was originally produced with the intent to show) was manipulated. Testing both sets of demonstrations as well as both possible interpre tations as separate factors enables us to understand how they interact and each influence learning from demonstration. 
Materials and Design 
The stimuli used were the state/action/next-state tuples from the original study. These were generated from the eight main trials from the 29 participants in the doing and showing demonstrator conditions, for a total of 464 demonstrations. Each participant was told they would observe a single demon stration from a partner. They were also assigned to a doing or showing observer condition. In the showing observer con dition, but not the doing one, they were told that their part ner “knows that you are watching and is trying to show you which colors are safe and dangerous”. Next, they were shown 
508

Figure 5: (a) Probability on the correct answer produced by the model. (b) Accuracy of human judgments. (c) Reported confidence of human judgments on a 0-100 scale. Error bars are bootstrap-estimated 95% confidence intervals. 
a page with the animated demonstration and answered, for each of the three colors (orange, purple, and cyan), whether they thought it was safe or dangerous and their confidence on a continuous scale (0 to 100). Each participant received a base pay of 10¢ and starting from a bonus of 15¢ won/lost 5¢ for each correct/incorrect answer. Two MTurkers were assigned to each demonstration and observer instruction combination using psiTurk (Gureckis et al., 2016). 
Models 
We implemented four models for the gridworld task: the do ing agent, the showing agent, the na¨ıve observer, and the sophisticated observer. The doing agent was parameterized with gDo = .99 and tDo = .08, while the showing model with a nested doing model was parameterized with gDo = .99, tDo = 3.0, gShow = .9, and tShow = 1.0. These values were chosen to produce trajectories that were qualitatively compa rable to human demonstrations. For each reward function and demonstrator type, one hundred trajectories were generated, which were then fed to either a na¨ıve observer who performed inference over possible models of doing (using the nested do ing parameters) or a sophisticated observer who performed inference over possible showing models. At the end of each observation of a trajectory, we converted the final probabil ity vector over possible MDPs to probabilities that each color was safe/dangerous. For each reward function, these were then converted to a corresponding probability on the true re ward of the color that the demonstration was generated from. As shown in Figure 5a, both model showing demonstrations and sophisticated observing led to greater probability mass assigned to the correct option. 
Experimental Results 
For both accuracy of which colors were safe/dangerous and confidence, we found main effects of both the demonstrator and observer instructions. For judgment accuracy, we used a repeated-measures logistic regression with correct/incorrect as the outcome variable, reward function and demonstra tor as random effects, and demonstrator instructions and 
observer instructions as fixed effects. We found signifi cant variance across reward function intercepts (SD = 0.92, c2(1) = 760.26, p < .0001) and demonstrator (SD = 0.33, c2(1) = 63.10, p < .0001). The most complex model with a significant increase in fit was one with the demonstrator and observer instruction conditions as main effects, but without their interaction. In the final model, there was a main effect of demonstrator instructions (b = 0.40, SE = 0.11, z = 3.63, p < .001), corresponding to showing demonstrations increas ing accuracy by 1.5 times, holding other factors at fixed val ues. There was also a main effect of observer instructions (b = 0.13, SE = 0.07, z = 1.97, p < .05), corresponding to observers’ interpretation of demonstrations as intentional showing increasing accuracy by 1.14 times. 
We similarly analyzed the confidence judgments provided by participants using a mixed-effects linear regression model. Confidence on a 0 to 100 scale was the outcome variable, while reward function, demonstrator, and observer were ran dom effects, and demonstrator and observer instructions were fixed effects. We found significant variance across reward function intercepts (SD = 3.65, c2(1) = 91.97, p < .0001), demonstrator (SD = 1.20, c2(1) = 23.72, p < .0001), and observer (SD = 14.14, c2(1) = 454.31, p < .0001). In the final model, which did not include the interaction between demonstrator and observer instructions, there was a main effect of demonstrator instructions (b = 3.34, SE = 0.93, t(57.2) = 3.59, p < .001) and observer instructions (b = 3.57, SE = 0.87, t(1790.8) = 4.08, p < .001). In short, across both measures (accuracy and confidence), we found main effects of demonstrator and observer instructions (Figure 5bc). 
Discussion 
We presented a computational framework for modeling demonstrator behavior and observer interpretation based on Theory of Mind (Baker et al., 2009) and recursive social rea soning (Camerer et al., 2004). In our models, the meaning of actions is grounded in what an agent performing a task would do, and a showing demonstrator is modeled as plan ning in the belief space of a na¨ıve observer using Theory of 
509
Mind. A sophisticated observer is then one who also reasons about the communicative goals of a showing demonstrator to draw stronger inferences about what they are being shown. This model has a number of advantages over one originally presented in Ho et al. (2016), and we found that it captures new aspects of the data in that study. Further, we can model the inferences of a sophisticated observer. In an experiment that used previously collected demonstrations, we found that, consistent with our models, both the observer’s interpretation of behavior as showing and demonstrator’s communicative intent to show positively influence learning. 
Our approach draws on a number of existing ideas and re lates to several other lines of research. Related formalisms have been explored in the context of making robot actions legible (Dragan, Lee, & Srinivasa, 2013) and from a “value alignment” perspective (Hadfield-Menell, Russell, Abbeel, & Dragan, 2016). Within cognitive science, this work builds on models of concept teaching by example (Shafto et al., 2014) and sequences of teacher interventions (Rafferty, Brunskill, Griffiths, & Shafto, 2016) as recursive reasoning and par tially observable planning, respectively. Additionally, simi lar models have been used to study how people generate and interpret pragmatics in language (Frank & Goodman, 2012). This work can be seen as a direct extension of the work in pragmatics to forms of non-verbal communication where the “semantics” of communicative behaviors are determined by world-directed intentional action (i.e. doing tasks). 
There are several directions to explore with these models. For instance, they make predictions about the time course of na¨ıve versus sophisticated observer inferences, but our exper iment did not test these directly. Important differences might arise in more complex domains with longer time horizons. Also, we model belief space transitions as deterministic and known with certainty, but in reality this is rarely the case. “Uncertainty in the observer’s uncertainty” could have an in fluence on demonstrator behavior that cannot be explained by the current model. Finally, some work in linguistics explores the back-and-forth of conversations from the perspective of recursive social reasoning (Hawkins, Stuhlmuller, Deegan, & ¨ Goodman, 2015). This work could be extended to model sit uations in which both the teacher and learner can take actions while observing and reasoning about one another. Future work will need to explore these questions to provide a clearer picture of everyday teaching, social learning, and communi cation. 
Acknowledgments 
MKH was supported by a Brown University Dissertation Fel lowship. 
References 
Baker, C. L., Saxe, R., & Tenenbaum, J. B. (2009). Action under standing as inverse planning. Cognition, 113(3), 329–349. doi: 10.1016/j.cognition.2009.07.005 
Boyd, R., Richerson, P. J., & Henrich, J. (2011). Collo quium Paper: The cultural niche: Why social learning is es sential for human adaptation. Proceedings of the National 
Academy of Sciences, 108(Supplement 2), 10918–10925. doi: 10.1073/pnas.1100290108 
Camerer, C. F., Ho, T.-H., & Chong, J.-K. (2004). A cognitive hierarchy model of games. The Quarterly Journal of Economics, 861–898. Csibra, G., & Gergely, G. (2009). Natural pedagogy. Trends in Cog 
nitive Sciences, 13(4), 148–153. doi: 10.1016/j.tics.2009.01.005 Dennett, D. C. (1987). The intentional stance. MIT press. Dragan, A., Lee, K., & Srinivasa, S. (2013). Legibility and pre dictability of robot motion. In 2013 8th ACM/IEEE International 
Conference on Human-Robot Interaction (HRI) (pp. 301–308). doi: 10.1109/HRI.2013.6483603 
Frank, M. C., & Goodman, N. D. (2012). Predicting Pragmatic Reasoning in Language Games. Science, 336(6084), 998–998. doi: 10.1126/science.1218633 
Gergely, G., Nadasdy, Z., Csibra, G., & B ´ ´ıro, S. (1995). Taking the ´ intentional stance at 12 months of age. Cognition, 56(2), 165– 193. doi: 10.1016/0010-0277(95)00661-H 
Gureckis, T. M., Martin, J., McDonnell, J., Rich, A. S., Markant, D., Coenen, A., . . . Chan, P. (2016). psiTurk: An open-source framework for conducting replicable behavioral experiments on line. Behavior research methods, 48(3), 829–842. 
Hadfield-Menell, D., Russell, S. J., Abbeel, P., & Dragan, A. (2016). Cooperative Inverse Reinforcement Learning. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, & R. Garnett (Eds.), Ad vances in Neural Information Processing Systems 29 (pp. 3909– 3917). Curran Associates, Inc. 
Hamlin, K., Ullman, T., Tenenbaum, J., Goodman, N., & Baker, C. (2013). The mentalistic basis of core social cognition: experi ments in preverbal infants and a computational model. Develop mental Science, 16(2), 209–226. doi: 10.1111/desc.12017 
Hawkins, R. X., Stuhlmuller, A., Deegan, J., & Goodman, N. D. ¨ (2015). Why do you ask? Good questions provoke informative answers. In D. Noelle et al. (Eds.), Proceedings of the 37th An nual Conference of the Cognitive Science Society (pp. 878–883). Austin, TX: Cognitive Science Society. 
Heider, F., & Simmel, M. (1944). An Experimental Study of Ap parent Behavior. The American Journal of Psychology, 57(2), 243–259. doi: 10.2307/1416950 
Ho, M. K., Littman, M., Cushman, F., & Austerweil, J. L. (in prep). Teaching with and learning from demonstrations. 
Ho, M. K., Littman, M., MacGlashan, J., Cushman, F., & Auster weil, J. L. (2016). Showing versus doing: Teaching by demon stration. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, & R. Garnett (Eds.), Advances in Neural Information Processing Systems 29 (pp. 3027–3035). Curran Associates, Inc. 
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning and acting in partially observable stochastic domains. Artificial Intelligence, 101(1), 99–134. doi: 10.1016/S0004- 3702(98)00023-X 
Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming (1st ed.). New York, NY, USA: John Wiley & Sons, Inc. 
Rafferty, A. N., Brunskill, E., Griffiths, T. L., & Shafto, P. (2016). Faster Teaching via POMDP Planning. Cognitive Science, 40(6), 1290–1332. doi: 10.1111/cogs.12290 
Shafto, P., Goodman, N. D., & Griffiths, T. L. (2014). A ratio nal account of pedagogical reasoning: Teaching by, and learn ing from, examples. Cognitive Psychology, 71, 55–89. doi: 10.1016/j.cogpsych.2013.12.004 
Sperber, D., & Wilson, D. (1986). Relevance: Communication and Cognition. Cambridge, Massachusetts: Harvard University Press. Sutton, R. S., & Barto, A. G. (1998). Reinforcement learning: An introduction. Cambridge, MA: MIT press. 
Tomasello, M., Carpenter, M., Call, J., Behne, T., Moll, H., et al. (2005). Understanding and sharing intentions: The origins of cul tural cognition. Behavioral and brain sciences, 28(5), 675–690. Wunder, M., Kaisers, M., Yaros, J. R., & Littman, M. (2011). Us ing iterated reasoning to predict opponent strategies. In The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2 (pp. 593–600). International Foundation for Autonomous Agents and Multiagent Systems. 
510
Predictors of L2 word learning accuracy: A big data investigation 
Elise W.M. Hopman (hopman@wisc.edu) 
Department of Psychology, 1202 W. Johnson Street 
Madison, WI 53706 USA 
Bill Thompson (biltho@mpi.nl) 
Language and Cognition Department, Max Planck Institute for Psycholinguistics Nijmegen, The Netherlands 
Joseph L. Austerweil (austerweil@wisc.edu) 
Department of Psychology, 1202 W. Johnson Street 
Madison, WI 53706 USA 
Gary Lupyan (lupyan@wisc.edu) 
Department of Psychology, 1202 W. Johnson Street 
Madison, WI 53706 USA 
Abstract 
What makes some words harder to learn than others in a second  language? Although some robust factors have been identified  based on small scale experimental studies, many relevant  factors are difficult to study in such experiments due to the  amount of data necessary to test them. Here, we investigate  what factors affect the ease of learning of a word in a second  language using a large data set of users learning English as a  second language through the Duolingo mobile app. In a  regression analysis, we test and confirm the well-studied effect  of cognate status on word learning accuracy. Furthermore, we  find significant effects for both cross-linguistic semantic  alignment and English semantic density, two novel predictors  derived from large scale distributional models of lexical  semantics. Finally, we provide data on several other  psycholinguistically plausible word level predictors. We  conclude with a discussion of the limits, benefits and future  research potential of using big data for investigating second  language learning. 
Keywords: second language learning; vocabulary; big data;  corpus analysis; distributional semantics; 
Introduction 
Spanish speakers learning English on Duolingo are more than  twice as likely to err with the word ‘blue’ than with the word  ‘gray’. They are also about 1.5 times more likely to make a  mistake with the word ‘blue’ than Italian speaking learners  are.1 What explains such differences in word learning? In this  paper we investigate these questions by examining which  word level factors predict accurate word learning in a large,  naturalistic dataset of Spanish, Italian, and Portuguese  speakers learning English. 
The second language (L2) literature has identified several  word level factors that predict how easy a new word in L2  will be to remember (De Groot & Keijzer, 2000). The  strongest predictor is concreteness. All else equal, concrete  
 
1 Example data-points based on the dataset used in this paper,  described in more detail in the methods section.  
511
words tend to be easier to learn. Researchers hypothesize that  concrete words have richer representations in memory, and  this richer representation provides more opportunities for the  learner to associate the L2 word with the L12 word. Another  predictor of learning ease is whether the L2 wordform is a  cognate (largely shares form and meaning) of the L1 form  (e.g., ‘actor’ in English and Spanish). Studies have  manipulated word frequency (both L1 frequency and L2  exposure), but this predictor does not have a simple robust  effect (De Groot & Keijzer, 2000). Finally, there is a body of  work on translation ambiguity, showing that if a word in one  language has two distinct translations into another language,  it will be harder to learn (Bracken, Degani, Eddington &  Tokowicz, 2017).  
Other potentially interesting factors that could matter for  L2 word learning have been proposed but not yet studied (De  Groot & Van Hell, 2005). Because L2 word learning studies  are typically conducted in small classrooms or laboratories, it  has been difficult to study word level factors at scale. There  have been several recent calls in the L2 literature to move  beyond small classroom studies and towards more  quantitatively robust analyses (e.g. Milin, Divjak,  Dimitrijevic, & Baayen, 2016; Norris & Ortega, 2000). One  larger scale study successfully assessed some factors that  predict L2 word recognition in fluent bilinguals (Lemhöfer et  al, 2008). Furthermore, there has been a broader call in the  cognitive sciences to use big and natural datasets to shed light  on the questions that the field has been struggling to answer  with experimental studies (Paxton & Griffiths, 2017). 
Here, we analyze a big and naturally occurring dataset to  analyze word level factors that may affect word-learning  performance of L2 English learners. The dataset we use  contains a total of 36,799 people using the program Duolingo  to learn English (Settles & Meeder, 2016).  
2 L1 = first language; We assume that for most users, the user  interface language in Duolingo is their native language.  
Figure 1: Screenshots from the Duolingo user interface.  (a) Progress bars wrapped around each skill encourage users  to do learning sessions that contain weak words.  (b) Example trial where the user translates a sentence from  L1 Spanish into L2 English by choosing words from a word  bank. In this example, the user has already translated the  sentence. Note that in this example, several words are  embedded in a longer phrase, as is typical in Duolingo.  
Duolingo is a popular free online program that gamifies  second language learning. It combines several best practices  from learning research in its design including explanations, implicit instruction and mastery learning (see Fig. 1 and  Settles & Meeder). In the app, a user does practice sessions  revolving around a skill (e.g. ‘food’ or ‘animals’, Fig. 1a). A  practice session consists of multiple trials, often involving  several words embedded in larger phrases (Fig. 1b). Different  trials embed different language learning tasks —written  translation, fill in the gap sentences, multiple choice,  matching tasks — in both the L2 to L1 and the L1 to L2  direction. Duolingo applies a carefully constructed  curriculum, so different skills are learned in a specified order  with the goal of aiding learning. To do so, Duolingo’s  algorithm predicts when words are becoming weak in  memory and should be studied again by the user (Fig. 1a).  
We focus our analyses on two types of word-level  predictors. The first is orthographic similarity between the L1  and L2 wordform — a measure that can be viewed as a  continuous estimate of cognate status. We expect words with  similar L1/L2 forms to be learned more easily. The second is  a novel measure of semantic similarity. For each pair of  words that are translation-equivalents, we compute semantic  similarity based on distributional semantics of L1 and L2.  This measure — described in more detail below — allows us  to ask whether words whose meanings are more alignable in  L1 and L2 are learned more easily.  
 
3 An OSF archive containing all data and analysis scripts for this  paper will be made publicly available at osf.io/uwdcm 
512
Method3 
Duolingo dataset manipulation 
The English learning part of the Duolingo dataset (Settles &  Meeder, 2016) as originally released contains 5.01 million  instances of data. Each instance contains accuracy data for a  single user during a single Duolingo session on a single  lexeme, though that lexeme may have been seen multiple  times during that session. A lexeme is a word with specified  morphological form. For example, ‘girl’ and ‘girls’ are  separate lexemes, as one is singular and the other plural.  While the Duolingo dataset has separate entries for different  lexemes that correspond to the same word (e.g. the lexemes  ‘cat’ and ‘cats’ both correspond to the lemma ‘cat’), most of  the other corpora that we used to obtain word-level predictors  did not. Thus, we aggregated the Duolingo dataset by word,  collapsing from 2983 different lexemes to 1412 different  words, with data on each word originating from 1-21  different lexemes. 
 The Duolingo dataset contained data collected over two  consecutive weeks, resulting in several (sometimes  thousands of) instances for a given learner being presented  with a given word. In addition to accuracy data for the current  session, the data instance also included a timestamp and  aggregated accuracy data on all previous encounters that the  learner had with that word. Since we are not interested here  in the time course of learning, we aggregated accuracy data  to get a single datapoint per learner per word that detailed  how often the learner had seen the word in total and how often  they had been correct on that word. This aggregated dataset  contained 1.86 million user-word data points taken from  English courses with three different user interface languages:  Portuguese, Spanish and Italian. 
 Because the order and frequency with which words are  presented to a learner are not random in Duolingo, we used  two user level measures as control predictors. The first, which  we call word experience, is the total number of times that a  user saw a given word in Duolingo. The second, user  experience, is the sum of word experience for all words a user  has practiced. Both of these predictors were log-scaled since  their distributions were highly skewed, with many words  practiced only a few times and some words practiced many  times, and many users practicing English only a little and  some users practicing it a lot.  
 We expect that user experience is predictive of word  learning accuracy, with more experienced users doing better  than less experienced users. For word experience, the naive  prediction would be that the more experience a user has with  a certain word, the better they should do on it. However,  Duolingo will present a word more often after a user has  made a mistake on it, meaning that words that a user has more  difficulty on will be practiced more often. Due to this biased  sampling procedure, average accuracy will be lower for  words that have been practiced more often.  
Word level predictors 
We used Google Translate to obtain word translations of the  English words into three different user interface languages,  since the dataset released by Duolingo did not include the L1  translation equivalents used in the app. We then calculated  the minimum edit4 (Levenshtein) distance between the  English word and its translation equivalent as a continuous  measure of cognate status, which we refer to as translation  distance. Earlier L2 studies have mainly used cognate status  as a binary predictor, comparing perfect cognates to non cognates. Our prediction for this measure of cognate status  goes in the same direction: the smaller the translation distance  between the two words in a translation pair is, the easier it  should be to learn the word.  
We focus on two main semantic predictors5, both derived  from large scale distributional models of lexical semantics.  The first, which we call semantic alignment, measures one  aspect of how similarly a word is situated in the semantic  representations of two languages. In intuitive terms, this  captures the quality of a translation pair from a distributional  perspective: If a translation pair keep similar semantic  associations in two languages, then their meanings can be  understood to be more aligned, and perhaps easier to learn.  To compute semantic alignment (denoted rho), we obtained  distributional models of lexical semantics for English,  Spanish, Portuguese, and Italian. These networks were  recently released by Facebook Artificial Intelligence  Research (Bojanowski et al., 2017). Each model encodes the  vocabulary of its language as points (or vectors) in an abstract  semantic space. The configuration of these points is estimated  by training a neural network to predict the words that appear  often with a word in a large dataset of text derived from  Wikipedia, and using this neural network to assign vectors to  words. Close proximity in this vector space, as measured by  cosine distance, implies a close semantic relationship  between words. 
 To compute the semantic alignment of a translation pair, we  found the semantic neighborhood of each word - the N closest words within their respective semantic spaces (here N  = 40). We only included in each space those words from the  Duolingo dataset for which we were able to translate between  the two languages using Google Translate. We then  compared the cosine similarity of the words in the semantic  neighborhoods of the translation pair. To do so, we first  identified common neighbors, i.e, words which are in the  semantic neighborhood of the target in English, and whose  translation equivalent appears in the semantic neighborhood  of the target in the other language. For each of these common  neighbors, we calculated the cosine similarity between the  target and the neighbor, in both semantic spaces. We   
4 Minimum number of edits needed to change one word into another. For example, the Levenshtein distance between ‘cat’ and  ‘gato’ is 2: the ‘c’ needs to become a ‘g’ and an ‘o’ needs to be  added. 
5 The method for calculating the new semantic predictors  mentioned in this section is described in more detail in Thompson,  Roberts & Lupyan (2018). 
513
calculated total structural alignment by computing the rank  correlation statistic (Spearman’s rho) between these aligned  target-neighbor cosine similarities6. If the similarity structure  is closely aligned between the semantic spaces of the two  languages near the translation pair, rho will approach 1, and  words with a large rho should be easier to learn. If the  similarity structure were inverted between languages (i.e. the  closest common neighbor in English is the most distant  common neighbor Spanish), rho would approach -1; such  words should be harder to learn.  
 Thus, while our new measure semantic alignment is  conceptually related to translation ambiguity, it is calculated  in an automated way that does not rely on human similarity  judgments, making it easier to calculate for a wide variety of words between different language pairs. Our prediction that  words that are less semantically aligned should be harder to  learn corresponds to experimental findings that words that are  more translation ambiguous are harder to learn (Bracken et  al., 2017). 
 In addition to semantic alignment, we also recorded the  mean edit (Levenshtein) distance between the target English  word and its N = 40 closest neighbors in English semantic  space. This measure identifies, for example, whether a word  has many morphological variants, which are close in  orthographic and semantic space. We call this variable  English morphological density. 
The third semantic predictor is English semantic density (a  measure distinct from phonological or orthographic density  used in some past studies). By identifying the N nearest neighbors of a target English word, we can obtain a measure  of how concentrated the region of semantic space the word  occupies. Some words are surrounded by many other words  with similar or related meanings, while others occupy  isolated territory with few close associations. This aspect of  a word can be quantified by the mean cosine distance to its  closest 40 neighbors (in this dataset), and can be understood  as a close analog of local clustering coefficients used in  network analysis (how many words connected to a target  word are connected to each other). An English word from a  dense semantic neighborhood may be more confusable with  its neighbors and thus harder to learn.  
 Besides our three measures of core interest (translation  distance, semantic alignment and semantic density), we  added several other exploratory word level predictors7. We  added English word recognition, a z-scored Reaction Time  measure for native English speakers in a lexical decision task  as a predictor (Balota et al, 2007). We added this measure  from the English Lexicon Project to see if and how native  speaker ease of processing of a word might be related to early  L2 English word learning accuracy. 
6 Neighbor overlap and semantic alignment are correlated  statistics; in ongoing work, we are exploring this relationship  further. 
7 Our dataset includes additional predictors, e.g. measures of  phonological neighborhood density; predictors that didn’t  significantly contribute to model fit in initial analyses were left out  of later analyses. 
 We used a multilingual WordNet (Bond & Paik, 2012) to  obtain a measure for the number of distinct meanings a word  has. We added concreteness (Brysbaert et al., 2014) as a  predictor because the L2 word learning literature identifies  this as the most robust predictor for early L2 word learning  accuracy, with more concrete words easier to learn.  
We also add user interface language word frequency (Cuetos,  Glez-Nosti, Barbón, & Brysbaert, 2012). Some, but not all  previous studies investigating early L2 word learning  accuracy find that more frequent words are easier to learn.  Since users’ frequency of exposure to a word in English is  already captured by our word experience variable, we opted  to use frequency estimates based on the user’s L1.  
 Finally, to control for part of speech, we included the  dominant part of speech based on a parse of SUBTLEX-US  in our dataset (Brysbaert et al, 2012). Since most closed part  
of speech categories (e.g. articles) only consist of a handful  of words in our dataset, we collapsed this into 4 big open  categories (noun, verb, adjective, adverb) and a 5th  miscellaneous category containing all other word types.  
Final corpus descriptives 
Combining all word level predictors with the Duolingo  dataset, we were able to collect a measure of each predictor  on 1064 different English words. We removed data from  users who completed less than 41 data instances (15% of total  users). We also excluded words for individual users that had  been practiced fewer than three times (2.6 % of user-word  data points). Table 1 shows, Mean, SD, and range for all  predictors in our final corpus. 
Table 1. Descriptive statistics (Mean, Standard Deviation and Range) for the predictors in data sets for the three different user  interface languages.  
Predictor Language of dataset Mean (SD) Range na user experience Spanish 5.59 (1.15) 3.71 to 12.69 26628 Portuguese 5.6 (1.13) 3.71 to 10.3 7329 
Italian 5.72 (1.17) 3.71 to 10.17 2843 
word experience Spanish 2.3 (0.82) 1.1 to 9.51 1007092 Portuguese 2.3 (0.79) 1.1 to 8.51 258609 
Italian 2.23 (0.79) 1.1 to 7.14 124471 
translation distance Spanish 3.99 (2.23) 0 to 12 979 Portuguese 4.1 (2.11) 0 to 12 952 
Italian 4.3 (2.37) 0 to 13 956 
semantic alignment Spanish 0.37 (0.3) -0.86 to 0.94 979 Portuguese 0.36 (0.29) -0.75 to 1 952 
Italian 0.35 (0.3) -0.77 to 0.96 956 
English morphological density Spanish 5.91 (1.28) 3.4 to 11.32 979 Portuguese 5.94 (1.29) 3.4 to 11.32 952 
Italian 5.91 (1.27) 3.4 to 11 956 
English semantic density Spanish 0.42 (0.07) 0.25 to 0.68 979 Portuguese 0.42 (0.07) 0.25 to 0.71 952 
Italian 0.42 (0.08) 0.25 to 0.71 956 
English word recognition Spanish -0.59 (0.19) -1 to 0.17 979 Portuguese -0.59 (0.19) -1 to 0.17 952 
Italian -0.59 (0.18) -1 to 0.17 956 
distinct meanings Spanish 1.48 (0.82) 0 to 3.56 911 Portuguese 1.46 (0.69) 0 to 3.5 896 
Italian 1.41 (0.65) 0 to 3.66 892 
concreteness Spanish 3.32 (1.11) 1.12 to 5 979 Portuguese 3.32 (1.11) 1.12 to 5 952 
Italian 3.31 (1.12) 1.12 to 5 956 
word frequency Spanish 7.86 (1.75) 0.69 to 14.16 911 Portuguese 8.25 (1.75) 0.69 to 14.38 896 
Italian 8.54 (1.8) 0 to 14.67 892 
aThe number of data points n used to estimate the reported statistics of each predictor are different due to properties of the  data set. For example, user experience is determined based on the number of users in a dataset, concreteness is determined  based on the number of English words in a dataset, and word experience is based on unique user-word data-points. Finally,  
since there were many more Spanish users, some words had enough users to make it into the Spanish but not the Portuguese  or Italian datasets. 
514
Results 
Overall accuracy was 90% and was remarkably similar for  the three L1 languages in the dataset: Italian (90.9%), Spanish  (90.0%), Portuguese (89.9%). Not surprisingly, performance  was better for users who used Duolingo more, b=.10, 95% CI  = [0.094, 0.104], t=41.2. Despite the robustness of user  experience as a predictor of performance, absolute  differences in performance were quite small. Users at the  lowest quartile of usage had 90.5% accuracy, while users at  the highest quartile of usage had 91.3% accuracy. This highly  restricted range of accuracies speaks to the adaptive nature of  Duolingo’s platform. When users make mistakes, they are  more likely to practice the words later, keeping overall  accuracy high and relatively constant. Accordingly,  controlling for overall user experience, greater experience  with a given word is associated with lower accuracy b=-.04,  95% CI=[-0.052, -0.047], t=-36.8, most likely because users  get increased exposure to a word because they made mistakes  with it. This adaptive-sampling property of Duolingo makes  it difficult to predict accuracy from word-level properties, but  as we describe below, we can nevertheless account for what  makes some words more difficult than others. 
We modeled accuracy for each user-word datapoint with  mixed-effects regression, running separate models for each  of three user interface languages (Italian, Spanish, and  Portuguese). This model included a random intercept for user  (since each user had seen multiple words), English word  (since each word was seen by multiple users) and major part  of speech (to ensure that some predictors like concreteness  are not confounded by differences between parts of speech).  We show standardized coefficients and 95% CIs for each L1  language model in Fig. 2. For example, a 1SD increase in user  
experience for Portuguese users leads to .12 SD increase in  overall accuracy. Corresponding p-values can be inferred  from the displayed 95% CIs. 
Controlling for both user and word experience, we find that  translation distance between L1 and English is negatively associated with accuracy. This relationship is significant for  Spanish-English, Italian-English and is marginal for  Portuguese-English. Accuracy in all three languages is  associated positively with semantic alignment. The larger the  semantic alignment between L1 and English for a given  translation-pair, the more likely people are to be accurate  (controlling for all other factors in the model). Accuracy in  all three L1s models is associated negatively with the density  of the English word’s semantic neighborhood. Words having  high density neighborhoods such as “something”, “anything”  and “anybody” pose greater learning challenges (controlling  for other factors) compared to words such as “register”,  “profile”, and “special”, which reside in neighborhoods with  lower semantic density. 
Aside from these three predictors, we found some other  effects which we did not explicitly predict and which should  be interpreted with caution. Words being learned with larger  English morphological density were associated with larger  accuracy for Portuguese-English users, but this was not a  significant predictor in the Italian and Spanish datasets.  Concreteness is not reliably associated with accuracy when  we take part of speech into account. Puzzlingly, more  concrete words were associated with marginally lower  accuracy for Portuguese-English users. Finally, longer lexical  decision times from native-English speakers (Balota et al.,  2007) were associated with numerically lower accuracy.  However, this predictor was only significant for the  Portuguese-English users. 
Figure 2: Regression results for accuracy. Standardized coefficients and 95% CIs are plotted for the predictors in each L1  language model. Standardized coefficients are interpreted as in the following example: a 1SD increase in user experience  for Portuguese users leads to .12 SD increase in overall accuracy. 
515
Discussion 
We predicted accuracy of learning English words by  Italian, Spanish, and Portuguese users of Duolingo as  measured in a large naturally collected dataset. We found  evidence for several novel factors for L2 word learning. As  predicted, English words having smaller orthographic  distances to their translation-equivalents were easier to  learn. In addition to orthographic similarity, semantic similarity (obtained by cross-linguistic alignment of word  embeddings derived from distributional semantics) was  also associated with higher accuracy. Finally, words  residing in dense English semantic neighborhoods were  harder to learn than words residing in less dense semantic  neighborhoods, when controlling for all other predictors.  We also examined several other predictors that might be of  interest to researchers investigating L2 word learning (Fig.  2). 
These results can form the basis for future experimental  studies. Since classroom and experimental studies are often  necessarily limited in the number of words they test,  predictors could first be investigated in this or similar big  data, so that items for controlled studies can be strategically  chosen.  
Limitations 
There are several aspects of this naturally occurring dataset  that, despite its size, limit its usefulness for answering  theoretically interesting questions about L2 learning. The  Duolingo curriculum is constructed to maximize accuracy.  This leads to a much smaller range of accuracy scores than  is usually seen in experimental studies of word-learning.  Furthermore, highly biased sampling of words produces a  non-random ordering that affects several of our word level  predictors. This may be why we do not find effects for  certain predictors that are typically strong in experimental  studies. Relatedly, Duolingo’s algorithm presents a word  sooner for repeated study after the user gets it wrong on a  trial. Finally, we have very little information about the  users. In experimental studies, a participant’s language  background and other demographics that might influence  learning abilities can be measured in questionnaires,  whereas for this dataset even a user’s native language is  only inferred. Such limitations in using big and naturally  occurring datasets should not, however, preclude their use  in cognitive science (Paxton & Griffiths, 2017). 
 On the positive side, these data provide certain ecological  validity absent from lab studies, and allow us to look at a  longer slice of learning time compared to typical lab  studies. Duolingo users are self-motivated to learn a second  language, which is not necessarily true for learners in  school classrooms who might just be meeting a curriculum  requirement and participants in experimental studies.  Furthermore, the size of this dataset allowed us to  investigate many more word level predictors than can  easily be manipulated in any one classroom study.  
References  
Balota, D. A., Yap, M. J., Hutchinson, K. A., Cortese, M.  J., Kessler, B., Loftis, B., Neely, J. H., Nelson, D. L.,  Simpson, G. B., & Treiman, R. (2007). The English  lexicon project. Behavior Research Methods, 39(3). 
Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T.  (2017). Enriching Word Vectors with Subword  Information. Transactions of the Association for  Computational Linguistics, 5. 
Bond, F. & Paik, K. (2012). A survey of wordnets and their  licenses. Proceedings of the 6th Global WordNet  Conference (GWC 2012). Matsue. 
Bracken, J., Degani, T., Eddington, C. & Tokowicz, N.  (2017). Translation semantic variability: How semantic  relatedness affects learning of translation-ambiguous  words. Bilingualism: Language and Cognition 20 (4). 
Brysbaert, M., New, B., & Keuleers, E. (2012). Adding  part-of-speech information to the SUBTLEX-US word  frequencies. Behavior Research Methods, 44, 991-997. 
Brysbaert, M., Warriner, A.B., & Kuperman, V. (2014).  Concreteness ratings for 40 thousand generally known  English word lemmas. Behavior Research Methods, 46. 
Cuetos, F., Glez-Nosti, M., Barbón, A., & Brysbaert, M.  (2012). SUBTLEX-ESP: Spanish word frequencies  based on film subtitles. Psicológica, 33(2). 
De Groot, A., & Keijzer, R. (2000). What is hard to learn  is easy to forget: The roles of word concreteness, cognate  status, and word frequency in foreign-language  vocabulary learning and forgetting. Language Learning,  50(1). 
De Groot, A.M.B., & Van Hell, J.G. (2005). The learning  of foreign language vocabulary. In J.F. Kroll & A.M.B.  De Groot (Eds.), Handbook of bilingualism:  Psycholinguistic approaches. Oxford University Press. 
Lemhöfer, K., Dijkstra, T., Schriefers, H., Baayen, R.H.,  Grainger, J. & Zwitserlood, P. (2008). Native language  influences on word recognition in second language: a  megastudy. Journal of Experimental Psychology:  Learning, Memory and Cognition, 34. 
Milin, P., Divjak, D., Dimitrijevic, S., & Baayen, R. H.  (2016). Towards cognitive plausible data science in  language research. Cognitive Linguistics, 27(4). 
Norris, J. M., & Ortega, L. (2000). Effectiveness of L2  instruction: A research synthesis and quantitative meta analysis. Language learning, 50(3). 
Paxton, A. & Griffiths, T. L. (2017). Finding the traces of  behavioral and cognitive processes in big data and  naturally occurring datasets. Behavior Research  Methods, 49. 
Settles, B., & Meeder, B. (2016). A trainable spaced  repetition model for language learning. In Proceedings  of the 54th Annual Meeting of the ACL. 
Thompson, B., Roberts, S., & Lupyan, G. (2018).  Quantifying Semantic Similarity Across Languages. In  Proceedings of the 40th annual conference of the  cognitive science society. 
516
On the instrumental value of hypothetical and counterfactual thought 
Thomas Icard (icard@stanford.edu) Stanford University 
Fiery Cushman (cushman@fas.harvard.edu) Harvard University 
Abstract 
Joshua Knobe (joshua.knobe@yale.edu) 
Yale University 
to simulate. We can then ask which way of selecting options 
People often engage in “offline simulation”, considering what would happen if they performed certain actions in the future, or had performed different actions in the past. Prior research shows that these simulations are biased towards actions a per son considers to be good—i.e., likely to pay off. We ask whether, and why, this bias might be adaptive. Through com putational experiments we compare five agents who differ only in the way they engage in offline simulation, across a variety of different environment types. Broadly speaking, our exper iments reveal that simulating actions one already regards as good does in fact confer an advantage in downstream decision making, although this general pattern interacts with features of the environment in important ways. We contrast this bias with alternatives such as simulating actions whose outcomes are instead uncertain. 
Introduction 
People spend a remarkable amount of time asking “what if?”—considering things they could have done but didn’t (“counterfactuals”), or might do but haven’t yet (“hypothet icals”). Specifically, they tend to simulate options that they regard as good (Kahneman and Miller, 1986; McCloy and Byrne, 2000; Phillips and Cushman, 2017; Icard et al., 2017). Suppose we observe a person saying something insulting and thereby getting into a terrible argument. We might immedi ately find ourselves thinking: “What if he had instead said something more tactful? What would have happened then?” But we would not normally show the opposite tendency. If we observe an individual saying something completely reason able and tactful, we would not spontaneously begin thinking: “What if he had instead said something insulting?” 
Here, we ask whether this tendency is an adaptive one. To answer this question, we conduct a series of computational experiments. These compare the performance of agents who show a tendency to consider options they regard as good with agents of several alternative designs. Is the “good action bias” advantageous, and when? 
At the core of our approach is the idea that hypotheti cal simulation might prove helpful in subsequent decision making. In our computational simulations, each agent faces a decision among a range of options. At the time of the de cision, the agent is not able to reflect on all of these options, and it therefore has to make the decision in a way that in volves only limited reflection. However, in the time prior to the actual decision, the agent can engage in off-line simula tion of some options. The more the agent simulates a specific option, the more accurate its representation of this option be comes. The key difference between the various agents is that each one uses a different method to determine which options 
at the time of off-line simulation leads to the best actions at the time of actual decision-making. 
How to Improve Action by Thinking Ahead If the func tion of simulation is to improve future action then, broadly speaking, it must work by correcting errors in people’s cur rent assumption about the values of various actions. Such errors could take two forms: The value could be set too high (in which case the person would choose the action too often) or the value could be set too low (in which case the person might overlook the action too often). Moreover, if an error of either type exists and can be corrected merely through sim ulation, then the error existed because the individual had not yet devoted enough attention to simulating the action. This analysis suggests three broad heuristic strategies to allocating limited cognitive resources to simulation: 
1. Focus on actions that you haven’t considered. If you have already considered an action a large number of times, you are unlikely to learn a lot more by considering that same action again. Focus instead on considering those ac tions that you have thus far considered the least. 
2. Focus on actions you currently think are low value. If you now regard certain actions as having low value, focus on simulating them and thereby learning more about them. 
3. Focus on actions you currently think are high value. If you now regard certain actions as having high value, focus on simulating them and thereby learning more about them. 
Of these three strategies, the first does the best job of max imizing the overall accuracy of the agent’s representations of the expected value of each action. Thus, if the agent follows one of the other two strategies, the overall accuracy of its rep resentations will be lower than it would have been if it had simply followed the first strategy. But, of course, the aim is not necessarily to have maximally accurate representations, but rather to have representations that are optimal for guid ing action. It is therefore possible that one of the other two strategies will be more adaptive in the relevant sense. 
The second strategy will tend to generate especially ac curate representations of the lowest-value actions. Thus, to return to our original example, suppose that there are a num ber of different ways of making insulting comments (insulting the person’s weight, insulting the person’s family, etc.). This strategy allows the agent to develop a highly accurate rep resentation of which of these actions would be the absolute 
517
worst, which would be somewhat less bad, and so forth. By contrast, the third strategy will tend to leave the agent with relatively inaccurate representations of the worst actions but highly accurate representations of the best ones. An agent who followed this strategy would tend to be inaccurate about which specific insulting comment was the absolute worst. However, it would tend to be highly accurate about which specific tactful comment was the absolute best, which was only second best, and so on. 
There is reason to expect that this third strategy might be especially adaptive. After all, the task at the time of decision making is to pick out the best of the available actions. Given this task, it is far more important to be able to accurately dis tinguish the best from the second-best than it is to accurately distinguish the worst from the second-worst. 
Within existing research in computer science, reinforce ment learning, and planning, there is a sizable literature on 
chance of gaining a reward, choosing the action with high est estimated reward, v(A), based on observations made so far. But the hypothetical simulations can be determined in any number of ways. We study five agent types representing different simulation strategies. Three of these use v(A) itself. 
1. Softmax: Stochastically chooses an action with probabil ity proportional to estimated success probability—that is, choose A with probability ∝ exp(v(A)/τ). Throughout we set the “temperature” parameter τ to 0.1. 
2. Softmin: Stochastically chooses an action with probabil ity proportional to estimated failure probability—just like softmax, but 1−v(A) is used in place of v(A). 
3. Infomax: Deterministically chooses the action that will maximize expected information gain. If action A has been observed to succeed s times and fail f times, the expected 
questions related to ours. Some of this research goes under the heading of “pure exploration” (e.g., Bubeck et al. 2011), 
information gain for A is s s+f 
s+1+f)   + 
s+f) − H(s+1 
H(s 
 
f 
H(s 
s+f+1)  , where H(p) is entropy of p. 
where an agent may test the effects of various actions with 
 
 
s+f 
out facing any concrete consequences. In this context, and 
s+f)−H(s 
in other contexts related to planning, one popular approach blends the first and third possibilities sketched above. That is, simulation is devoted to actions (or sequences of actions, 
4. Upper Confidence Bound (UCB): Deterministically chooses an action that balances estimated goodness with 
q2lnL 
expected information gain by maximizing v(A)+c 
LA, 
e.g., chess plays) that are assumed to be good, but also rela tively unexplored (see Browne et al. 2012 for an overview). Below, we explore the performance of agents showing (1) a bias towards good actions, or (2) a bias towards bad actions, (3) a bias towards unexplored actions, (4) a blend of good and unexplored actions, following the current state of the art in computer science, as well as (5) a baseline of random action selection. Options 1, 3, 4 and 5 give us something approxi mating a factorial design, crossing the factors of whether the agent shows a tendency to focus on actions regarded as good and whether the agent shows a tendency to focus on actions that have not been considered. This design thereby gives us a number of different opportunities to explore the impact of a tendency to focus on actions regarded as good. We can ask whether it is better to focus on actions regarded as good than simply to select actions at random (comparing 1 to 5). We can ask whether it is better to focus on actions regarded as good than to focus on actions that have not yet been considered (comparing 1 to 3). Finally, we can ask whether it is better to adopt a blend of focusing on actions that have not yet been considered and focusing on actions regarded as good than it is to focus only on actions that have not yet been considered (comparing 4 to 3). We now elaborate on the details of our setting and each of the five algorithms. 
Computational Experiments 
Consider a simple scenario in which some agent will be fac ing some future decision problem, but before this has the op portunity to perform a number of hypothetical simulations, punctuated by occasional concrete actions and observation of their consequences. These concrete actions we assume are chosen by the agent itself in such a way as to give the highest 
where L is the total number of observations, and LA is the number of times A has been observed (Browne et al., 2012). 
5. Random: Chooses an action uniformly at random. 
When making a concrete decision, all agents softmax-select an action using the currently estimated success probabilities. There are three key parameters in our scenario that are worth highlighting. The first is the number N of possible ac tions that the agent might consider. The second is the distri bution of rewards among these actions. For instance, do we expect there to be many good actions or only relatively few? The third parameter of interest is the number R of actions that the agent is able to retrieve for deliberation at decision time. We assume that online deliberation reveals the true expected value of an action to an agent, and thus the practical constraint on optimal decision-making is that the agent cannot deliber ate about all actions—i.e., R < N. Thus, as R approaches N, the importance of prior simulations decreases. 
Before going into the details of our experiments, here is a summary of the most important general patterns we observe: 
1. When comparing two agents that differ only in whether they incorporate a bias toward the good actions (Random vs. Softmax, or Infomax vs. UCB), the agent that focuses on good actions almost always performs better. 
2. Even an agent biased toward good actions who ignores uncertainty altogether (Softmax) generally outperforms an agent who minimizes uncertainty without focusing on good actions (Infomax). While this result appears to hold “on average,” there are scenarios where we see the opposite. 
518

Figure 1: Performance of different agent types over 80 learning trials (x-axis), with N = 10, 25, and 100 actions. The color key is as follows: yellow=Omniscient, orange=Softmax, red=Softmin, blue=Infomax, green=UCB, purple=Random. 
3. Whereas good-seeking and information-seeking tenden cies are both generally helpful, the helpfulness of a good seeking bias (unlike that of the information-seeking bias) seems to depend on the idea that the number (R) of actions we can retrieve at decision time is relatively small. 
These results thus demonstrate the sense in which the empiri cal tendency we observe may indeed be adaptive. The results also raise a number of subtle empirical and theoretical ques tions, which we will discuss further below. 
Initial Experiment: Actions Uniformly Distributed Let us begin by assuming that at decision time each agent will merely (softmax) retrieve a single action (i.e., R = 1). Intuitively, these are scenarios that allow no time for delibera tion, and decision making thus depends entirely on the agent’s view of the various actions as established through prior sim ulation and observation. Suppose each of the N actions has some “success” probability, and as a very simple first assump tion, suppose that these success probabilities are drawn uni formly from the unit interval (0,1). 
Fig. 1 shows the results. A learning trial involves observ ing of an action outcome (success/fail), where this action is chosen according to one of the strategies above, or (every fifth trial) by softmax selecting an action using current value esti mates. The latter is intended to simulate observations of ac tual (not just hypothetical or counterfactual) choices. At the end of L trials (x-axis) we assess each of the five agents ac cording to how well they perform by using their currently es timated action values. (Success probabilities are drawn inde pendently 5,000 times for each value of L, ensuring no corre lation.) We include results for an “omniscient” agent (R = N), revealing the average maximal success probabilities. 
The most striking pattern here is that a bias toward good actions is helpful no matter whether there are 10, 25, or 100 actions. That is (as a first illustration of point 1 above), the Softmax Agent, who biases simulation toward good actions, is significantly better than the Random agent; and the UCB agent, who mixes uncertainty minimization with a bias to ward good actions, drastically outperforms the pure Infomax agent. Moreover (in line with point 2 above), the Softmax agent has a significant advantage over the Infomax agent, whose performance is even slightly worse than Random. 
Varying the Distribution on Success Probabilities 
An obvious question is whether these results depend on the distribution of success probabilities being uniform. What if the distribution were instead highly skewed toward good or bad actions? Or what if most actions were intermediate with only a few very good or very bad actions? 
To investigate this question it is convenient to think of success distributions as themselves drawn from a beta prior, Beta(α,β), giving a distribution over Bernoulli success prob abilities. The two parameters α and β have the following sig nificance: α 
α+βgives the mean of the distribution, while α+β determines the shape of the distribution. For instance, if one of α or β is much higher than the other, this will result in a highly skewed distribution. The uniform distribution consid ered above is the simple case of Beta(1,1). 
Two other familiar distributions on Bernoulli probabilities are the so called Jeffreys distribution (which is equivalent to Beta(0.5,0.5)) and a standard bell-shaped distribution (we take Beta(2,2) as a representative example). These are both symmetric distributions, meaning that there are typically as many good actions as bad actions. However, they differ from the uniform distribution in that success probabilities are more concentrated either around 0.5 (bell-shaped) or closer to the extremes, 0 and 1 (Jeffreys). The results for these two envi ronments, with N = 25 actions, are not appreciably different from the uniform case, as shown in Fig. 2 
A somewhat different pattern is revealed in environments with highly skewed success probabilities. Again in Fig. 2 we show the results for two distributions that are skewed toward either very low or very high success probabilities (specifically these are Beta(.05,.5) and Beta(.5,.05), respectively). In these scenarios the advantage of good-seeking is either mod est or not present at all. What might explain this pattern? 
Consider the extremely positive skewed case (that is, Beta(.05,.5)). In this setting, the initial expectation for ev ery action is quite low (< .1), but there is a small set of ac tions that outperform this expectation significantly (typically at least one > 0.8). When it stumbles upon just one of these outliers in simulation, a Softmax agent will perseverate on it, to the exclusion of discovering one of the few others (at least one of which is likely to be quite superior). In con trast, the Infomax agent will gradually survey the full set of 
519
Jeffreys Bell-shaped 

Skew-positive (extreme) Skew-negative (extreme) 

Figure 2: Performance graphs for other distributions on action success probabilities. All simulations are with N = 25 actions. 
options and eventually discover the true optimal action. In other words, the positively skewed case presents the Softmax agent with “distractors” that capture simulation because they easily outperform the default expectation, while significantly underperforming the optimal action. 
These observations suggest at least two further questions. First, how “typical” are these environments? Second, does the pattern depend at all on the number (N) of actions? 
Turning briefly to this second question, suppose that the distribution on actions is the same, but that there are many more possible actions, say 100. Intuitively, the relative ad vantage of Infomax should be reduced in this setting as the number of candidate actions increases. This is because the Infomax approach of exhaustive, balanced search becomes especially inefficient as the space of actions grows. Fig. 3 shows that this intuition is indeed borne out with 100 actions. 

Figure 3: N = 100 actions, Skew-positive (extreme). 
In fact, with this scenario we see no apparent advantage of In fomax even over the Random agent (similar to what we saw 
in Fig. 1). This suggests that the advantage of information seeking in this scenario may not be particularly robust. This naturally leads us to the first question: what happens “on av erage” as we consider the entire parameter space? 
As an illustration, let us return to the setting of only 25 actions. The example distributions so far have been cherry picked based on specifically notable characteristics (flat, bell shaped, skewed). But what happens when we average over the environment parameters, α and β? To investigate this question we present the results of a large-scale experiment in which parameters are drawn from a hyper-prior. A relatively neutral hyper-prior, used often in cognitive science (see, e.g., Griffiths et al. 2008), defines reasonably “unbiased” distribu tions on the mean ( α 
α+β) and the shape (α+β), which together 
uniquely determine α and β. Specifically, we draw the mean from the uniform distribution on (0,1), while the shape is drawn from an exponential distribution with rate parameter 1 (meaning that the density function is simply e−x). The results in this broader setting are shown in Fig. 4. 

Figure 4: N = 25, averaged over environmental settings. 520
This reveals that, even in the scenario of 25 possible actions where we know the Infomax agent occasionally outperforms the Softmax agent, this makes up a relatively low-probability portion of the space. On average, Infomax performs only slightly better than a Random agent. But most importantly (and again, in line with point 1 above), both good-seeking agents (Softmax and UCB) have a clear advantage over the agents without this bias (Random and Infomax). 
Adding Reflective Deliberation 
Up to this point we have been assuming that each agent has the capacity to consider only one possible action at decision time. This means the quality of decision is dependent entirely on how well previous observations and simulations have bi ased better actions to come to mind. We now turn to scenarios in which an agent may have the capacity to deliberate over several possible actions and then choose whichever of these looks best upon reflection. In this kind of scenario, the aim of prior hypothetical simulation is somewhat broader. A strat egy is effective to the extent that the set of R actions brought to mind in a decision context will likely include at least one very good action (for this particular context). 
One straightforward way to formalize this type of scenario is to associate each action, not with a specific Bernoulli suc cess probability, but with a distribution over such probabili ties. The intended interpretation is that actions may be good or bad overall, but that the agent has the capability to figure out the actual success probability in a given circumstance, which might be quite different from what one would expect on average. Thus, in these simulations we again test how well different agents perform after learning, but decision making is assumed to work in a more sophisticated way. Instead of sampling an action in proportion to its expected goodness, each agent (softmax-)selects some number R of actions and then deterministically chooses whichever of these R actions turns out to be best in the given situation, reaping that reward. Intuitively, as R increases the learning problem becomes sig nificantly easier, since it is easier to find oneself with good options to consider at decision time. 
The setting we are now studying is similar to the scenario studied earlier (and presented in Fig 4), except that in the present case each action is associated with its own beta distri bution. Specifically, the distribution for each action is drawn from a hyper-prior with α + β ∼ Exp(1). Suppose first that the means α 
α+βare drawn uniformly. In Fig. 5 we report on the case of N = 25 actions with only 25 learning trials. We show the results with R = 1,...,10. 
As is evident from the figure, the differences among algo rithms is relatively pronounced at R = 1 and is still noticeable at R = 4. That is to say, even if an agent has the capability to retrieve and reflectively deliberate over 4 actions at deci sion time, simulating better actions may still have an appre ciably positive effect on an agent’s success. However, closer to R = 10 (just less than half the total) all differences vanish. 
Once again, we can ask the question of whether this pattern depends on the specific assumption that the (average) success 

Figure 5: Uniform distribution on success probability means. R actions retrieved at decision time. 
probabilities are uniformly distributed, i.e., that we expect the same number of actions for any particular range of success probabilities. As one would expect, when the distribution of means is at all favorable toward (generally) good actions, the differences among algorithms disappears even more rapidly: it is simply too easy to find at least one rewarding action. 
Other symmetric distributions with mean 0.5 show the same pattern as in the uniform case. In Fig. 6 are two ex amples, where the means themselves are drawn from Jeffreys and bell-shaped priors (again, Beta(.5,.5) and Beta(2,2)). 
Also presented in Fig. 6 is the scenario where means are drawn from a highly (positively) skewed prior.1In such a scenario almost all of the actions are almost always very bad, but there are a few that are typically very good. As dis cussed above, the aim of simulation is figure out which ac tions should be included among the R to retrieve at decision time. If there is only one “needle in the haystack,” then an agent will perform better the more likely it is to identify that uniquely good option. 
Notice that the x-axis on this third graph in Fig. 6 goes all the way to R = 20. Remarkably, the advantage of info seeking is still apparent even when the agent can retrieve and deliberate over 20 of the 25 possible actions. Also remark able is the observation that, while the Softmax agent clearly outperforms the Random agent, the UCB agent’s attention to goodness does not effect any significant gains over Infomax. A reasonable conclusion from this study is that, in such a sce nario, efficient and exhaustive pure search is hard to beat. 
This last case study uncovers an important caveat to the general finding that a bias toward good actions in hypothetical thinking is adaptive. In contrast to biases toward uncertainty minimization, the advantage of the good-seeking bias de pends on the assumption that the number (R) of actions that a person can consider in deliberation is small relative to the number (N) of possible actions that one could conceivably consider. A worthy hypothesis is that this is exactly the kind of situation people typically face. 
1We do not show the graph for the negatively skewed prior. In the present setting such a distribution results in too many good actions, and there are no observable distinctions among agents. 
521
Jeffreys Bell-shaped Skew-positive Figure 6: Distribution of means drawn from Jeffreys, bell-shaped, and positively skewed priors. 
Discussion 
We demonstrate three basic patterns in the way that a “good action bias” during offline simulation later improves online decision-making. First, we find that a good action bias im proves performance whether or not there is an additional bias to reduce uncertainty. Second, although an uncertainty reduction bias typically improves performance, its effect is usually smaller than the good action bias. Though we oc casionally see the opposite pattern—particularly in environ ments with just one or two very good actions and many quite poor actions (“needle in a haystack” problems)—the trend is robust when averaging over environmental parame ter settings. Third, the benefit of a good action bias depends strongly on the assumption that the agent is unable to retrieve and deliberate over a large number of actions at decision time. By contrast, the benefit that minimizing uncertainty confers (in those cases where the benefit is especially apparent) does not seem to depend on this assumption. 
These results strongly confirm our hypothesis that the bias people empirically show toward thinking about actions they deem good is adaptive. But the results also raise several new theoretical and empirical questions and possibilities. 
While our focus has been on the bias people show toward good actions, our results also confirm a hypothesis that hy pothetical and counterfactual simulation aimed at minimiz ing uncertainty would be quite helpful. We saw several in stances in which Infomax outperformed Softmax. But more strikingly, the UCB agent, who employs a blend of the two approaches, seeking out good as well as informative actions, outperforms every other agent in virtually every context. In deed, this is one of the reasons UCB-type agents have been so heavily studied in computer science, including in the setting of offline planning. Moreover, it is easy to imagine that in many domains that have this “needle in a haystack” character (in areas of science, for example), thinking more about less explored possibilities could be especially rewarding. 
Empirical research has clearly demonstrated the first type of bias, toward good actions. An obvious question is whether people also show a bias toward simulating more informative actions. That is, in a case where actions A and B appear equally good, but where the person simply has less informa tion or experience revealing how A might turn out, would she 
then show a tendency to think about A more than about B, for example, when imagining counterfactual scenarios? We leave this as an intriguing open question. 
Finally, the third result mentioned above—that the bias to ward good actions is only effective when the number R of actions retrieved is relatively small—may point toward an important fact about the kinds of problems that cognition is adapted to solve. It seems evident that in any given decision making context, people can only bring to mind and deliber ative over a very small number of options—certainly quite small relative to the number of conceivable actions. As our agent experiments show, these are precisely the cases where such a bias is advantageous. Perhaps this pervasive feature of our cognitive predicament explains a great deal about the nature of hypothetical and counterfactual thought. 
References 
Browne, C., Powley, E., Whitehouse, D., Lucas, S., Cowling, P. I., Rohlfshagen, P., Tavener, S., Perez, D., Samothrakis, S., and Colton, S. (2012). A survey of Monte Carlo tree search methods. IEEE Transactions on Computational In telligence and AI in Games, 4(1). 
Bubeck, S., Munos, R., and Stoltz, G. (2011). Pure ex ploration in finitely-armed and continuous-armed bandits. Theoretical Computer Science, 412:1832–1852. 
Griffiths, T. L., Kemp, C., and Tenenbaum, J. B. (2008). Bayesian models of cognition. In Sun, R., editor, The Cam bridge Handbook of Computational Cognitive Modeling. Cambridge University Press. 
Icard, T., Kominsky, J., and Knobe, J. (2017). Normality and actual causal strength. Cognition, 161:80–93. 
Kahneman, D. and Miller, D. T. (1986). Norm theory: com paring reality to its alternatives. Psych. Rev., 94:136–153. McCloy, R. and Byrne, R. (2000). Counterfactual thinking and controllable events. Memory and Cognition, 28:1071– 1078. 
Phillips, J. and Cushman, F. (2017). Morality constrains the default representation of what is possible. Proceedings of the National Academy of Sciences, 114(18):4649–4654. 
522
Arithmetic Sense Predicts Children’s Mathematical Achievement Better Than Arithmetic Fluency 
Soo-hyun Im (imxxx045@umn.edu) 
Sashank Varma (sashank@umn.edu) 
Department of Educational Psychology, 56 East River Road 
Minneapolis, MN 55455 USA 
Abstract 
Research on arithmetic competence has emphasized the importance of arithmetic fluency – the use of  efficient direct strategies when solving simple, conventional problems. Comparatively little attention has  been focused on arithmetic sense, which we define as the adaptive use of direct and indirect strategies  when solving complex, novel problems. The current study evaluates the new construct of arithmetic  sense and investigates its predictive relationship to mathematical achievement. 302 students in 6th grade  completed a battery of tests of their cognitive and numerical abilities, arithmetic fluency, arithmetic  sense, mathematics achievement, and pre-algebra skills. The central finding is that arithmetic sense is  the best single predictor of mathematical achievement. In particular, it is better than arithmetic fluency. 
These findings open a new pathway for improving school-aged students’ algebraic thinking and  mathematical achievement. 
Keywords: arithmetic fluency; arithmetic sense; mathematics achievement; pre-algebra 523
Do social media messages incorporated into television programming impact  learning? The effects of disposition to critical thinking  
Miwa Inuzuka (minuzuka@u-gakugei.ac.jp)  
Department of Education, Tokyo Gakugei University  
4-1-1 Nukui-kita-machi, Koganei, Tokyo, 184-8501 Japan  
Yuko Tanaka (tanaka.yuko@nitec.ac.jp)  
Graduate School of Engineering, Nagoya Institute of Technology  
Gokiso-cho, Showa-uk, Nagoya, 466-8555 Japan  
Mio Tsubakimoto (mio@fye.c.u-tokyo.ac.jp)  
Division of First-Year Education, Komaba Organization for Educational Excellence,  Graduate School of Arts and Sciences, College of Arts and Sciences, The University of Tokyo  3-8-1 Komaba, Meguro-ku, Tokyo, 153-8902 Japan  
Abstract  
The present study explores the impact on memory and attitude  change of social media messages that are incorporated into  television programs, and the interaction of such messages with  the viewer’s disposition to critical thinking. Sixty university  students were allocated to one of two experimental conditions  and viewed television content: social media messages were  included in only one condition. The results showed a  significant interaction between participants’ disposition  (Objectiveness) and the experimental condition: participants  with higher Objectiveness scores exhibited larger changes in  their attitudes. An analysis of 10 participants’ eye fixations  suggested participants’ tendency to change their allocation of  attention to different types of message over time. Additionally,  there was a significant correlation between the tendency to  focus on these messages and scores for disposition to critical  thinking (Objectiveness and Logical thinking). We discuss the  possible conclusions on the impact of showing social media  messages and the limitations of this study.  
Keywords: critical thinking; attitudes; mass media; social  media messages; thinking disposition.  
Introduction  
Learning from television programs and inclusion of  social media messages  
People learn not only in classrooms, but also from various  media sources. Among many, television programs serve as  one of the primary resources for everyday learning. Some  people even warn that television has gained the power to  control education (Postman, 1985). Although times have  changed and various new media have appeared, television  remains a major source of information.  
 In the investigation of everyday learning, it is important to  consider changes in attitudes as the results of learning as well  as memory of the contents. While this topic is less widely  considered in the literature on learning, attitude constitute one  of the essential results of learning from the perspective of  critical thinking. To think critically is “to make reasonable  decisions about what to believe and what to do” (Ennis, 1996).  
524
The goal of learning is not only to remember content, but also  to form an appropriate attitude that leads to reasonable  decisions based on the information obtained. The idea of  critical thinking in learning is especially important for  learning from television programs, since the issues tackled in  television programs are often relevant to viewers’ lives and  require them to decide what to believe and what to do.  
The ways television programs are produced have changed  over time, and new features are being used. For example, one  new feature is a real-time display of social media feeds. (See  Figure 1 for an example of how this feed may be presented.)  The inclusion of social media posts in television content is  not an issue limited to specific countries and regions since  we can find various examples of these television programs  (Inuzuka, Tanaka, & Tsubakimoto, 2017; Barra & Scaglioni,  2014) and accessing social media while watching television  is widely popular (Maruyama, Robertson, Douglas, Semaan,  & Fucett,2014; Maruyama, Robertson, Douglas, & Raine,  2017).  
Learning appropriately from television programs requires  viewers to integrate information from multiple media and to  examine information critically. These requirements become  even more important when television content includes social  media messages. Social media messages that mainly consist  of text, such as posts on Twitter, are more likely to be  incorporated into television programs and can show various  types of information: useful facts, critical opinions, and  unrelated comments. How do viewers read the messages, and  how do the messages impact their learning? How do viewers  pay attention to these messages? The present study focuses  on these questions.  
Learning from multimedia  
Learning from multimedia has been studied in educational  psychology and learning science. Mayer (2009) suggested the  “coherence principle”: learners understand the topic better  when irrelevant and seductive elements are removed from the  learning materials. The coherence principle can be explained  by the split-attention effect theory. Sweller and Chandler  
(1996) pointed out that a multimedia resource results in less  learning when it splits learners' attention. This attention split  is more likely to occur when the resource contains  information that shares the same modality and is incoherent  with the other information presented (Mayer, 2009; Mayer &  Moreno, 1998).  
The effect of incorporation of social media messages is  thus to violate the coherence principle; therefore, doing so  may cause the split-attention effect. Inuzuka et al. (2017)  investigated the impact of social media message presentation  using a fake television program. Participants were tested on  the content of the video after watching it, and the longer they  spent paying attention to the accompanying social media  messages, the fewer questions they answered correctly. Thus,  we expected the damaging effects on learning of presentation  of social media messages to be replicated in the present study.  
Attitude formation in learning from television  programming with social media messages  The direct effects of presentations of social media messages  on attitudes were not clear. Maruyama et al. (2017) revealed  that viewers’ attitudes were different in the direction of the  social media messages. However, it was not clear if the  presentation of social messages itself may influence the  viewers’ attitudes, since Maruyama et al. (2017) did not  investigate the condition without social media messages.  Although Maruyama et al. (2014) implied that only observing  social media messages would not affect viewers’ attitudes, a  controlled investigation is necessary to clarify the process of  viewing and the effects of messages on attitude changes.  
While Maruyama et al. (2014) suggested that the viewers  would not change their attitudes owing to the unbiased  messages, a different prediction is possible. Since people may  perceive information differently even when they see the same  message, it is highly possible that the viewers may analyze  the opinion of the social media messages differently from  each other. Thus, the viewers would develop a more positive  attitude when they see the presented messages support the  claims of the television program. Likewise, viewers would be  expected to adopt a more cautious stance when they think the  social media messages suggest opposing ideas.  
Furthermore, the inclusion of social media messages may  impact the learner’s attitude indirectly via their disposition to  critical thinking. Critical thinking has affective as well as  cognitive aspects. Disposition is included in the affective  aspect and represents attitude and tendency in thought. The  existence and influence of varying dispositions to critical  thinking have been demonstrated in several studies  (Hirayama & Kusumi, 2004; Stanovich & West, 1997). West,  Toplak, and Stanovich (2008) revealed that people’s disposition to thinking, such as their Open-minded thinking  and Need for cognition, explain performance in reasoning  independently of cognitive ability. Participants with a strong  disposition to thinking tend to make less heuristics-based  judgments and to be less affected by their own bias (West et  al., 2008). According to these studies, we expect that a viewer  
525
with a disposition to critical thinking would react more  appropriately to the presentation of social media messages.  Additionally, we can assume that differences in participants’  disposition will also affect the viewing process, such as how  they distribute attention when the messages are presented.  
Aim of the study  
The present study focused on how the incorporation of social  media into television programs affects learning. More  specifically, we examine the effects of presenting balanced  social media messages including supportive, opposing, and  neutral opinions. We hypothesized:  
(1) The direct impact of message presentation would be  seen in retention and attitude changes. The presentation of  messages would cause less memory retention by participants  because their attention would be disturbed. Participants  would change their attitudes based on how they regarded the social media messages. Thus, we expected that participants  would develop a more positive attitude when they regarded  the messages as supporting, and would become more cautious  when they perceived the messages as opposing the program’s  claims.  
(2) The impact of message presentation would be  moderated by viewers’ disposition to critical thinking,  because viewers with a strong disposition to critical thinking  would process the messages more appropriately than those  with a weaker disposition to critical thinking. In accordance  with this hypothesis, we expected interaction effects between  the presentation of messages and the learner’s disposition to  critical thinking. Additionally, participants’ eye gaze patterns  would differ depending on their level of disposition to critical  thinking.  
Method  
Participants  
Sixty undergraduates from two different universities  participated in this study after providing informed consent.  As a reward for their participation, they received a 1000  Japanese yen (approximately $9.50) cash voucher.  
Figure 1. A frame from the video material used in the present  study (condition with social media messages). This material  imitates a television program displaying a social media feed  saying, “It's nice to have microbes clean the river.”  
Table 1. Contents of each part of the fake television program and the number of messages presented  Part Time (s) Summary of the content Number of fake messages  Supportive Opposing Neutral  
1 0–374 Introduction: host welcomes the scientists and Scientist A  begins to introduce their work.  
2 375–554 Scientist A explains what “EM” is and how it works. Scientist  B poses questions.  
3 555–775 Discussion continues. Scientist B tries to dispute Scientist A’s  claims by pointing out the possibility that EM may cause  further water pollution.  
4 776–914 Discussion continues. Scientist A tries to rebut Scientist B’s  claims, and Scientist B claims that EM lacks rationale and  evidence.  
5 915–1084 Concluding remarks: both scientists summarize their opinion  of “EM.” Host makes closing comments.  
1 1 21  5 4 12  3 3 10  
2 3 16  1 1 4  
Materials  
Fake television program The video material used by  Inuzuka et al. (2017) was also used in this study (Figure 1).  The video was produced to mimic a scientific talk show. In  the video, a host and two scientists (actually actors) discuss  whether “Effective Microorganisms” (EM) are effective for  improving water quality. “EM” is a pseudoscience based on  the idea that a particular collection of microorganisms can  solve virtually all health and environmental problems. We  chose the topic because it is relevant to participants’ lives and  yet unfamiliar to them.  
The video was approximately 18 minutes long. We divided  the video into six parts of roughly the same duration (Table  1). We analyzed participants’ eye fixations during the parts  in which more than two supportive and opposing messages  were presented (parts 2 to 4).  
We presented fake social media messages that simulated  Twitter posts. The messages consisted of text, with each  containing one or two short sentences. We designed messages  to agree or disagree with a particular argument made by either  character (Scientist A or Scientist B). Messages were either  supportive (of the effectiveness of EM) or opposing. For  example, a supportive message could be one that agreed with  a clear and supportive statement on EM made by scientist A.  The rest of the messages were designed to be neutral to the  discussion. All the messages were inserted approximately  five seconds after the appearance of the relevant topic or  incident. As a result of this manipulation scheme, 66 fake  social messages (12 supportive, 12 opposing, 42 neutral)  were selected and inserted into the video. These messages  were placed at the bottom of the screen (Figure 1). 
Critical thinking disposition scale The critical thinking  disposition scale by Hirayama and Kusumi (2004) was administered to participants. On the basis of factor analysis  by Hirayama and Kusumi (2004), we selected four items for  each of four subscales: Awareness of logical thinking (e.g.,  
526
“I am confident in thinking things through accurately”),  Evidence-based judgment (e.g., “When I draw conclusions, I  place importance on the existence of evidence”),  Objectiveness (e.g., “I keep an objective attitude when I make  a decision”), and Inquiring mind (e.g., “I am interested in  people with different ways of thinking”).  
Attitude questionnaire We measured participants’ attitudes  toward the topic (the effectiveness of EM) before and after  watching the fake television program. The attitude  questionnaire contained two subscales with three items each:  positive attitude (e.g., “I think EM will somehow do some  good”) and careful attitude (e.g., “We need more  investigation on the effectiveness of EM”).  
Retention test A retention test with six quiz items (e.g.,  “What was the name of the river that Scientist A claimed that  EM cleaned up?”) was developed and administered after  participants had watched the video.  
Apparatus  
We collected eye-tracking data using the Tobii X2-60 eye tracker for half of the participants and the Tobii Pro TX300  for the other half. We used different devices because the  experiment was conducted in two different locations. The  present paper reports the analysis for the latter half of the eye tracking data. Because of the differences between the two  experimental settings, we judged that it would be  inappropriate to combine the data from the two different  experimental setups. The eye-tracker for the latter half of the  participants was paired with a 24" LCD monitor that was set  at a resolution of 1920×1080. The eye-tracker sampled the  position of participants’ eyes at a rate of 300 Hz and with an  accuracy of 0.4°. The video was shown at a size of 1280×720  pixels. The Tobii Studio software package logged gaze data.  We performed a 5-point calibration for each participant  before he or she watched the video. The area of interest was  set to the screen area where the fake social-media messages