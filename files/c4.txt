input data. The task is then re-framed as choosing how words map onto those concepts by ruling out impossible or less probable hypotheses until a consistent hypothesis is reached. The now seminal implementation of this is a Bayesian infer ence model from Xu and Tenenbaum (2007). Given some set of attested referents, a Bayesian learner evaluates all hypothe ses (h) for candidate word meanings according to Bayes rule, by computing their posterior probabilities (the likelihood of each hypothesis given the input data p(h|re f erents)), pro portional to the product of prior probabilities p(h) and likeli hoods p(re f erents|h). 
This family of models is global in two senses. First, cal culations of hypothesis-fit to the data are taken over all in put received. The learner would need to track some record of every attested exemplar in order to compute probabilities over them. The second global notion is that all alternative hypotheses are also calculated for goodness-of-fit to the in put data. This allows for global comparison not only between total input and some temporary hypothesis but between all hypotheses themselves. 
This makes an intuitive prediction dubbed the ‘suspicious coincidence effect’, that if a learner is exposed to some new word ‘fep’ (adapted from (Xu & Tenenbaum, 2007, p.249)): “It would be quite surprising to observe only Dalmatians called feps if in fact the word referred to all dogs and if the first four examples were a random sample of feps in the world. This intuition can be captured by a Bayesian infer ence mechanism that scores alternative hypotheses about a words meaning according to how well they predict the ob served data, as well as how they fit with the learners prior expectations about natural meanings.” 
Experimental Findings 
Experimental support for this prediction is offered from Xu and Tenenbaum (2007). The task is that participants are in teracting with an ‘alien’ puppet, ostensibly a monolingual speaker of ‘alien puppet talk’. On each trial, participants are presented with one or several training objects below the test grid along with an accompanying monosyllabic nonce word label. For instance, a participant may be shown a picture of a dalmatian with the label ‘fep’ and asked to pick out all the other ‘feps’ for the puppet from the simultaneously displayed test grid. The general findings in this paradigm are consistent across both child and adult participants. 
The test grid consists of photographs of real objects dis tributed across three different broad categories or genres (an imals, vegetables, and vehicles) to be used as stimuli. For any particular item, we operationally define a ‘basic-level’ term (Markman, 1990; Mervis, 1984; Rosch, Mervis, Gray, Johnson, & Boyes-Braem, 1976) as the label which would most likely be given to it in isolation (e.g. a dog) . In re lation to the basic-level term, that same item might also be referred to using a more narrow ‘subordinate-category’ label such as ‘poodle’ or a broader ‘superordinate-category’ label such as ‘animal’. Within each genre in the test grid, objects exist within these three hierarchical label levels. The set of 
‘test’ objects is consistent across trials with only their posi tion on the grid randomized. 
The broad experimental results are as follows: When only a single object is presented with a label, then subjects most commonly generalize to the basic-level category (e.g. select ing all dogs rather than only dalmations given that the sin gle training item was a dalmatian) (Xu & Tenenbaum, 2007; Spencer et al., 2011). This is consistent with the robust ef fects of a ‘basic-level’ bias (Markman, 1990). When multiple training examples are presented simultaneously, then gener alization is made narrower (e.g. selecting only dalmatians). This ‘suspicious coincidence effect’, that category narrow ness is linked to the size of the training sample, has been pre sented in favor of the Bayesian model of word learning. Yet, Bayesian inference is not the only family of models which make such a prediction. What’s more, global evaluation mod els face empirical challenges from conditions under which the ‘effect’ is not obtained. When the same training items are given a single label but displayed to participants in sequence rather than all at once, the SCE disappears (Spencer et al., 2011) (see Table 1 for a summary). i.e. all dogs are chosen rather than only dalmatians. The lack of a ‘suspicious coin cidence effect’ under serial presentation runs counter to the predictions of Bayesian inference. We note that the tempo ral gap introduced between referents under sequential vs. si multaneous presentation is only a single second between item displays. 
Category generalization is simply one of a wide range of cognitive tasks which exhibit a difference in outcome based on presentation style of exemplars. For instance, inductive category learning (Carvalho & Goldstone, 2015), visual pat tern differentiation (Lappin & Bell, 1972), relational reason ing (Son, Smith, & Goldstone, 2011), property projection (Lawson, 2017), etc. all show important differences under sequential vs. simultaneous presentation of stimuli. Taken to gether, the effects of presentation style across this wide range of domains and studies should be understood as an important phenomenon whose root causes make up a core aspect of cat egorization models. 
In the next section, we introduce the Na¨ıve Generalization Model (NGM), which implements a system of word learning as category formation. Learners extract properties of objects and store a mental record of them. Grounded in classic lit erature on category formation (Smith & Medin, 1981), these mental representations serve as the basis of word meanings and generalization. We describe the range of experimental findings captured by this model, including the effects of pre sentation style which are not accounted for under a model of Bayesian inference. 
Na¨ıve Generalization Model 
Word learning is to construct mental representations of words. While the Bayesian inference account of this process posits a global probability optimization over a large set of la tent hypotheses, we instead argue that word learning is a dy 
1447
Trial Type Level of Generalization Example Meaning 
Single Exemplar Broad “Dog” 
Multiple Simultaneous Objects Narrow “Dalmatian” 
Multiple Objects in Sequence Broad “Dog” 
Table 1: Basic generalization patterns from (Xu & Tenenbaum, 2007; Spencer et al., 2011). Both the size of the training set as well as the temporal manner of presentation have notable effects on the meanings posited by participants. 
namical process. Hypothesized representations are generated and only locally revised (as needed) based on input data. On this account, not all plausible hypotheses are simultaneously available. Meanings are built incrementally; any evaluation metric functions only over what is generated from input by the learner. 
As this does not necessarily maximize global probability of the output vocabulary, we term this model the Naive Gener alization Model (NGM). The term ‘naive’ here is intended to highlight the lack of an explicit optimization function. Rather, empirical pattern in word learning arise from largely mechanistic means. The NGM is able to capture a range of previous unaccounted for empirical findings in meaning generalization with respect to word learning. This includes the basic-level bias, the ‘suspicious coincidence effect’ that multiple simultaneous exposures to labeled training instances narrows hypothesized meanings, as well as the effects of pre sentation style which seemingly block the ‘suspicious coinci dence effect’. On the NGM, word learning is fundamentally a local mechanism by which mental representations of words are constructed rather than strictly evaluated. 
The generalization model does not function in isolation. The NGM is embedded within a larger understanding of word learning and is consistent with previous work regarding other stages in learning required for vocabulary acquisition. No tably this includes the mechanisms behind referent mapping posited in (Stevens, Gleitman, Trueswell, & Yang, 2016) The contribution of the NGM is to explain the way in which repre 
tures with elevated prominence (the driving force behind the basic-level bias) and one for all other features. Of course, the prototypical-ness of items within a class, or the salience of certain features depends on the class and the objects them selves. But this is simply a way of formally implementing the notion that some levels of generalization are privileged compared to others. It is of theoretical interest that the model functions with such an impoverished feature space. For in stance, the features in use are ‘flat’—without inherent hier archical relation between them—from the perspective of the learner. Yet the combination of these ‘flat’ features results in hierarchically nested extensions for word meanings. 
When a learner encounters a new word, the model sam ples from the appropriate salience distribution for each fea ture present. The result is a mental representation as a gra dient vector of features (Figure 1). Values are allowed to be any decimal between zero and one. The upper-bound of one is important because, conceptually, this corresponds to the fea ture being as present mentally as it is in the physical world. The learner iterates over the items displayed (if more than one present) and each feature present in the real world will be stored in mental representation at a proportion relative to that feature’s salience. 
A representation R is computed for a label w based on an example set of training items T by sampling all features ∀ f with salience S(f). This is adapted from classic approaches to category membership calculation (Smith & Medin 1981). 
sentations of meaning are created, updated, and maintained. Features 
Rw = ∑ t∈T 
∀ f ∈ tp,S(f) (1) 
Our implementation follows the classic literature on cate gories (Smith & Medin, 1981) by representing concepts as salient features. What we call ‘features’ are simply proper ties that hold for some item. While any two properties will be equally true of an object, in the sense that they are formal op erators, it should be clear intuitively that some properties are more salient than others. Consider the number 73. It is prob ably easier to determine that 73 is odd than it is to determine that 73 is a prime; it’s not that its prime-ness is less valid than its being odd, rather it is simply a matter of salience (i.e. how noticeable it is to an average person quickly). 
To simulate the degree to which a property is noticed by a learner, we model two normal distributions over salience. These ‘salience distributions’ differ only in mean; one for fea 
tp is the set of features (or properties) of the item t. S(f) is the salience function for a feature f which returns a value samples from the normal distribution with mean µ determined by the hierarchical level of f . 
While features for an object in the world are formal op erators, the mental stored values for a given feature are gra dient. Multiple (simultaneous) exposures for a label causes entrenchment (Lawson, 2017). We sum the values of each present feature (until reaching a ceiling condition). This is in line with previous featural implementations of categories, e.g. Kruschke (2008): ‘the simplest way [to learn associa tive strengths] is adding a constant increment to the weight whenever both its source and target node are simultaneously activated.’ 
1448
Computing distances 
The NGM makes a standard distance calculation between any new objects and extant mental representations. The compar ison of that value to a fixed parameter threshold determines category membership. Distance is then calculated between a test item and a mental representation for a label (Smith & Medin, 1981). 
There is a distance penalty for any feature present in the mental representation that is missing in the test object under consideration. This value is in accordance with the repre sented featural salience. However, there is no cost incurred for features which are present in a test item which are miss ing in the mental representation of a class. For example, every object in the world is going to be perceived as having some color value, but that color plays no role in these items mem bership in any of various natural classes being learned here. 
Mutual exclusivity is a powerful and well-established con straint in word learning (Markman, 1990). We formally im plement a feature-level adaption of this in the NGM by allow ing properties in conflict to block addition to a single mental representation. 
Learning by presentation style 
When trained on a single exemplar, the experimental finding is that learners’ generalization to basic-level items occurs a substantial proportion of the time. This is driven by the privi leged status of certain features for generalization over others. When training objects are initially presented simultaneously, whether that is a single exemplar or many, then a hypothe sis category needs to be formed in a single shot. Thus when they are co-present, the function which extracts features from a scene is able to essentially compare exemplars to exem plars. When features are activated multiple times, they un dergo entrenchment —creating stronger links in mental rep resentation (Smith & Medin, 1981). Properties which, when encountered in isolation, would not have a significant effect on stored meaning can, through this entrenchment, lead to more narrow-generalization. The NGM’s mechanistic account of featural entrenchments thus makes the same predictions are Bayesian inference with respect to the ‘suspicious coinci dence effect’ under parallel presentation. 
When the same stimuli are presented in sequence rather than in parallel, learners’ generalization occurs primarily at the basic-level rather than the subordinate level. Even though training objects are shown to learners multiple times, the learner only constructs an initial hypothesis only once. Af ter the first exemplar has disappeared from view, the learner needs to construct some mental representation for the pre sented word. Once a mental representation exists, there is no onus to change it significantly so long as subsequent objects picked out by the word are congruent with what’s stored. This process is analogous to localist models of referent learning such as Pursuit (Stevens et al., 2016). Learners select a single hypothesis and either stick with it if evidence is consistent, or move to a new hypothesis when faced with inconsistent 
evidence. When subsequent training instances appear, the original exemplar(s) have disappeared from view with only the generated category remaining. This means that learners are essentially comparing new exemplars to a category repre sentation rather than directly comparing exemplars with each other. Since all of these trials concern levels of generaliza tion, no new training item will disprove an over-generalized hypothesis. Therefore, learners will simply continue along with whatever initial hypothesis was created. Repeat ex posures increase a learner’s confidence in the hypothesized meaning rather than triggering any change in the word’s inter nal contents. This continues until some ‘convergence point’ is reached and a semantic representation is more or less fixed. Such a convergence point is a required component of any model of word learning. The cause of the ‘basic-level bias’ on sequential presentation trials is the same as in the single exemplar trails: certain types of features lead to privileged levels of generalization. 
Results 
Qualitative Evaluation 
When evaluating the output of a computational cognitive model with respect to human experimental performance it is important to keep in mind the status of qualitative effect pres ence. The evidence that results from experiments such as Xu and Tenenbaum (2007); Spencer et al. (2011) is informative largely on the basis of indicating which experimental condi tions drive a significant difference in participant performance. It is the presence of the performance gap rather than the ex act percentage of test items that some sample of participants selected which we should primarily be concerned with. The gap in basic-level items selected when training objects were presented in parallel vs. sequentially happens to be approxi mately 40% (Spencer et al., 2011). The interpretation of the experiment, however, would be the same whether the size of that gap turned out to be 35% or 65% instead. 
It is important for the validity of a parameter-dependent cognitive model that there exist a set of input parameters which results in approximating true human performance on a task. However, another crucial question is to determine the degree to which qualitative effects of model performance are driven by factors internal to the model itself or dependent on specific parameter inputs. 
To investigate the parameter independent performance of the NGM, we measured the proportion of parameter config urations which result in qualitatively the same trends as em pirical output from Spencer et al. (2011). This is measured in two parts. First that the ‘suspicious coincidence effect’ is present under parallel presentation trials. This is defined as the proportion of basic-level test items selected being at least 15% lower in the parallel presentation trial compared with the single exemplar trial. Secondly that the sequential presenta tion demonstrates the same basic trend as baseline general ization. This means that the proportion of test items selected in the sequential condition be within 15% of the single ex 
1449
  

Figure 1: Table showing results comparison between experiments run in Spencer et al. (2011) and output of the NGM. Standard deviations are given in parentheses. 
emplar trials. 15% was chosen has a representative sample standard deviation based on the results reported in Spencer et al. (2011). 
With multiple parameters in the NGM (means for the salience distributions as well as standard deviation, cate gory distance cutoff) a large number of parameter configu rations are possible for the model to be seeded with. A grid search with step-size of 0.1 resulted in 432 tested configura tions each run with 1000 simulated ‘participants’. The out put trends of the NGM were qualitatively consistent with hu man performance on all trials. The mean size of the ‘suspi cious coincidence effect’ under parallel presentation was µ = 58.18% with standard deviation σ = 17.29%. Under sequen tial presentation the mean gap in generalization from baseline was µ = 0.8% with standard deviation σ = 0.7%. The quali tative trends required to be captured by the model are, on the whole, independent of individual parameter setting. 
Quantitative Evaluation 
Parameter tuning and quantitative testing of the computa tional model was performed by feeding in the same input data from published experiments and scoring the resultant output like the empirical findings. There are seven different trials types (single exemplar trial, three trials with objects presented in parallel, and three trials with objects presented simultane ously) which we would like to model the experimental find ings for. To ensure fair evaluation (and avoid over-fitting), we train the model on only two of the cases originally described 
in Xu and Tenenbaum (2007) —training over a single exem plar and training over three basic-level matches in parallel. Testing was then performed on all experimental conditions from Spencer et al. (2011) varying the hierarchical organiza tion and presentation style of the input. Parameter tuning was performed by running a five-way stepwise (step size = 0.1) grid search (two salience distributions means, salience stan dard deviation, distance threshold, mutual exclusivity thresh old). 
For each trial, there are three different generalization levels (sub, basic, super) each with a different proportion. To com pute the distance from a parameter setting for the model and the empirical data we sum the absolute value of the difference for the proportion for each level. Each trial configuration was run with 1000 simulated ‘participants’ in the model. 
This model captures a broad range of experimental find ings in category generalization as shown in (Table 1). The mean divergence per trial between the experimental data and the output of the model is 5.67%. 96% of trial configurations were within a single standard deviation of the empirical find ing. 
Overall, the output of the NGM is strikingly consistent with human performance on generalization tasks in word learn ing. Several general patterns are captured here; the strong basic-level bias in generalization from a single, labeled train ing instance, the ‘suspicious coincidence effect’ that general ization is more narrow when multiple labeled training items are presented simultaneously, as well as the fact that this ef 
1450
fect is sensitive to temporal manner of presentation. While for practical reasons the NGM was evaluated on a set of seven particular experimental conditions, the underlying trends in generalization are robust under numerous related conditions (Gentner & Namy, 1999; Lawson, 2017; Spencer et al., 2011) Capturing and explaining these trends in a single model is an important contribution. 
General Discussion 
Previous ‘hypothesis evaluation’ models of word learning such as (Xu & Tenenbaum, 2007) attempt to solve the prob lem of generalization by globally computing the posterior probability of each potential meaning compared to an accu mulated set of attested exemplars. While some experimen tal evidence seems to support this type of globally optimized computation (Xu & Tenenbaum, 2007), other experimental findings (Spencer et al., 2011; Lawson, 2017) are in conflict. The manner in which a fixed set of stimuli is presented to learners (whether simultaneously or in quick succession for instance) induces a large difference in inferred word mean ings. Models which attempt to maximize the output proba bility over hypothesized lexicons cannot account for this ef fect in a straight-forward manner. To date, no model of word learning has been able to fully capture the range of learner behavior on these tasks. 
The Na¨ıve Generalization Model (NGM) presented in this paper offers an explanation of word learning phenomena grounded in category formation (Smith & Medin, 1981). We argue that word learning is fundamentally to construct men tal representations of words rather than strictly evaluate them. This is a mechanistic yet dynamical process in which hypoth esized representations are generated and only locally revised (as needed) based on input data. This does not necessar ily maximize global probability of the output vocabulary, but rather the evaluation metric for meanings functions only over what is generated from input by the learner. The NGM ex plains the mechanism behind meaning generation and gener alization for word learning and category formation in a man ner that is consistent with and complementary to localist mod els of referent mapping. Taken together, a more complete picture of word learning begins to emerge. 
The NGM correctly predicts the sensitivity of learners to presentation style. These effects of presentation style are ro bust across related domains, so the explanation offered by the NGM is a real contribution and not simply a method of making rational-level models fit a set of data. 
Acknowledgments 
I would like to thank to Charles Yang for helpful advice and feedback throughout this work. Also thank you to John Trueswell and the Penn Language Development and Lan guage Processing Lab for important discussion. 
References 
Carvalho, P. F., & Goldstone, R. L. (2015). What you learn is more than what you see: what can sequencing effects tell 
us about inductive category learning? Frontiers in psychol ogy, 6. 
Gelman, S. A., & Markman, E. M. (1986). Categories and induction in young children. Cognition, 23(3), 183–209. Gentner, D., & Namy, L. L. (1999). Comparison in the devel 
opment of categories. Cognitive development, 14(4), 487– 513. 
Gillette, J., Gleitman, H., Gleitman, L., & Lederer, A. (1999). Human simulations of vocabulary learning. Cognition, 73(2), 135–176. 
Kruschke, J. K. (2008). Models of categorization. The Cam bridge handbook of computational psychology, 267–301. Landau, B., Smith, L. B., & Jones, S. S. (1988). The impor 
tance of shape in early lexical learning. Cognitive develop ment, 3(3), 299–321. 
Lappin, J. S., & Bell, H. H. (1972). Perceptual differentia tion of sequential visual patterns. Attention, Perception, & Psychophysics, 12(2), 129–134. 
Lawson, C. A. (2017). The influence of task dynamics on in ductive generalizations: How sequential and simultaneous presentation of evidence impact the strength and scope of property projections. Journal of Cognition and Develop ment(just-accepted). 
Markman, E. M. (1990). Constraints children place on word meanings. Cognitive Science, 14(1), 57–77. 
Mervis, C. B. (1984). Early lexical development: The con tributions of mother and child. Origins of cognitive skills, 339–370. 
Rosch, E., Mervis, C. B., Gray, W. D., Johnson, D. M., & Boyes-Braem, P. (1976). Basic objects in natural cate gories. Cognitive psychology, 8(3), 382–439. 
Smith, E. E., & Medin, D. L. (1981). Categories and con cepts. Harvard University Press Cambridge, MA. Snedeker, J., Gleitman, L., et al. (2004). Why it is hard to label our concepts. Weaving a lexicon, 257294. Son, J. Y., Smith, L. B., & Goldstone, R. L. (2011). Con necting instances to promote childrens relational reason ing. Journal of experimental child psychology, 108(2), 260–277. 
Spencer, J. P., Perone, S., Smith, L. B., & Samuelson, L. K. (2011). Learning words in space and time probing the mechanisms behind the suspicious-coincidence effect. Psy chological science, 22(8), 1049–1057. 
Stevens, J. S., Gleitman, L. R., Trueswell, J. C., & Yang, C. (2016). The pursuit of word meanings. Cognitive Science. Waxman, S. R., & Markow, D. B. (1995). Words as invita tions to form categories: Evidence from 12-to 13-month 
old infants. Cognitive psychology, 29(3), 257–302. Xu, F., & Tenenbaum, J. B. (2007). Word learning as bayesian inference. Psychological review, 114(2), 245. Yu, C. (2008). A statistical associative account of vocabu lary growth in early word learning. Language learning and Development, 4(1), 32–62. 
1451
Child-guided math practice: The role of regulatory emotional self-efficacy for  children experiencing homelessness 
Macey D. Cartwright1 (cartwrmd@mail.uc.edu) 
Heidi Kloos1 (heidi.kloos@uc.edu) 
Quintino R. Mano1 (quintino.mano@uc.edu) 
Casey Hord2 (casey.hord@uc.edu) 
1Department of Psychology, University of Cincinnati, Cincinnati, OH 45221 USA 
2Special Education, University of Cincinnati, Cincinnati, OH 45221 USA 
Abstract 
A child’s perceived ability, over and above actual ability,  matters for various behavioral outcomes, academic or personal.  In the current paper, we looked at one type of self-efficacy:  children’s perceived ability to regulate their own negative  emotions. Our question was whether regulatory emotional self 
efficacy (RESE) affects math learning for children who are  faced with homelessness. The specific math enrichment  centered on child-guided math practice: Children were given a  commercially available app and encouraged to pick out their  own practice problems. Our thought was that RESE might  affect children’s learning when they are given a chance to  determine their own math-practice path. The goal of the current  study was to establish this link empirically. The sample  included 5- to 12-year-olds who attended a summer program  organized for homeless children. Results confirmed our  hypothesis. Children who scored lowest on the RESE scales (N = 40) benefited less from the math practice than children who  scored highest (N = 46). Specifically, the improvement in math  was correlated with number of practice sessions only for high RESE children, not for low-RESE children. These results  suggest that RESE is an important factor in learning math, to 
be considered when developing student-centered pedagogy. 
Keywords: learning; math competence; homelessness;  summer camp  
Introduction 
Math is a subject that many children find difficult to master.  Even thinking about math can cause children to experience  negative emotions (Wigfield & Meece, 1988). Therefore, it is  likely that regulatory emotional self-efficacy (RESE) plays  an important role in how children learn math. If children do  not believe that they can manage their negative emotions,  they are not likely to respond well when faced with a difficult  math problem. The population of children in the current study are experiencing homelessness and attending a summer  enrichment camp. As a result, they are at a higher risk of math  difficulty and low RESE than their housed peers, adding to  the troubles. In the current paper, we investigate the role of  RESE in how these children respond to child-guided math  practice. In what follows, we will first discuss the nature of  math and why child-guided practice is necessary. We will then consider the importance of RESE. 
A Model of Math Learning 
Because math is persistent and cumulative, it becomes easy  for children to get left behind early. When the material  advances quicker than children are able to comprehend, they  continue to fall further behind. For example, soon after  children are introduced to numerical quantities and counting,  they are expected to learn addition, where an ability to count  is crucial. Beyond that, addition is nested within  multiplication, so if addition is not understood, multiplication  is much more difficult to understand and execute. 
The material in the classroom moves at a quick pace: by  the end of third grade children are expected to multiply whole  numbers, and by the end of fifth grade they are expected to  multiply fractions. Add to that all of the underlying concepts  necessary to understand multiplication and fractions, and  children can easily become overwhelmed, particularly those  who are already struggling. Without proficiency in underlying concepts, a child is likely to struggle with  multiplication of fractions when this topic is introduced  through formal classroom instruction. In turn, this could 
potentially render formal instruction ineffective, leaving the  child with little more knowledge than they entered with. The importance of math may not be explicitly evident to  children, and its purpose can easily be misconstrued as  arbitrary. Even children who excel at the subject are not likely  to be interested in seeking out math content during summer  months, let alone children who fell behind. In contrast,  exposure to reading material can be more enjoyable for  children. They can choose to read what they like, so reading  practice can relate to any range of interests they may have.  This is not necessarily true of math, which is limited in what practice can pertain to that is commonly of interest to  children. In addition, math practice is not as easily carried out  when compared to reading practice. For reading, there are  public libraries that provide free books, and 70% of parents  interviewed in a 2013 report claimed to have taken their  child(ren) to a library within the past year, 87% of those visits  resulted in the child checking out a book (Pew Research  Center, 2013). No such opportunities are publicly available  for math practice, leaving caregivers with the options of  paying for math enrichment programs or taking the time to  developing practice problems and deliver feedback on their  
1452
own. Neither of these options are likely to be feasible for  parents of low-income households. 
During the school year, students often sit in a large  classroom with many other students; they receive the same  instruction in the same environment from the same teacher.  This has the advantage of being low-cost: typically, free for  the families, and of minimal cost to the school. It also ensures  uniformity, that each child is given the same information.  Therefore, much research is invested in what teaching  strategies and curriculum are most effective (National  Mathematics Advisory Panel, 2008). However, there are a  multitude of factors that differ among students that contribute  to their differences in mathematical abilities, and those who  perform the lowest can easily be overlooked. With many  classrooms reaching 30 or more students, it can be a difficult  feat to address and account for each individual student’s  strengths and weaknesses in an instructional setting. 
An individualized approach is an alternative option that  might better account for each students’ abilities (Horak,  1981). While it is clear that students who are behind need to  catch up, they cannot easily do so by learning the more  advanced topics at the ability of some of their peers. Whole 
class instruction, where a larger group of children are taught  by a single person delivering a lesson to the entire group, has  the disadvantage of neglecting children who have fallen  behind. Specifically, whole-class math instruction can be  problematic for children from low-SES communities who are  more likely to be low-achieving and lack the most resources  outside of school. Further, Klem & Connell (2004) have  stressed the importance of personalization in learning  environments, where the students feel that they are supported  by the teacher. With this support comes a level of  individualization for the teacher to fully engage with each  student. 
Technology-based interventions are a promising way to  carry out individualized approaches. These are typically  designed so that children can work on math appropriate for  their skill level, they are engaging by allowing children to  work toward a goal, and they provide immediate feedback  (Gross & Duhon, 2013). One commonly used intervention is  called “Math Facts in a Flash” (MFF). MFF is designed to  improve math fluency and automaticity on the four basic  operations. It is a computer-based software, hierarchically  organized so that children must master a level before moving  on to the next (Burns, Kanive, &DeGrande, 2010). MFF has  been effective for improving elementary school children’s  performance regardless of their skill level, and in some cases  with significantly fewer children rendered at-risk for math  failure at the end of the intervention (Burns et al., 2010).  Another computer-based math intervention that led to improved math performance had children practice math at  their own level at home on a computer game for 15 minutes  each day (Kucian et al., 2011). Beyond improvements in  math performance, math programs that utilize tablets  specifically are beneficial for positive self-perceptions, self efficacy, and increased motivation (Hilton, 2016). 
The Importance of RESE 
Emotional intelligence is a term used to describe the ability  to regulate emotions and navigate information regarding  emotions (Mayer, Salovey, & Caruso, 2004). Studies have  demonstrated that emotional intelligence has a positive  relationship with academic success (Chew, Zain & Hassan,  2013). For example, it has an effect on performance on  cognitive tasks, above and beyond that of general intelligence  (Lam & Kirby, 2002). Regulatory emotional self-efficacy  (RESE) is defined as the perceived ability to regulate one’s  own negative emotions. 
General self-efficacy, or perceived control over one’s  situation, plays an important role in well-being (Bandura,  1997). Without confidence in one’s own abilities, there  would be no incentive to push through barriers and persist in  achieving an outcome. If an individual questions whether his  or her actions will affect an outcome, even the smallest  challenge is likely to become a deterrent (cf., Ajzen, 2002).  Self-efficacy affects the perception of roadblocks, which, in  turn, affects the degree of persistence and resilience  (Spillane, Reiser, & Reimer, 2002). Thus, perceived  competence has its own value in well-being, over and above  actual competence. 
Math is challenging for many children, which can lead  to negative emotions. During child-guided practice, if a child  chooses a problem that is too difficult, the emotions that follow direct their decisions about future practice. For  example, if a child with high RESE chooses math that is too  difficult, causing him or her to become frustrated, he or she may recognize those feelings and use the opportunity to  switch to math at a more appropriate level of difficulty. On  the contrary, if that child with low RESE chooses math that  is too difficult and becomes frustrated, he or she may become  overwhelmed and discontinue practice altogether. This might  also affect their willingness to participate in the future. 
Overview of the current study 
It is possible that RESE is an important factor in how a child  practices math. The potential link between RESE and math  practice is particularly interesting when considering the  complications that homelessness presents. In this study, we  seek to investigate how children experiencing homelessness 
respond to child-guided math practice as a function of their  RESE. During a 7-week summer day camp, children engaged  in practice sessions several times a week. During a typical  session, children used the IXL app on a touchscreen tablet for  40 minutes, and facilitators worked in small groups of two to  four children. Math competence was assessed via two  measures: math fluency and math comprehension. RESE was 
assessed via a survey (Canfield, Cartwright, Kloos, Schmerr,  & Aigner, 2018). We predict that more practice will be  correlated with an improvement in math comprehension for  children with high RESE, but not for children with low  RESE. 
1453
Method 
Sample 
Children included in this study were 182 elementary-school  children who attended the summer camp, ages 5-13 years (M  = 8.93, SD = 2.14). Overall, 44.50% of the children were  girls, 50% were African-American and 38.46% were  Caucasian. They met the guidelines for experiencing  homelessness according to the non-profit group that  organized the summer camp. Participation in the camp was  free to the children, and they were provided with  transportation, as well as two meals.  
Summer Camp 
The summer camp was held at two different sites (A and B),  five days per week, for seven consecutive weeks. There were  
three groups of children, loosely organized by the grade level  children entered after the summer. The demographics of  children in each of the groups are reported in Table 1, broken  down by site. Academic enrichment was offered during the 
mornings (9 AM -12 AM), and child-guided practice took  place during some part of that time. Specifically, practice was  offered three times a week at Site A and one time a week at  Site B. Each practice session lasted approximately 40  minutes. Children occasionally received additional math  lessons, not in conjunction with the child-guided practice, by  a certified teacher during the additional day of academic  enrichment. 
Table 1: Demographics of the summer camp, organized by age, group, and site. 
Group 1 
(Grades 1-2) 
Group 2 
(Grades 3-4) 
Group 3 
(Grades 5-6) 
Site A Site B Site A Site B Site A Site B N 18 45 17 34 20 48 Age in years 
M (SD) 6.52(.69) 6.63(1.04) 8.53(.88) 8.89(.63) 11.29(.82) 11.14(1.00) Gender (%) 
Female 33.33 46.67 52.94 47.06 45.00 41.67 Race (%) 
African 
American 11.11 68.89 5.88 85.29 5.00 56.29 
Caucasian Biracial 
66.76 22.22 
26.67 4.44 
82.35 11.76 
8.82 2.94 
75.00 2.00 
29.17 10.42 
Facilitators 
College students were recruited to serve as facilitators, with  approximately 6-7 in attendance during each session. They  were given a brief 10-minute training that discussed their role  in the program, and additional coaching was provided onsite.  Facilitators were discouraged to explain a math concept or  procedure to the child. Feeling the urge to do so anyway  could serve as a sign that the child is working on a problem  set that is too difficult. Rather than explaining math to  children, children should be encouraged to switch to  something easier. In contrast, if a child was growing bored  and unengaged because the problem set was too easy, the  facilitator should suggest a more difficult problem set. The  monitoring and modulating of problem difficulty were central to the program and thus was encouraged throughout. 
Materials 
Math practice app. IXL organizes problems pursuant to the  common core by grade level, math topic, and specific  variations within a math topic. A problem set is one of those  variations (e.g., multiplication tables up to 12), nested within  a topic (e.g., multiplication fluency), which is nested within  a grade level (3rd grade). A problem set presents individual  
problems one at a time in a simple format, free from  distracting colors and designs. After a correct answer, the  child is greeted by simple positive feedback (i.e.,  “Fantastic”), awarded points, and then presented with the  next problem. After an incorrect answer, a few points are  deducted, the child is given the correct answer and a  suggestion for how to solve the problem. They can easily  click to move on to the next problem (with or without reading  the explanations). Point progress is displayed as a bar at the  top of the screen, with a goal of 100 points per problem set. 
Curriculum matrix. A unique matrix was created for  each age group. Matrices were strategically designed to  include a range of six or seven topic domains that  corresponded with the common core for relevant grade levels.  Specifically, for Group 1, the domains were: counting,  patterns, addition, subtraction, word problems, and fractions.  For Group 2, the domains were: counting, addition,  subtraction, word problems, fractions, multiplication, and  division. Finally, for Group 3, the domains were:  addition/subtraction, word problems, fractions,  multiplication, division, decimals, and pre-algebra. Within  each topic, there were several levels of difficulty. Each cell  of the matrix referenced specific problem sets in IXL so that  children and facilitators could easily find appropriate practice  problems that fit the child’s skill level. 
1454
Measures 
Math fluency. The standardized T10 subtest from  Version IV of the Woodcock-Johnson test battery was used  to capture a child’s math fluency. It is a two-page 3-minute  timed test, comprised of single-digit addition, subtraction,  and multiplication problems. 
Math comprehension. Math comprehension was  assessed using an existing readiness test (Excel Math  curriculum), modified slightly. Specifically, Group 1  received the readiness test for 1st and 2nd grade; Group 2  received the readiness test for 3rd and 4th grade; and Group 3  received the readiness test for 5th and 6th grade. We also added  a set of fraction problems to supplement the tests. Each group  received 4-9 fraction problems, with increasing difficulty,  reflecting the grade-specific fraction problems in IXL. 
Regulatory emotional self-efficacy survey. An eight question survey was used to capture a child’s perceived  ability to manage their own emotions. Specifically, it  consisted of questions directed at a child’s ability to control  negative emotions such as “I know how to calm myself down  when I get scared” and “I know how to make myself feel  better when I start worrying about something.” Children  responded on a 5-point Likert-scale, ranging from strongly  disagree to strongly agree. 
Procedure of Data Collection and Scoring Math fluency. Children were assessed on the math  fluency measure during the first and last week of the camp. It  was administered by a researcher to the entire classroom,  while facilitators enforced the procedures in small groups.  Children were instructed to start on the first page, answer as  many questions as they could during the 3-minutes, and skip  any that they did not know. The raw score was calculated as  a number of how many questions were answered correctly. Math comprehension. Children were assessed on the  math comprehension measure during the first and last week  of camp. Similar to the math fluency test, it was administered  by a researcher to the entire class, while facilitators enforced  the procedures in small groups. Children were instructed to  complete only the problems they liked, and if they did not  like a problem, or did not know how to do it, to simply cross  the problem out and move on, rather than get too frustrated  or overwhelmed. Teachers, volunteers, and facilitators  assisted if children had questions, but they did not help with  answers. Assessments were scored as a percentage of  questions answered correct (e.g., if a child answered 48% of  the questions correct, they received a score of 48). Regulatory emotional self-efficacy. During the first  week of camp, the RESE survey was distributed as part of a larger battery of surveys to children. Only children who were  present on the given day of testing received the survey.  Children who required reading assistance were read each  question aloud by a camp staff member or volunteer. RESE  was scored as an average of each 1-5 response. 
Procedure of a Practice Session 
After logging in, facilitators instructed children to begin their  practice with a warm-up problem set. These problem sets  were designed to be easy and quick, to get all children started  on time and actively engaged in math. During the first few  weeks, the warm-up problem set was pre-determined by the  research team. Children were given a choice between two  problem sets of the same topic, and while both were easy, one  was slightly more difficult than the other. For the remainder  of the weeks, children were encouraged to decide their own  warm-up problem. After each child reached 100 points on  their warm-up problem, they earned a small piece of candy. 
During the rest of the session, children had the  opportunity to determine much of what they practiced  themselves. Suggestions were occasionally given and  progress was monitored by the facilitators, but children were  encouraged to self-guide their own practice. In conjunction  with the matrices, the child and facilitator found appropriate  problem sets and the child worked largely independently.  Five minutes before the end of the session, a prompt was  given to the class to finish what they were working on, log  out, and put all materials away. Once all of these were completed, the child earned a second piece of candy. 
Results 
To determine whether child-guided math practice improved  children’s math competence, we looked at (1) the number of  practice problems, (2) children’s changes in math fluency,  and (3) children changes in math comprehension. Table 2  provides the descriptive statistics of these variables. Using  the data from the RESE survey, the children were divided into  two groups; children who scored above the mean of 3.95 (SD = 0.80) were placed in the high RESE group (M = 4.52, SD =  0.40), while children who scored below the mean were placed  in the low RESE group (M = 3.30, SD = 0.64). When  comparing the means of the high and low RESE groups, the  results of the Levene’s test for homogeneity of variance were  significant, F = 7.27, p = .01, so homogeneity was not  assumed. An independent samples t-test revealed a  significant difference between these two means t(62.13) = - 10.29, p < .001. In the context of these data, the terms high 
and low are used relative to the mean of the sample, rather  than to indicate high and low on the 1-5 scale (where 3.30  would represent neutral rather than low).  
At pre-test, there was no difference in performance  between the high- and low-RESE groups on either math  fluency (t[74] = .68, p = .50), or math comprehension (t[83]  = -.58, p = .56). For children with high RESE, as the number  of practice sessions increased, math competence significantly  increased from pre- to post-test, including both math fluency  (r[40] = .35, p = .02) and math comprehension (r[39] = .39,  p = .01). However, for children with low RESE, as the  number of practice sessions increased, math competence did  not increase, either for math fluency (r[32] = .06, p = .74), or  math comprehension (r[34] = .15, p = .38).  
1455
Table 2: Descriptive statistics for practice and math  competence measures. 
Group 1 Group 2 Group 3 
# of sessions 
et al., 2008; Kirk, Schutte, & Hine, 2008). Therefore, a  child’s belief that they can regulate their emotions is  important in their ability to do so successfully. Given the link  between RESE, emotional intelligence, and academic  success, a logical prediction would be that children with  
N 
Range 
M  
(SD) 
Math fluency*  N 
Pre 
Range 
M  
(SD) 
Post 
Range 
M  
(SD) 
Math  
comprehension** N 
Pre 
Range 
M  
(SD) 
Post 
Range 
M  
(SD) 
34 
1-20 
9.91 
(5.89) 33 
0-43 
11.48 
(11.33) 
0-40 
11.24  (10.21) 
27 
2-73 
35.76 (21.97) 
2-93 
44.07 (24.68) 
35 
2-19 
7.94 
(5.79) 31 
4-90 
35.61 
(18.12) 
6-90 
37.81 
(20.29) 
28 
3-69 
29.36  (16.61) 
5-85 
32.57 (22.41) 
40 
0-19 
7.28 
(6.08) 33 
27-94 60.24 (15.53) 
4-102 58.09 (21.98) 
38 
7-45 
21.89 (11.10) 
4-48 
23.76 (11.67) 
lower RESE would have lower scores at pre-test than their  high RESE peers. However, this was not the case for either  the measure of math fluency or the measure of math  comprehension.  
Rather than focusing on a difference in performance at  pre-test, this study addresses how RESE might affect the  learning that happens as the result of child-guided math  practice. What is the potential for children with low RESE to  learn through a program or intervention that targets learning  gains? Interestingly, results showed that more practice did not  lead to an improvement in math competence if a child had  low RESE. However, more practice did lead to an  improvement in math competence if a child had high RESE.  These findings suggest that in order for personalized math  practice to be effective, children must enter the program with  high RESE.  
There are a few possible explanations for this finding.  One is that if a child with low RESE is presented with a  challenging problem and becomes frustrated or angry, he or  she is not prepared to manage those negative emotions  appropriately. Therefore, the child might be hindered from  learning and continuing through the remainder of the session.  Another explanation has been demonstrated empirically:  Emotional self-efficacy moderates the negative effects of  anxiety on math performance (Galla & Wood, 2012). In the  
Note. Children who were not assessed both pre- and post test for the relevant variables were excluded. 
*Scored as number of correctly answered problems.  **Scored as a percent of correctly answered problems.  
Discussion 
Results suggest that there is a relation between personalized  math practice and improvement in math competence.  Children differed in what topics they practiced, how many  questions they answered, and how difficult the problems  were. This could imply that exposure to math, regardless of  specific content, is important in improving competence. It could help explain why children from low-SES, who have a  lack of exposure to math during the summer, experience  summer learning loss (Burkham, Ready, Lee, & LoGerfo,  2004). Or, rather, it could mean that the personalized, child 
centered approach is what is important. There is some  evidence from other studies to suggest that the latter is the  case, such as with Math Facts in a Flash (Burns et al., 2010).  
Of particular interest is the difference in effect math  practice had on math competence for children of different  RESE. As a concept, general self-efficacy is derived from  social cognitive theory, which states that one’s beliefs are an  important element to achievement (Bandura, 1997). RESE  refers to how one perceives their own ability to regulate their  emotions, and the perceived ability to achieve a goal can be  thought of as a prerequisite for actualizing the goal (Caprara  
study by Galla & Wood, anxiety was only predictive of math  performance in children with low emotional self-efficacy.  Children with high emotional-self efficacy showed no effect  of anxiety on math performance, inferring that the perceived  ability to manage negative emotions protects children from  the negative effects of anxiety on math performance.  Therefore, while children in the current study were not tested  on any anxiety measures, it is possible that children with high  RESE were protected from any negative effect that might  exist of anxiety on learning. Conversely, it is likely that some  children with low-RESE also had low anxiety, ultimately  affecting their learning experience. 
Limitations 
Including the community as a partner in the design of the  research can be limiting. Community organizations have  concerns to carry out their programs that take priority over  the integrity of the research. For example, the differences in  demographics between sites are very large. The number of  children in Site A is far less than Site B. The racial makeup  of Site A is majority Caucasian, while Site B is majority  African-American. These details are part of the structure of  the summer camp, which is dependent on outside factors and  resources. Nonetheless, they are important in considering the  findings. Specifically, personalized math practice was only  available for children in Site B once per week. Therefore, in  
1456
correlations that include frequency of practice, children from  Site B only represent data points that are low in frequency.  There are many instances in which this project did not  follow a strict research protocol. Even though the protocol  was designed by the research team and the non-profit  organization, it was not always possible to follow it strictly.  Because it was carried out in the community, the protocol was  left flexible. In some instances, children were particularly  stubborn in refusing to practice predetermined math topics,  measures were not always appropriate for all children (i.e.,  children with disabilities that impaired writing), not enough  facilitators were available to work in predetermined  facilitator-to-child ratios, or the camp was scheduled for  activities that interfered with the regular sessions. In these  instances, accommodations were made so as to benefit the  child, the non-profit organization, and the research as best as  possible, without disregarding any one in particular.  
Conclusions 
Child-guided math practice was most beneficial when  children had high RESE. This finding posits valuable  questions for future research: Is there a way to improve  children’s emotional self-efficacy? Or rather, is there a way  to structure practice to prevent the child from choosing math  problems that can cause overwhelming emotions? It is  possible that restricting the child’s ability to guide their  practice will only exacerbate the problem. 
A lack of improvement in math performance must be  considered in the context of a summer program. Generally, it  is expected that children decline in math performance over  the course of the summer (Cooper et al., 1996). Therefore,  they should experience a portion of that decline between the  beginning and the end of a 7-week camp. On average,  children retained or improved their math performance more  often than they did not (see Table 2), which is an  accomplishment relative to the expected decline. 
Acknowledgments 
This material is based upon work supported by an NSF  Graduate Research Fellowship awarded to MC  (#1000221730) and by a University of Cincinnati grant  awarded to HK. Any opinions, findings, and conclusions or  recommendations expressed in this material are those of the  authors and do not necessarily reflect the views of the  funders. We would like to thank Upspring for providing us  with their program data. 
References 
Ajzen, I. (2002). Perceived behavioral control, self-efficacy, locus of control, and the theory of planned behavior.  Journal of applied social psychology, 32(4), 665-683. 
Bandura, A. (1997). Self-efficacy: The exercise of control.  New York: Freeman. 
Burkam, D. T., Ready, D. D., Lee, V. E., & LoGerfo, L. F.  (2004). Social-class differences in summer learning  between kindergarten and first grade: Model  
specification and estimation. Sociology of Education,  77(1), 1-31.  
Burns, M. K., Kanive, R., & DeGrande, M. (2012). Effect of  a computer-delivered math fact intervention as a  supplemental intervention for math in third and fourth  grades. Remedial and Special Education, 33(3), 184- 
191. 
Canfield, J.P., Cartwright, M.D., Kloos, H., Schmerr, M.,  Aigner, J.M. (2018). Initial validation of a regulatory  emotional self-efficacy scale for homeless children. Manuscript in preparation.  
Caprara, G. V., Di Giunta, L., Eisenberg, N., Gerbino, M.,  Pastorelli, C., & Tramontano, C. (2008). Assessing  regulatory emotional self-efficacy in three countries.  Psychological Assessment, 20(3), 227.  
Chew, B. H., Zain, A. M., & Hassan, F. (2013). Emotional  intelligence and academic performance in first and final  year medical students: A cross-sectional study. BMC  Medical Education, 13(1), 44.  
Galla, B. M., & Wood, J. J. (2012). Emotional self-efficacy  moderates anxiety-related impairments in math  performance in elementary school-age youth.  Personality and Individual Differences, 52(2), 118-122.  
Gross, T. J., & Duhon, G. (2013). Evaluation of computer assisted instruction for math accuracy intervention.  Journal of Applied School Psychology, 29(3), 246-261.  
Hilton, A. (2016). Engaging primary school students in  mathematics: Can iPads make a difference? International  Journal of Science and Mathematics Education, 1-21.  
Horak, V. M. (1981). A meta-analysis of research findings on  individualized instruction in mathematics. The Journal  of Educational Research, 74(4), 249-253.  
Kirk, B. A., Schutte, N. S., & Hine, D. W. (2008).  Development and preliminary validation of an emotional  self-efficacy scale. Personality and Individual  Differences, 45(5), 432-436.  
Klem, A. M., & Connell, J. P. (2004). Relationships matter:  Linking teacher support to student engagement and  achievement. Journal of school health, 74(7), 262-273. 
Kucian, K., Grond, U., Rotzer, S., Henzi, B., Schönmann, C.,  Plangger, F., et al. (2011). Mental number line training  in children with developmental dyscalculia.  Neuroimage, 57(3), 782-795.  
Lam, L. T., & Kirby, S. L. (2002). Is emotional intelligence  an advantage? An exploration of the impact of emotional  and general intelligence on individual performance. The  Journal of Social Psychology, 142(1), 133-143.  
Mayer, D., Salovey, P., & Caruso, R. (2004). Emotional  intelligence: Theory, findings, and implications.  Psychological Inquiry, 15(3), 197-215. 
Spillane, J. P., Reiser, B. J., & Reimer, T. (2002). Policy  implementation and cognition: Reframing and  refocusing implementation research. Review of  educational research, 72(3), 387-431. 
Wigfield, A., & Meece, J. L. (1988). Math anxiety in  elementary and secondary school students. Journal of  educational Psychology, 80(2), 210. 
1457
Not all Active Learning is Equal: Predicting and Explaining Improves Transfer  Relative to Answering Practice Questions 
Paulo F. Carvalho (pcarvalh@andrew.cmu.edu) 
Human-Computer Interaction Institute, Carnegie Mellon University, Pittsburgh, PA 15213 USA 
Kody J. Manke (kmanke@andrew.cmu.edu) 
Department of Psychology, Carnegie Mellon University, Pittsburgh, PA 15213 USA 
Kenneth R. Koedinger (koedinger@cmu.edu) 
Human-Computer Interaction Institute, Carnegie Mellon University, Pittsburgh, PA 15213 USA 
Abstract 
We compared students’ exam performance following one of  two different types of active learning assignments. In one  version students read text describing experimental evidence for  the principle being studied. In the other version, students  instead created a hypothesis and explanation, and then studied  and explained the results. The content was matched across  conditions. Students performed better in exams requiring  generalization to novel situations, after providing hypotheses  and explanations than after reading the text and answering  questions about it. These results suggest that prediction and  explanation cycles might be a better active learning approach  to promote generalization and transfer than practice questions. 
Keywords: predict-observe-explain; active learning; retrieval  practice 
Introduction 
Students learn better when they engage in active learning  (Freeman et al., 2014; Wieman, 2014). Yet, much instructional practice emphasizes passive learning such as  reading text, attending lectures, and watching videos.  Contrary to evidence of the clear benefits of active learning,  students (and a surprisingly high number of instructors) feel that passive strategies such as re-reading are useful study  methods (Morehead et al., 2016). 
This disconnect between evidence and practice highlights  the need to develop active learning practices that are  grounded in empirical evidence and can support effective  learning. One important step is understanding which types of  active learning practices support transfer of different types of  knowledge. For example, an active learning practice that  supports memorization processes might not be equally  effective for generalization (Koedinger et al., 2011). 
As an initial step towards developing better active learning  practices, in this paper we compare two different types of  active learning activities in a psychology course and evaluate  their impact on students’ learning outcomes. Specifically, we  compare active learning that involves responding to  questions after reading a text to active learning that involves  generating hypotheses and explanations. 
Previous research has shown that, even following an  introductory course in psychology, students often lack the  ability to provide clear scientific reasoning and identify  methodological issues (Lawson et al., 2015). These  difficulties include generating testable hypotheses and  
connecting scientific evidence to theory. One possible  explanation is that the learning practices used do not match the learning goals. Despite the expectation, across most  STEM courses, that students learn scientific principles  through the study of empirical evidence, most learning  practices are either passive (e.g., reading) or focus on active  learning that promotes memorization of evidence, instead of  extrapolation (e.g., retrieval practice or pre-questions). 
In this research, we compare the use of practice questions and another active learning practice: the predict-observe explain process. As we discuss in greater detail below, the  use of practice questions has been shown to successfully  improve learning and memory for studied facts, whereas the  predict-observe-explain process promotes predicting and  explaining evidence and therefore has the potential to foster scientific understanding and inductive processes. 
Practice questions approach 
Learning from reading is remarkably poor compared to active  learning. For example, completing more practice activities is  a better predictor of learning in online courses than  completing more readings (Carvalho et al., 2017).  Furthermore, practices that reduce the amount of reading 
often improve learning; across multiple studies, learners  remembered more facts when they read only the summaries  instead of the textbook prose, even when the critical  information was highlighted (Reder & Anderson, 1980a). 
Several active learning strategies have been suggested to  improve learning from passive information. These often  include questions either before the text (pre-questions; e.g.,  Rickards, 1976a), along with reading the text (Rickards,  1976b), or after reading the text instead of re-reading  (Roediger & Karpicke, 2006). The inclusion of practice  questions, in any of these ways, has been shown to improve  learning compared to having no questions (Pressley,  Tanenbaum, McDaniel, & Wood, 1990; Richland, Kornell, &  Kao, 2009; Rickards, 1976b, 1976a), re-reading the materials  instead of answering the questions (Butler & Roediger, 2007;  Richland et al., 2009), or reading the questions along with the  answers (Carpenter et al., 2017). 
Several (not mutually exclusive) mechanisms have been  suggested for why practice questions improve learning.  Practice questions orient the learner towards the critical  aspects of the text or video (Reder, 1980b), they require the  learner to retrieve previous information (Roediger &  
1458
Karpicke, 2006), and they act as metacognitive checks on the  learners’ knowledge (Bjork et al., 2013). Although the exact  mechanisms of why practice questions improve learning are  still debated, the active learning nature of practice questions  compared to only reading is undeniable. 
Importantly, most of the research on the benefits of practice  questions has focused on retrieval of information. That is, the  amount of information learners can successfully recall from  the text or video provided. Thus, it is possible that, despite its  high effectiveness in improving learners encoding and recall  of information (compared to passive learning), practice  questions might not elicit induction and refinement processes  (Koedinger et al., 2011). When the goal of learning is to  generalize or induce scientific principles from specific  empirical examples, practice questions may be insufficient. 
The Predict-observe-explain approach 
Another active learning approach is the use of the predict observe-explain process to guide learners through learning  materials, often regarding empirical results (White &  Frederiksen, 1998). In the predict-observe-explain (POE) process, learners are introduced to a scientific question (e.g.,  Are we influenced by group pressure?) and an experimental  setup (e.g., Two groups are answering questions about line  lengths. In one group participants answer questions  individually, in the other group participants answer  questions in a group in which 4 people previously provided  the ostensive wrong answer). Learners are then asked to make  a prediction about the study results (Predict step). After  learners make their predictions, they are presented with the  experimental results (Observe step), usually in the form of a  graph or table, and then asked to explain why the  experimental manipulation yielded such results (Explain 
step). 
When compared to business-as-usual classroom practices  (passive learning), POE has been shown to improve learning  (Karamustafaoglu & Mamlok-Naaman, 2015; Kibirige,  Osodo, & Tlala, 2014; White & Frederiksen, 1998). The  exact mechanisms through which POE improves learning are  not well understood. It may involve the ability to directly  address a priori misconceptions students might have with  evidence that directly contradicts it (Kowalski & Taylor,  2009). It may also involve creating explanations, which  might improve learning as repeatedly shown with the self explanation effect (VanLehn, Jones, & Chi, 1992). In  particular, creating explanations has been shown to promote  generalization and transfer (Lombrozo, 2006), albeit hurting  memory (Williams & Lombrozo, 2010). 
Regardless of the exact mechanism, POE has the  distinctive feature of providing active learning, similar to  practice questions. However, POE emphasizes generalization  and extrapolation from data, whereas practice questions  emphasize rote memorization of the information provided in  the text. Thus, it is possible, even likely, that POE leads to  better learning than practice questions when we test students’  ability to generalize evidence covered in class. 
The present study 
The main goal of this initial study is to compare the learning  benefits of two active learning approaches – practice  questions and predict-observe-explain – for generalization of  empirical evidence to new situations. Unlike previous  research, we compare a POE approach not with business-as 
usual passive learning practices, but with another active  learning practice shown to also improve learning. We used an in-vivo procedure in which we embedded an  experimental manipulation in the instructional activities of an  introductory undergraduate Social Psychology course. To  increase power and decrease potential issues related to individual differences, we used a within-subjects design.  There were two conditions: a practice questions condition  and a predict-explain-observe-explain (PEOE) condition. In  the practice questions condition, students read the description  of three social psychology studies for two different social  psychology topics and were asked to answer questions about  those materials. In the PEOE condition, students studied the  same empirical studies but were only provided the research  question and experimental design before being asked to  predict the outcome, explain their prediction, observe a plot  of the results, and explain the results observed. We included  the additional explanation step compared to the typical POE  to maximize the potential benefit of self-explanations  associated with POE. 
Students completed a series of open-ended and multiple choice questions as part of their exam. None of the questions  in the exam probed memory for the specific studies practiced  or asked about specifics of those studies. Instead, questions  asked students to apply general knowledge about the  principles to novel situations or relate it to other principles covered in class. We measured students’ performance on  exam questions about the topics covered in the activity,  general exam performance, time spent completing the  activities, and performance on the activities themselves. 
We predicted that active learning that involves PEOE will  result in improved generalizable learning and transfer to other  topics. 
Method 
Participants 
One hundred nineteen undergraduate students volunteered to  participate in the study as part of their Social Psychology  course taught by the second author at Carnegie Mellon  University. All students completed all conditions (see below 
for details), order counterbalanced across participants. Forty four students did not complete at least one assignment before  each exam and their data were excluded from analyses. The  final sample includes data from 75 students. 
Materials 
Study materials were created using the Open Learning  Initiative (OLI) authoring tools and distributed to students as  assignments using the course’s Canvas website.  
1459
Table 1: Topics of the six assignments and schedule. Assignment Topic Course week 1 Obedience 1 
2 Cognitive Dissonance 2 
Exam 1 5 
3 Conformity 8 
4 Social Facilitation 9 
Exam 2 10 
5 Stereotype Threat 11 6 Misattribution of Arousal 14 Exam 3 15 
Final Exam Finals week 
For assignments 3-6 (see Table 1), we created two  versions: Practice Questions and PEOE. The first two  assignments did not have a PEOE version and were used as a  baseline condition. 
Figure 1: Example of one of the assignments in the Practice    Questions condition. 
Each assignment covered three empirical studies in Social  Psychology about a topic covered in that unit. The studies  were chosen for their relevance and representativeness for the  topic but were not covered by the instructor in class (though  the topic was). Table 1 includes all topics covered. 
The Practice Questions version of the materials included a short description of the study procedure and background, and  description of the hypothesis, predictions, results (including  a plot), and conclusions of the study (see Figure 1). After the  text, students were asked to answer four open-ended  questions about the text they just read: (1) What were the  researchers’ predictions?, (2) Why did the researchers make  those predictions?, (3) What did the results show?, and (4)  Why did participants respond that way?. Answers to each of  these questions were clearly and succinctly stated in the text  students had just read. No feedback was provided for any of  the questions. Although questions were presented after the  text, because they were in the same page, students were free  
to use the questions as pre-questions or along with the text as  they wished. 
The PEOE version of the materials started with a brief  description of the study procedure and background (same  paragraph as in the Practice Questions version). After that  paragraph a series of questions instantiating the predict 
explain-observe-explain procedure were presented, one at  time: (1) What do you expect will happen?, (2) Why do you  think [hypothesis selected]?, (3) The graph above shows the  results of this study. What do the results show?, and (4) Why  do you think the results show [results]? (see Figure 2 for an  example)   
Figure 2: Example of one of the assignments in the PEOE  condition. 
Questions (1) and (3) were multiple-choice questions in  which the same three possible hypotheses were presented, but  the graph with the experimental results was presented only  along with question (3) and not before students answered (1)  and (2). Questions (2) and (4) were open-ended questions.  Feedback was provided only to questions (3) and (4).  Feedback for question (3) indicated the correct description of  the results and feedback for question (4) indicated the  conclusions extracted by the experimenters in the target study and matched the corresponding paragraph in the  Practice Questions version of the materials. 
The two versions of the materials were closely matched for  active engagement/retrieval (in all versions students had to  answer questions) and content (the questions and text were  matched across versions and the same text was used across  conditions where needed). 
Students completed three non-cumulative exams during  the semester and a cumulative final exam. For each exam, we  created five questions targeting the topic covered in the  activities. Two were open-ended questions asking students to  apply the principles to novel situations, two were multiple 
choice questions targeting students’ understanding of the  principle, and another was a multiple-choice question about  general understanding of research methods. None of the  exams included questions about the specific studies covered  
1460
in the activities but included other questions relating activity topics with other class topics. The open-ended questions were  scored by a team of course TAs, blind to condition  assignment 
Design and Procedure 
The study took place over a semester and the study activities  were assigned as homework. Students completed two assignments before each exam. The first two assignments had  only a Practice Questions version and were used as a baseline  condition for all students. A crossover design was used such  that students were randomly assigned to one of two groups.  The first group completed the Practice Questions version of  assignments 3 and 4 (before Exam 2) and the PEOE version  of assignments 5 and 6 (before Exam 3). The other group  completed the inverse versions – the PEOE version of  assignments 3 and 4, and the Practice Questions version of  assignments 5 and 6. All assignments were due the week the  topic was covered in class, and students received course  points if they completed the assignment, regardless of  performance on the assignment. All students were required to  complete all course exams. 
Results 
We normalized all measures by calculating z-scores of all  predictor and outcome measures. This allowed us to compare  estimates across analyses as normalized effect sizes and  addressed possible scaling issues related to using very  different variables in regression models. 
Time spent completing the activities 
Students spent on average more time completing the Practice  Questions version (M = 53 minutes, SD = 54 minutes) than  the PEOE version of the assignments (M = 31 minutes, SD =  35 minutes). This difference was statistically significant  when controlling for average time spent on baseline  assignments (M = 55, SD = 53) and counterbalancing  condition, β = -0.48, t (74) = -4.14, p < .00001. 
We also analyzed the logged data to ascertain that students  were completing the practice questions in the corresponding version of the assignments, and whether they were doing so  before or after reading the text. Only one student did not  complete all questions for all assignments (one question was  left blank). The average number of words written in the  answer to the questions was 26 (SD = 7), and initial  inspection of the responses suggests that students actively  tried to respond and not only directly copy from the text or  provide random strings. Finally, on average, students spent  less than a minute on the page before clicking on one of the  question activities, suggesting that students were using the  questions as guides to read the text (M = 4.81 seconds, SD =  5.01 seconds), and not the other way around. 
Exam grades 
Next, we looked at students’ exam performance across  conditions, controlling for time spent on the assignments,  counterbalancing condition, performance on the baseline  
exam, and time spent on the baseline assignments. Students  performed better on the exams covering topics practiced  using the PEOE version of the assignments than in those with  topics practiced using the Practice Questions version of the  assignments (see Figure 3), β = 0.13, t (82.18) = 2.75, p 
=.007. The effect is even larger when we do not control for  time spent in the activities, β = 0.31, t (74) = 2.65, p = .010.   
Figure 3. Students’ performance on the exams following  Practice Questions and PEOE activities. 
We also looked at the effect of the version of the  assignments (PEOE vs. Practice Questions) on the questions  we included in the exam specifically only about the studies  covered in the assignments. Students showed only slightly  fewer errors on exam questions specifically about the topics 
covered in the PEOE version of the assignments (M = 0.07,  SD = 0.10), compared to the Practice Questions version of the  assignments (M = 0.08, SD = 0.10). This difference was not  statistically significant when controlling for time spent in the  activities, counterbalancing condition, performance on the  baseline exam, and time spent on the baseline assignments, β = 0.01, t (74) = 0.07, p = .945. One potential reason for this  result is the compressed scale. We included only four questions referring only to the specific topics covered in the  activities and, overall, the range was smaller (between 0%- 
45% error rate), compared to the range of results for the entire  sample (between 0%-58% error rate). Importantly,  performance on the assignment-specific questions was a  strong predictor of performance on the exam, β = 0.50, t (71)  = 8.07, p < .00001. This suggests that the overall exam  differences may be related to potential spillover from  stronger understanding of a considerable portion of the  materials covered on the exam, even if the remaining  questions did not uniquely target assignment topics. 
Quality of students’ predictions 
Beyond time spent on the assignments (included in all  analyses above), does accuracy on the assignment itself  predict exam performance? Specifically, do students who  make correct predictions in the first step of the PEOE version  benefit more from that assignment activity compared to those  who make the wrong prediction? To answer this question, we  compared performance on the exam for students in the PEOE condition as a function of their accuracy in the predict step.  
1461
On average, students made the correct prediction on the first  try 54% of the time (SD = 18), and 53% of the time when  considering all tries (SD = 18; mean number of tries = 1.32). Importantly, students who made correct predictions on the  first try performed better in the exam compared to those who  did not, β = 0.21, t (71) = 2.17, p = .033.  
For comparison, and to determine if this effect is unique to  the predict step or reflects general ability or compliance differences among students, we compared student exam  performance depending on their accuracy responding to the  question about the plot – the observe step. The answer to this  question was presented in the plot if students successfully  read it. Students provided the correct response to this  question on the first try 81% of the times (SD = 15), and 82%  of the times when considering all attempts (SD = 14, mean  number of tries = 1.28). We found no difference on exam  performance depending on the students’ accuracy describing  the graph, β = 0.140, t (71) = 1.44, p = .153. Finally, even 
when controlling for accuracy on the predict step, overall  students performed better in the exam following PEOE  activities compared to Practice Questions, β = 0.34, t (82) =  2.78, p = .007. 
Discussion 
The results of this study suggest that active learning activities  using a predict-explain-observe-explain approach take less  time to complete than reading along with practice questions  while simultaneously yielding better learning, as measured  by exam performance. 
The predict-observe-explain approach had been used  before with positive learning outcomes. However, previous  studies focused on comparing the POE approach to business as-usual classroom activities. The present study is, to the best  of our knowledge, the first study that demonstrates the  benefits of PEOE when compared to another active learning  activity recognized to improve learning. 
Reading text materials and answering practice questions is  an intensive, time-consuming activity and students in the  current study seem to use the practice questions as reading  guides, which has been shown to improve learning (Carpenter  et al., 2017). The effortful use of practice questions could  make this type of activity a “desirable difficulty” (Bjork,  1994) – a group of practices that, despite involving greater  effort and worse immediate outcomes, improve learning in  the long run. However, the current study shows that, despite  high compliance and student engagement in the practice  question assignments, students performed better on topics  practiced using the PEOE approach. 
Previous research has shown that practice questions are  effective at improving learning compared to many other  approaches (Roediger & Karpicke, 2006). Critically, the  benefit of PEOE compared to practice questions seen here is  likely to be tied to the different types of assessments and  corresponding types of learning (Koedinger et al., 2011).  Previous work assessing the benefit of practice questions has  focused mostly on verbatim memory for the materials studied  or conceptual extrapolations that had been probed in the  
practice questions (Butler, 2010; Roediger & Karpicke,  2006). In the present work, however, the exam asked students  to generalize psychological principles from the materials  studied and connect them to other concepts. Under these  circumstances, learners who created and explained hypotheses performed better on the exam than those who read  the same information and answered questions about it.  Although we did not include on the exam questions testing 
verbatim memory for the activities in this first study, future  work will further test this hypothesis by directly comparing  performance on verbatim and generalization questions. This  hypothesis is also consistent with previous evidence showing  that learners who provided explanations in a categorization  task showed better category generalization but worse  memory for specific items studied compared to learners  asked to describe the items (Williams & Lombrozo, 2010). 
Any of the differences between PEOE and practice  questions could account for the results presented here,  including the reduced text, asking for explanations, or step by-step presentation. Although the main goal of this study  was not to identify the specific mechanisms that make POE  (and its PEOE counterpart used here) good for learning, there  are several suggestions from the current literature. Previous  work has suggested that addressing previous misconceptions  by providing hypotheses before seeing the results might  improve learning of scientific phenomena. Our results do not  seem to support this hypothesis. Students who provided the  correct hypothesis on the first try performed better in the  exam than those who did not.  
An important aspect of PEOE is the several steps of  explanation required. Providing explanations – even if  incorrect – might improve learning and generalization by  promoting integration of new information with previous  knowledge (Chi, 2000), and by guiding learners’ attention  towards structural features that are relevant for generalization  (Lombrozo, 2006; Rittle-Johnson, 2006). Thus, it is possible  that, compared to practice questions, PEOE led learners to  connect the information in the assignments with other  materials from the textbook or class and emphasized critical  similarities and differences among several examples, within  the assignments and across sources. 
Importantly, the present study used an in-vivo approach to  experimentation. That is, we implemented the manipulation  as part of regular classroom activities using relevant  materials. This approach increases the external validity of the  results and their direct relevance for both theory and practice,  while maintaining precise control over the manipulation. 
In sum, the present results suggest that asking students to  predict and explain information yields better generalization  and transfer than other practices known to improve learning  over passive learning. Future research is necessary to test the  generalizability of the present results (e.g., does PEOE work  with other materials beyond scientific results?) and to probe  the exact mechanisms of action. The present results also  reiterate the importance of considering the whole learning  context when identifying the best learning approach  (Carvalho & Goldstone, 2015; Koedinger et al., 2011). 
1462
Acknowledgements 
All data is available through LearnSphere Datashop  (pslcdatashop.web.cmu.edu), dataset #2342. We would like to thank  the Eberly Center and the Open Learning Initiative for their help  creating the study materials and implementing the study. This work  was supported by a ProSEED grant from CMU Simon Initiative. 
References 
Bjork, R. A. (1994). Memory and metamemory  considerations in the training of human beings. In J.  Metcalfe & A. P. Shimamura (Eds.), Metacognition:  Knowing about knowing (pp. 185–205). Cambridge, MA:  MIT Press. 
Bjork, R. A., Dunlosky, J., & Kornell, N. (2013). Self Regulated Learning: Beliefs, Techniques, and Illusions.  Annual Review of Psychology, 64(1), 417–444. 
Butler, A. C. (2010). Repeated testing produces superior  transfer of learning relative to repeated studying. Journal  of Experimental Psychology: Learning, Memory, and  Cognition, 36(5), 1118–1133. 
Butler, A. C., & Roediger, H. L. (2007). Testing improves  long-term retention in a simulated classroom setting.  European Journal of Cognitive Psychology, 19(4–5), 514– 527. 
Carpenter, S. K., Rahman, S., Lund, T. J. S., Armstrong, P.  I., Lamm, M. H., Reason, R. D., & Coffman, C. R. (2017).  Students’ Use of Optional Online Reviews and Its  Relationship to Summative Assessment Outcomes in  Introductory Biology. Cell Biology Education, 16(2), ar23. 
Carvalho, P. F., & Goldstone, R. L. (2015). What you learn  is more than what you see: What can sequencing effects tell  us about inductive category learning? Frontiers in  Psychology, 6. 
Carvalho, P. F., McLaughlin, E. A., & Koedinger, K. R.  (2017). Is there an explicit learning bias? Students beliefs,  behaviors and learning outcomes. In G. Gunzelmann, A.  Howes, T. Tenbrink, & E. Davelaar (Eds.), Proceedings of  the 39th Annual Conference of the Cognitive Science  Society (pp. 204–209). 
Chi, M. T. H. (2000). Self-explaining: The dual processes of  generating inference and repairing mental models. In R.  Glaser (Ed.), Advances in instructional psychology:  Educational design and cognitive science. Vol. 5 (pp. 161- 
238). Mahwah, NJ: Lawrence Erlbaum Associates. Freeman, S., Eddy, S. L., McDonough, M., Smith, M. K.,  Okoroafor, N., Jordt, H., & Wenderoth, M. P. (2014).  Active learning increases student performance in science,  engineering, and mathematics. PNAS, 111(23),8410–8415. Karamustafaoglu, S., & Mamlok-Naaman, W. (2015).  Understanding Electrochemistry Concepts using the  Predict-Observe-Explain Strategy. Eurasia Journal of  Mathematics, Science & Technology Education, 11(5), Kibirige, I., Osodo, J., & Tlala, K. M. (2014). The Effect of  Predict-Observe-Explain Strategy on Learners’  Misconceptions about Dissolved Salts. Mediterranean  Journal of Social Sciences. 
Koedinger, K. R., Corbett, A. T., & Perfetti, C. (2011). The  Knowledge-Learning-Instruction (KLI) Framework:  Bridging the Science-Practice Chasm to Enhance Robust  Student Learning. Cognitive Science, 36(5), 757–798. 
Kowalski, P., & Taylor, A. K. (2009). The Effect of Refuting  Misconceptions in the Introductory Psychology Class.  Teaching of Psychology, 36(3), 153–159. 
Lawson, T. J., Jordan-Fleming, M. K., & Bodle, J. H. (2015).  Measuring Psychological Critical Thinking: An Update.  Teaching of Psychology, 42(3), 248–253. 
Lombrozo, T. (2006). The structure and function of  explanations. Trends in Cognitive Sciences, 10(10), 464– 470. 
Morehead, K., Rhodes, M. G., & DeLozier, S. (2016).  Instructor and student knowledge of study strategies.  Memory, 24(2), 257–271. 
Pressley, M., Tanenbaum, R., McDaniel, M. A., & Wood, E.  (1990). What happens when university students try to  answer prequestions that accompany textbook material?  Contemporary Educational Psychology, 15(1), 27–35. 
Reder, L. M. (1980a). A comparison of text and their  summaries: memory consequences. Journal of Verbal  Learning and Verbal Behavior, 134, 121–134. 
Reder, L. M. (1980b). The role of elaboration in the  comprehension and retention of prose: A critical review.  Review of Educational Research, 50(1), 5–53.  
Richland, L. E., Kornell, N., & Kao, L. S. (2009). The  pretesting effect: Do unsuccessful retrieval attempts  enhance learning? Journal of Experimental Psychology:  Applied, 15(3), 243–257. 
Rickards, J. P. (1976a). Interaction of position and conceptual  level of adjunct questions on immediate and delayed  retention of text. Journal of Educational Psychology,  68(2), 210. 
Rickards, J. P. (1976b). Type of verbatim question  interspersed in text: A new look at the position effect.  Journal of Reading Behavior, 8(1), 37–45. 
Rittle-Johnson, B. (2006). Promoting transfer: Effects of self explanation and direct instruction. Child Development,  77(1), 1–15. 
Roediger, H. L., & Karpicke, J. D. (2006). The Power of  Testing Memory: Basic Research and Implications for  Educational Practice. Perspectives on Psychological  Science, 1(3), 181–210. 
VanLehn, K., Jones, R. M., & Chi, M. T. (1992). A model of  the self-explanation effect. The Journal of the Learning  Sciences, 2(1), 1–59. 
White, B. Y., & Frederiksen, J. R. (1998). Inquiry, Modeling,  and Metacognition: Making Science Accessible to All  Students. Cognition and Instruction, 16(1), 3–118.  
Wieman, C. E. (2014). Large-scale comparison of science  teaching methods sends clear message. PNAS 111(23),  8319–8320. 
Williams, J. J., & Lombrozo, T. (2010). The Role of  Explanation in Discovery and Generalization: Evidence  From Category Learning. Cognitive Science, 34(5), 776– 806. 
1463
The Cognitive Processes Underlying Moral Judgment Across Development  
Lisa Chalik (lisa.chalik@yale.edu)  
Department of Psychology, 2 Hillhouse Avenue  
New Haven, CT 06511 USA  
Jay J. Van Bavel (jay.vanbavel@nyu.edu)  
Department of Psychology, 6 Washington Place  
New York, NY 10003 USA  
Marjorie Rhodes (marjorie.rhodes@nyu.edu)  
Department of Psychology, 6 Washington Place  
New York, NY 10003 USA  
Abstract  
Some moral philosophers have suggested that a basic  prohibition against intentional harm ought to be at the core of  moral belief systems across human societies. Yet,  experimental work suggests that not all harm is viewed  equally—people often respond more negatively to harm that  occurs among fellow social group members, rather than  between members of different groups. The present two studies  investigated how concerns about social group membership  factor into the moral judgment system. Adults (N = 111, Study  1) and children (N = 110, Study 2) evaluated instances of  inter- and intra-group harm under varying levels of cognitive  load. Both children and adults responded more slowly to  intergroup harm than to intragroup harm. Furthermore, adults  under cognitive load rated intergroup harm more leniently  than intragroup harm, but adults who were not under load  rated the two types of behaviors similarly. These findings  suggest that across development, evaluations of intergroup  harm rely more heavily on conscious deliberation than  evaluations of intragroup harm. Thus, people's evaluations of  harmful behaviors are made in light of information about the  social category membership of the people involved.  
Keywords: moral judgment; social cognitive development;  intergroup cognition  
Introduction  
In the Fall of 2009, an army major on a United States  military base in Fort Hood, Texas opened fire on his fellow  American soldiers, killing 13 people and injuring more than  30 others. As with other mass shootings, this tragedy evoked  outrage around the country. Yet, this particular event was  quite different from other mass shootings. It was not a  random attack upon people mostly unknown to the attacker —it was an attack by an American soldier upon fellow  members of the American military. In a statement at a  memorial service for the victims, President Obama stated  that the fact that the attack was committed by an American,  on American soil, “makes the tragedy even more painful,  even more incomprehensible” (The White House, 2009).  
This response illustrates a common phenomenon: People  often treat harm against members of the same social group  (e.g., among fellow Americans) as more morally  reprehensible than harm between members of different groups. Yet, the question of how concerns for social groups  fit into the moral judgment system is a challenging one.  
Many moral philosophers and psychologists have argued  that the prohibition against intentional harm is the most  fundamental component of human morality, regardless of  other features of a situation, such as group membership  (Nagel, 1987; Smetana, 2006). By this account, group  membership should not factor into people’s initial  evaluation of a harmful act—people should simply respond  negatively to any sort of harm. Indeed, a general prohibition  against harming others exists across human cultures (Haidt  & Joseph, 2004; Helwig, 2006; Wainryb, 2006).  
Yet, experimental work has revealed that not all harm is  viewed equally. People often evaluate harm more or less  severely depending on features of the situation aside from  the harmful behavior itself, such as group membership  (Meier & Hinsz, 2004; Rai & Fiske, 2011; Rozin, Lowery,  Imada, & Haidt, 1999; Shweder, Much, Mahapatra, & Park,  1997). For example, across many human societies, people  actually value violence toward outgroup members,  especially if they identify strongly with their ingroup  (Cohen, Montoya, & Insko, 2006), and viewing harm  against outgroup members is associated with the activation  of brain regions that encode reward (Cikara, Botvinick, &  Fiske, 2011). Thus, it remains an open question exactly how  the moral judgment system weighs concerns for group  membership against more general prohibitions against harm.  
Developmental research provides one possible answer to  the question of how group membership factors into the  moral judgment system. Young children have a basic  expectation that social groups mark people who hold  intrinsic, moral obligations toward one another. For  instance, one study introduced 3- to 9-year-old children to  two novel social categories and showed them instances of  intra- or inter-group harm (e.g., someone teasing a member  of their own or another group; Rhodes & Chalik, 2013).  Children were asked to give an initial evaluation of the  harmful action. Then, children were told that there were no  explicit rules prohibiting the harmful behavior in the actors’  context (e.g., there were no rules prohibiting teasing), and  were asked to evaluate the action a second time. When the  actions involved members of the same group, children rated  the action as equivalently bad regardless of the rule,  suggesting that they thought the agent was intrinsically  obligated not to harm a member of the agent’s own group,  regardless of the circumstance. In contrast, when the actions  
1464
involved members of different groups, children evaluated  the action more leniently after they were told there were no  specific rules in place prohibiting the behavior. This pattern  of findings suggests that children have a basic intuition that  people should not harm their own group members, but  require more information (e.g., consideration of local rules)  to determine when intergroup harm is prohibited or  permissible. No prior research, however, has investigated  the role of cognitive resources in these judgments.  
Based on these findings, as well as prior work to suggest  that moral judgments vary in the degree to which they rely  on deliberative reasoning (Greene, Sommerville, Nystrom,  Darley, & Cohen, 2001), the present work tests the  hypothesis that deliberative reasoning plays a more critical  role in the evaluation of intergroup harm, whereas the  evaluation of intragroup harm is more reflexive. In other  words, a prohibition against intragroup harm (but not  intergroup harm) is present in the expectations that children  hold very early in life. Because intergroup harmful  behaviors do not violate this prohibition, evaluating these  behaviors as wrong requires more conscious deliberation  (e.g., consideration of local rules and other contextual  features). On the other hand, behaviors that do violate the  basic prohibition against intragroup harm (i.e., harmful  actions among members of the same group) should be  reflexively judged as wrong, and require less deliberation.  To test these hypotheses, we recorded adults’ and children’s  response times to and ratings of negative intra- and inter group behaviors under varying levels of cognitive load.  
We made two predictions: First, if reactions against  intergroup harm require more deliberation than those against  intragroup harm, then people should be slower to evaluate  intergroup harm than they are to evaluate intragroup harm.  Second, since deliberative responses require more cognitive  effort than responses that rely more heavily on intuition  (Chaiken & Trope, 1999; Kahneman, 2003; Stanovich &  West, 2000), cognitive load should interfere more severely  with reasoning processes that require more deliberation.  Thus, our second prediction was that people would evaluate  intergroup harm, but not intragroup harm, more leniently  under cognitive load. 
Study 1  
Participants  
We recruited 111 adults (M age = 20.4, range = 19.0 - 24.0,  82 female) from New York University in exchange for  course credit. Data were collected in the 2015-2016  academic year. An additional 5 participants were tested but  excluded from analysis because they failed to correctly  answer attention check questions. Participants were  randomly assigned to the No Load (n = 55) or Load (n = 56)  conditions.  
Procedure  
Participants sat in front of a computer. On the screen, they  saw a seven-point scale ranging from -3 to 3, represented  visually by smiley faces (-3 = big frown, 0 = neutral face, 3  = big smile). Participants were told that they would see a  series of social interactions, and for each, they had to choose  
the corresponding point on the scale. Participants completed  two blocks of four practice trials each to familiarize them  with this procedure. For each trial, a social interaction  appeared (as a hand-drawn picture) and was described in an  audio recording, and the participant had to press a key  keyboard corresponding to one of the points on the scale  (denoted by images on the keys).  
For participants in the No Load condition, the second  block of practice trials was a repeat of the first block. For  participants in the Load condition, the second block of  practice trials included a cognitive load manipulation: In  addition to rating each interaction, participants performed a  trial-by-trial digit span memory task (Longstaffe, Hood, &  Gilchrist, 2014). In each trial, a 5-digit number appeared on  the screen for three seconds, followed by the social  interaction. After rating the interaction, participants were  prompted to enter the string of digits that they had seen  before moving to the next trial. We used this cognitive load  manipulation because we hypothesized that when people  deliberate over moral evaluations, their deliberation centers  around retrieving the relevant social rule (or rules) from  their semantic memory and deciding whether it applies in  the present scenario; for example, when thinking about  stealing, an individual might first think about whether there  exist social rules against stealing, then think about whether  the present behavior violates those rules. Thus, any  manipulation intended to interfere with this process would  need to interfere directly with the retrieval of semantic  knowledge. Because the working memory system acts to  control attention and allow for the retrieval of information  (Engle, 2002), we predicted that a digit span memory task,  which taxes the working memory system, would interfere  with participants’ evaluations of intergroup harm.  
After the practice trials, participants read a short story on  the screen in which they were introduced to two novel  groups of children, marked by shirt color and team names  (the “Flurps,” in blue shirts, and the “Zazzes,” in red shirts)  and described as engaged in noncompetitive activities— each group was working together to build a tower out of  blocks. Participants were not members of either group—this  allows us to test people’s abstract beliefs about intergroup  behavior, rather than their own affective ingroup biases.  Participants completed two attention check questions  (“Look at these two children. Are they in the same group or  different groups?”) to ensure that they recognized the two  groups. Next, participants completed 12 test trials in which  they saw and heard about a social interaction that had  occurred among the characters in the story, then evaluated  the interaction by choosing a point on the scale. Six trials  were about interactions that had occurred among members  of the same group (e.g., between a Flurp and another Flurp),  and six trials were about interactions that had occurred  between members of different groups (e.g., between a Flurp  and a Zazz). The intragroup and intergroup interactions  were presented in blocked counterbalanced order.  Additionally, for each group context, half of the social  interactions were about harmful behaviors (intended to test  our hypotheses about the cognitive processes underlying  judgments about inter- and intra-group harm) and half of the  social interactions were about prosocial behaviors (intended  
1465
as control items to ensure that participants used the full range of the scale). In the Load condition, the test trials  included the digit span memory task described above.  
We recorded participants’ ratings of the behaviors as well  as their reaction times for each rating. Reaction times were  measured from the moment the interaction appeared on the  screen until the moment participants chose a point on the  rating scale. Ratings are presented as participants’ average  rating of how bad the harmful behaviors were, with higher  numbers indicating a more negative rating. Reaction times  were log-transformed for analysis, but for ease of  interpretation are presented in milliseconds. Trials in which  participants took fewer than 500 ms or more than 10,000 ms  to respond were excluded from analysis, as participants  were unlikely to be answering meaningfully on these trials.  All raw data and code are available on the Open Science  Framework at http://osf.io/xr2wh/.  
Results  
To test our first hypothesis, that people would take longer to  evaluate intergroup harm than they would to evaluate  intragroup harm, we averaged participants’ reaction times  for the three harmful social interactions in each group  context (intergroup and intragroup). We subjected these  times to a repeated measures ANOVA with cognitive load  (Load or No Load) as a between-subjects factor and group  context (intergroup or intragroup) as a within-subjects  factor. We found a main effect of group context, such that  participants responded more slowly for intergroup harm (M = 3900, CI = 3700 - 4099) than they did for intragroup harm  (M = 3297, CI = 3150 - 3445), F(1,109) = 30.20, p < .001.  There was no main or interactive effect of cognitive load.  
To test our second hypothesis, that cognitive load would  selectively interfere with participants’ ratings of intergroup,  but not intragroup, harm, we repeated the above analysis on  participants’ ratings, rather than their reaction times. We  found a main effect of group context, indicating that  participants rated intragroup harm as worse than intergroup  harm, F(1,109) = 4.46, p < .05. We also found a main effect  of cognitive load, indicating that participants in the Load  condition rated behaviors as worse than participants in the  No load condition, F(1,109) = 5.31, p < .05.  
Contrary to our prediction, the interaction between group  context and cognitive load was not statistically significant  (F(1,109) = 1.29, p = .26), but an examination of the means  suggests that the effect of context was driven by participants  in the Load condition (see Figure 1). In the Load condition,  participants reliably evaluated intragroup harm (M = 2.67,  CI = 2.50 - 2.83) as worse than intergroup harm (M = 2.44,  CI = 2.24 - 2.63), t(55) = 2.51, p < .05, whereas in the No  Load condition, participants evaluated the two types of harm  similarly (intragroup: M = 2.34, CI = 2.17 - 2.51;  intergroup: M = 2.27, CI = 2.08 - 2.46), p = .53.  
To investigate these effects further, as a set of post-hoc  analyses, we tested whether the above effects differed based  on the extent to which participants’ cognitive resources were  taxed in the Load condition: Participants in this condition  correctly reported the number that they had been told to  remember on an average of 4.6 trials (out of the six trials  that involved harmful behaviors). Because this task was  
  
Figure 1: Participants’ ratings of intra- and inter group harm in each condition. Error bars represent  95% confidence intervals. 
designed to interfere with the relevant cognitive processing  for these types of moral evaluations, participants who were  more successful on the task (i.e., who remembered more  numbers correctly) should have had less working memory  capacity available to allow them to deliberate over their  evaluations. We thus tested the correlation between  performance on the memory task and the difference between  participants’ ratings of inter- and intra-group harm. The  difference between participants’ ratings was positively  correlated with memory performance (r = .26, p < .05), such  that participants who remembered more numbers correctly  showed a greater difference between their ratings of intra and inter-group harm (rating intragroup harm as worse than  intergroup harm). This finding supports the conclusion that  our cognitive load manipulation was successful in  interfering with deliberative processing, but also suggests  that a stronger load manipulation (perhaps one that  distinguishes between visual and verbal working memory;  Amit & Greene, 2012) might produce an even stronger  pattern of results.  
Discussion  
Across both of the measures that we tested (ratings and  reaction times), participants responded differently for intra and inter-group harm. In both conditions, participants  responded more slowly for intergroup harm than for  intragroup harm, suggesting that evaluations of intergroup  harm require more conscious deliberation than evaluations  of intragroup harm; this difference was quite large. Also,  participants rated intragroup harm as worse than intergroup  harm. Furthermore, cognitive load interfered modestly with  evaluations of intergroup harm—participants under load  rated intergroup harm as less bad than intragroup harm,  whereas participants under no load evaluated the two types  of harm similarly (although the interaction effect was not  statistically significant). A post hoc analysis revealed that  the difference between ratings of intergroup and intragroup  harm among participants under cognitive load was more  pronounced for participants whose cognitive resources were  more taxed by the load manipulation. Taken together, these  findings suggests that evaluations of intragroup harm rely  more heavily on reflexive responses, whereas evaluations of  intergroup harm rely more heavily on conscious  deliberation. 
1466
Study 2  
In Study 2, we tested whether the effects we documented in  adults are continuous throughout the lifespan by conducting  a similar study with children at the age when a host of social  category-based processes emerge (4-6 years). No prior  research has investigated the role of cognitive resources in  children's evaluations of inter- and intragroup harm. It is  possible that these processes are different from those we  found in adults; although young children certainly use social  categories when evaluating harmful behaviors (Rhodes &  Chalik, 2013), it is possible that the adult evaluations— evoking more or less deliberation in different group contexts 
—emerge slowly over time. On this account, we would  expect deliberative responses, which have been documented  in young children in prior research on moral development  (Smetana, 1985), to play a consistent role in children's  evaluations across various types of behaviors.  
It is also possible that the cognitive processes that guide  evaluations of inter- and intragroup harm in adults are  similarly variable in young children. Prior work supports  this possibility; as early as the preschool years, children  treat intragroup harm as a serious moral violation, whereas  they treat intergroup harm as wrong for more conventional  reasons (Rhodes & Chalik, 2013). Thus, there does appear  to be some variation in how children arrive at their  judgments of inter- and intragroup harm. Yet, no work has  examined the cognitive processes underlying these  judgments. If children’s evaluations are driven by the same  variation in deliberation that we found in adults in Study 1,  then we should find similar effects in children’s reaction  times to and their ratings of inter- and intragroup harm.  
Participants  
We recruited 110 4- to 6-year-old children (M age = 5.35,  range = 3.98 - 7.07, 57 female) at the Children’s Museum of  Manhattan. Researchers approached parents at the museum  and invited them to participate in research. Once parents had  given consent, children participated in a quiet room at the  museum. An additional 9 children were tested but excluded  from analysis because they did not complete the entire  testing session. Children were randomly assigned to the No  Load (n = 52) or Load (n = 58) conditions.  
Procedure  
Children performed the same task as the adults in Study 1  with the exception that instead of a digit span recall task, in  the Load condition, we used a prospective memory task— requiring participants to remember to perform a future  action after a cue. Prospective memory tasks have been  shown to interfere with performance in ongoing tasks in 4-  to 6-year-old children (Leigh & Marcovitch, 2014). Thus, in  the Load condition, children were told that they had to look  for a picture that contained someone wearing green shoes,  and they were instructed to ring a bell when they saw this  special picture. Children saw two prospective memory trials  (pictures containing green shoes) throughout the study—one  after each block of six test trials. Thus, children saw 14  trials total (six intragroup test trials, one intragroup  prospective memory trial, six intergroup test trials, one  
intergroup prospective memory trial), counterbalanced in  the same manner as in Study 1. Because the prospective  memory trials required a different type of response from the  test trials, these two trials were not included in our analyses 
—thus, our analyses included only the six test trials in each  block. Children therefore completed the same number of test  trials as adults in Study 1 (who did not see prospective  memory trials).  
As in Study 1, we recorded participants’ ratings of the  behaviors as well as their reaction times for each rating.  Trials in which participants took fewer than 500 ms or more  than 10,000 ms to respond were excluded from analysis. An  additional two children who took longer than 10,000 ms to  respond on over 25% of trials were excluded from analysis.  
Results  
To test whether children took longer to evaluate intergroup  harm than intragroup harm, we subjected children’s average  reaction times to a repeated measures ANOVA with  condition (Load or No Load) as a between-subjects factor  and group context (intergroup or intragroup) as a within 
subjects factor. We replicated the main effect of group  context, indicating that children responded more slowly for  intergroup harm (M = 5345, CI = 5108 - 5582) than they did  for intragroup harm (M = 4901, CI = 4607 - 5196), F(1,108)  =10.52, p < .005. There was no main or interactive effect of  condition.  
To test whether cognitive load interfered with children’s  ratings of the behaviors, we repeated the above analysis  with rating as the dependent variable. There were no  significant effects, indicating that children responded  similarly for intragroup and intergroup harm in both  conditions (Load: intragroup M = 2.10, CI = 1.76 - 2.43,  intergroup M = 2.26, CI = 1.99 - 2.54; No load: intragroup  M = 2.19, CI = 1.83 - 2.54, intergroup M = 2.02, CI = 1.73 -  2.31; ps > .13). We then investigated whether the effects  differed based on the extent to which children engaged in  the prospective memory task: Of the 58 children in the Load  condition, 25 successfully remembered to ring the bell on  the first prospective memory trial, and 27 failed to do so  (these children needed prompting to remember that they  were supposed to ring the bell). The remaining six  children’s sessions were not videotaped, and we thus could  not code whether they performed the prospective memory  task. We then repeated the above analysis excluding  children who failed the prospective memory task. Again,  there were no significant effects (ps > .41), indicating that  children responded similarly for intragroup and intergroup  harm in both conditions. Thus, the lack of an effect of  cognitive load here may be due to the fact that the  manipulation was not strong enough to divert children’s  cognitive resources.  
Discussion  
We replicated our reaction time results from Study 1 in  young children—children responded more slowly for  intergroup harm than for intragroup harm. But contrary to  our findings with adults, we found no effect on children’s  ratings of the behaviors—children rated the harmful  behaviors as equivalently bad, regardless of condition.  
1467
There are several possible explanations for these  discrepant findings across the two measures (ratings and  reaction times). One possibility is that, as we hypothesized,  children were slower to evaluate intergroup harm than  intragroup harm because they use more deliberative  processing to evaluate intergroup harm. From this  perspective, the cognitive load manipulation (the  prospective memory task) should have selectively interfered  with children’s evaluations of intergroup harm, as it did  among adults in Study 1. Yet, perhaps the load manipulation  that we used here was not successful among young children.  Similar manipulations have been used with children of these  ages in previous work (Leigh & Marcovitch, 2014);.  However, the present task was considerably more  complicated than previous studies. Thus, perhaps children  attended only to the evaluation task, because it was more  complicated, and did not devote any cognitive resources to  the prospective memory task. If so, then children in the  Load condition did not experience increased cognitive load,  despite the additional task that we asked them to perform.  The fact that excluding children who failed the prospective  memory task did not alter our findings lends support to this  possibility. If this is the case, then a stronger load  manipulation may selectively interfere with children's  evaluations of intergroup harm.  
Alternately, perhaps children were slower to evaluate  intergroup harm not because evaluating these behaviors  requires increased deliberation, but because of some other  feature of the intergroup trials. For example, these trials may  have required children to process more information than the  intragroup trials did (i.e., children had to note the presence  of two category memberships, instead of just one), so  perhaps this additional processing demand in the intergroup  trials caused children to respond more slowly, but not for  any reason that had to do with their evaluations of the  harmful behaviors. If this is the case, we might expect that  children's evaluations of intergroup harm would be  unaffected by an even stronger cognitive load manipulation.  Future research should distinguish these possibilities.  
General Discussion  
The present studies represent the first evidence that, among  adults, evaluations of intergroup harm rely more on  deliberative responses than evaluations of intragroup harm.  Two pieces of evidence support this claim. First, adults were  slower to evaluate intergroup harm than intragroup harm.  Second, under load, adults evaluated intergroup harm more  leniently than they evaluated intragroup harm, whereas  under no load, they evaluated the two types of harm  similarly. This pattern suggests that evaluations of  intergroup harm may require more deliberation. There was  some evidence for developmental continuity in these effects —young children were also slower to evaluate intergroup  than intragroup harm. Yet, because children's evaluations  were unaffected by the cognitive load manipulation, it  remains unclear whether similar processes shaped children's  and adults responses to these scenarios.  
These studies provide evidence that different cognitive  processes underlie the moral judgments that occur in  different types of intergroup contexts. A great deal of prior  
research has shown that concerns for social groups are an  important part of moral codes across human cultures (Haidt  & Joseph, 2004; Meier & Hinsz, 2004; Rai & Fiske, 2011;  Rozin et al., 1999; Shweder et al., 1997), yet no prior work  has attempted to document the actual processes by which  beliefs about social groups operate when people evaluate  moral scenarios. In the present studies, we have shown that  a combination of reflexive and deliberative processes shapes  people’s responses to these types of scenarios. Specifically,  we have shown that scenarios that do not violate people’s  basic intuitions about the function of social categories (i.e.,  harm between people from different groups) evoke more  conscious deliberation than scenarios that do violate those  intuitions (i.e., harm among people from the same group).  
These findings have important implications for the study  of moral development. Some theories have posited that  moral evaluation exists as a separate domain from other  types of reasoning. On this account, people make moral  judgments on the basis of whether an action causes harm or  unfair treatment, and only after the initial judgment is made  do they incorporate considerations for parts of the scenario  aside from the behavior itself, such as group membership  (Killen & Rizzo, 2014; Killen, Rutland, Abrams, Mulvey, &  Hitti, 2013). On this account, when people see a harmful  behavior occur, they immediately evaluate the behavior  negatively because it violates their basic intuition that harm  should be avoided. Then, after they have generated an initial  evaluation, they update that evaluation on the basis of  information about social group membership and other  relevant factors (possibly evaluating intragroup harm more  harshly, and intergroup harm more leniently, than they  initially had).  
The present findings suggest an alternative account: that  people's initial evaluations of harmful behaviors are made in  light of information about the social category membership  of the people involved. This is the intuition that President  Obama invoked when he noted that an American shooting  Americans on American soil made a tragedy even more  painful.  
Our results suggest that children hold the same basic  expectations about the function of social categories very  early in life. Their use of social categories in evaluating  moral scenarios appears to be a very early-emerging feature  of human cognition. Across both studies we designed the  stimuli and language to be simple enough that it could be  used with both children and adults. This approach was  helpful in that it allowed us to run similar studies with  adults and with children, which strengthens the conclusions  that we can draw about the relationship between children’s  and adults' moral evaluations. As such, the present work  suggests that this part of the moral judgment system remains  relatively stable across development. It is thus striking that  we did find such a clear pattern of responses in adults while  using child-friendly stimuli. Still, future work should  investigate adults' responses to a wider range of complex  intergroup scenarios.  
Future work should also further investigate the processes  underlying children’s judgments of inter- and intragroup  harm. As noted above, the present studies can be taken as  some evidence for developmental continuity in these  
1468
processes; both adults and children responded more slowly  to intergroup harm than to intragroup harm, suggesting that  across development, evaluations of intergroup harm rely  more on deliberative processing. Yet, because the cognitive  load manipulation used here did not influence children’s  judgments, any conclusions that we are able to draw  regarding developmental continuity remain tentative. To  fully investigate whether children's evaluations are guided  by the same underlying processes as those found in adults,  future work should examine children's evaluations of inter and intra-group harmful behaviors while putting them under  a greater degree of cognitive load. If a stronger cognitive  load manipulation selectively interferes with children's  judgments in the same way that it interfered with those of  adults, then we will be able to conclude more strongly that  children, like adults, rely more heavily on deliberative  processing when evaluating intergroup harm than when  evaluating intragroup harm.  
Despite these open questions, the present work represents  an important contribution to the literatures on child and  adult moral cognition. These studies have provided the first  step toward documenting the cognitive processes that  underlie people's responses to inter- and intra-group harm.  In doing so, they have expanded our understanding of the  nature of adult moral cognition as well as the processes that  shape moral cognition across development. 
Acknowledgments  
We are very grateful to the families and staff at the  Children’s Museum of Manhattan, and to the members of  the NYU Conceptual Development and Social Cognition  Lab. Funding was provided by NSF BCS-1226942, NSF  BCS-1147543, and NSF BCS-1349089.  
References  
Amit, E., & Green, J.D. (2012). You see, the ends don’t  justify the means: Visual imagery and moral judgment.  Psychological Science, 23, 861-868.  
Chaiken, S., & Trope, Y. (1999). Dual-process theories in  social psychology. New York: Guilford Press.  Cikara, M., Botvinick, M.M., & Fiske, S.T. (2011). Us  versus them: Social identity shapes neural responses to  intergroup competition and harm. Psychological Science,  22, 306-313. 
Cohen, T., Montoya, R., & Insko, C. (2006). Group morality  and inter-group relations: Cross-cultural and experimental  evidence. Personality and Social Psychology Bulletin, 11,  1559-1572.  
Engle, R.W. (2002). Working memory capacity as executive  attention. Current Directions in Psychological Science,  11, 19-23.  
Greene, J.D., Sommerville, R..B., Nystrom, L.E., Darley,  J.M., & Cohen, J.D. (2001). An fMRI investigation of  emotional engagement in moral judgment. Science, 293,  2105-2108.  
Haidt, J., & Joseph, C. (2004). Intuitive ethics: How  innately prepared intuitions generate culturally variable  virtues. Daedalus: Special Issue on Human Nature, 133,  55-66.  
Helwig, C.C. (2006). Rights, civil liberties, and democracy  across cultures. In M. Killen & J.G. Smetana (Eds.),  Handbook of moral development. Mahwah, NJ: Lawrence  Erlbaum Associates Publishers.  
Kahneman, D. (2003). A perspective on judgment and  choice: Mapping bounded rationality. American  Psychologist, 58, 697-720.  
Killen, M., & Rizzo, M. (2014). Morality, intentionality, and  intergroup attitudes. Behaviour, 151, 337-359.  Killen, M., Rutland, A., Abrams, D., Mulvey, K.L., & Hitti,  A. (2013). Development of intra- and intergroup  judgments in the context of moral and social-conventional  norms. Child Development, 84, 1063-1080.  
Leigh, J., & Marcovitch, S. (2014). The cognitive cost of  event-based prospective memory in children. Journal of  Experimental Child Psychology, 127, 24-35.  
Longstaffe, K.A., Hood, B.M., & Gilchrist, I.D. (2014). The  influence of cognitive load on spatial search performance.  Attention, Perception, & Psychophysics, 76, 49-63. 
Meier, B., & Hinsz, V. (2004). A comparison of human  aggression committed by groups and individuals: An  interindividual-intergroup discontinuity. Journal of  Experimental Social Psychology, 40, 551-559.  
Nagel, T. (1987). What does it all mean? New York: Oxford  University Press.  
Rai, T.S., & Fiske, A.P. (2011). Moral psychology is  relationship regulation: Moral motives for unity,  hierarchy, equality, and proportionality. Psychological  Review, 118, 57-75.  
Rhodes, M., & Chalik, L. (2013). Social categories as  markers of intrinsic interpersonal obligations.  Psychological Science, 24, 999-1006.  
Rozin, P., Lowery, L., Imada, S., & Haidt, J. (1999). The  CAD triad hypothesis: A mapping between three moral  emotions (contempt, anger, disgust) and three moral codes  (community, autonomy, divinity). Journal of Personality  and Social Psychology, 76, 574-586.  
Shweder, T.A., Much, N.C., Mahapatra, M., & Park, L.  (1997). The “big three” of morality (autonomy,  community, divinity) and the “big three” explanations of  suffering. In A.M. Brandt & P. Rozin (Eds.), Morality and  health. Florence: Taylor & Frances/Routledge.  
Smetana, J.G. (1985). Preschool children’s conceptions of  transgressions: Effects of varying moral and conventional  domain-related attributes. Developmental Psychology, 21,  18-29.  
Smetana, J. (2006). Social domain theory: Consistencies and  variations in children’s moral and social judgments. In M.  Killen & J.G. Smetana (Eds.), Handbook of moral  development. Mahwah, NJ: Erlbaum.  
Stanovich, K.E., & West, R.F. (2000). Individual differences  in reasoning: Implications for the rationality debate?  Behavioral and Brain Sciences, 23, 645-665.  
The White House, Office of the Press Secretary. (2009).  Remarks by the President at memorial service at Fort  Hood [Press release].  
Wainryb, C. (2006). Moral development in culture:  Diversity, tolerance, and justice. In M. Killen & J.G.  Smetana (Eds.), Handbook of moral development.  Mahwah, NJ: Lawrence Erlbaum Associates Publishers. 
1469
Words and non-speech sounds accesslexical and semantic knowledge differently 
Peiyao Chen1(pchen@u.northwestern.edu)  
James Bartolotti1(j-bartolotti@u.northwestern.edu) 
Scott R. Schroeder2(scott.r.schroeder@hofstra.edu) 
Sirada Rochanavibhata1(siradarochanavibhata2020@u.northwestern.edu)  
Viorica Marian1(v-marian@northwestern.edu) 
1Department of Communication Sciences and Disorders, Northwestern University, Evanston, IL 60208 USA 2Department of Speech-Language-Hearing Sciences, Hofstra University, Hempstead, NY 11549 USA  
Abstract 
Using an eye-tracking paradigm, we examined the strength  and speed of access to lexical knowledge (e.g., our  representation of the word dog in our mental vocabulary) and  semantic knowledge (e.g., our knowledge that a dog is  associated with a leash) via both spoken words (e.g., “dog”)  and characteristic sounds (e.g., a dog’s bark). Results show  that both spoken words and characteristic sounds activate  lexical and semantic knowledge, but with different patterns.  Spoken words activate lexical knowledge faster than  characteristic sounds do, but with the same strength. In  contrast, characteristic sounds access semantic knowledge  stronger than spoken words do, but with the same speed.  These findings reveal similarities and differences in the  activation of conceptual knowledge by verbal and non-verbal  means and advance our understanding of how auditory input  is cognitively processed. 
Keywords: speech comprehension; sound processing; lexical  competition; semantic competition; eye-tracking 
Introduction 
The human auditory system receives and processes different  types of input from the environment. Sounds that come from  an entity typically provide information about a specific  member of a group: the sound of a dog barking usually  reveals that particular dog’s size and location. Words, in  contrast, often refer to a category without providing  information about the specific member of that group. For  example, the spoken word “dog” provides no information  about either the dog’s size or location. Models of auditory  processing take into account words’ and sounds’ unique  features, and propose that these two types of input access  conceptual knowledge via different routes (Chen & Spence,  2011). In the current study, we directly examine and  compare the timecourse of semantic and lexical activation  by spoken words and characteristic sounds. 
During speech processing, individual words’ lexical form  and semantic meaning are rapidly accessed (Connolly &  Phillips, 1994; Van Petten, Coulson, Rubin, Plante, & Parks,  1999). Hearing “dog” activates the representation of the  word dog in the mental lexicon, as well as semantic features  associated with the concept of dog (e.g., “barks” and “has  fur”). Evidence of lexical and semantic activation cued by  spoken words can be observed in the form of spreading  activation to related words or concepts. Eye-tracking studies  
have shown that upon hearing a spoken word (e.g., “dog”),  people often briefly look at pictures representing words that  are lexically related (e.g., doctor which shares its onset with  the word dog) or semantically related (e.g., cat which  belongs to the same semantic category as dog, or leash which is associatively related to dog) (Allopenna,  Magnuson, & Tanenhaus, 1998; Huettig & McQueen, 2007;  Yee & Sedivy, 2006). 
Characteristic sounds, similar to spoken words, have been  shown to also trigger access to semantic information (Chen  & Spence, 2011, 2013; Edmiston & Lupyan, 2015). Hearing  a characteristic sound, like a dog’s bark, activates an entity’s  semantic features and facilitates picture identification (Chen  & Spence, 2011) and visual search (Iordanescu,  Grabowecky, Franconeri, Theeuwes, & Suzuki, 2010).  However, direct evidence of how sounds access lexical  information is lacking. Furthermore, while current speech  processing models, such as TRACE (McClelland & Elman,  1986), can be adapted to incorporate non-speech sounds,  empirical data comparing word and sound processing are  needed to inform modeling efforts. 
The aim of the current study is to directly compare the  strength and rate with which spoken words and  characteristic sounds provide access to information  associated with a concept. In a visual world eye-tracking  experiment, we assessed spreading activation from  auditorily-presented targets (a spoken word, e.g., “dog” or  characteristic sound, e.g., <bark-bark>) to their lexical and  semantic competitors. In lexical activation trials, a picture of  a phonological onset competitor was present on the screen  (e.g., a picture of a cloud when the target was the word  “clock” or a <tick-tock> sound). In semantic activation  trials, a picture of an associative semantic competitor was  present on the screen (e.g., a picture of a bone when the  target was the word “dog” or a <bark-bark> sound). Access  to lexical/sematic information is indexed by visual fixation  patterns to lexical/semantic competitors. 
Our predictions are based on the multisensory framework  proposed by Chen and Spence (2011), an extension of  Glaser and Glaser’s reading-naming interference model  (1989). Chen and Spence propose that spoken words and  characteristic sounds cue access to concepts via different  intermediaries. Spoken words have a direct connection to  phono-lexical representations, whereas characteristic sounds  connect directly to semantic representations. The phono 
1470
lexical and semantic representations are interconnected,  allowing for words and sounds to each access lexical and  semantic information. Based on this framework, we predict  that spoken words will activate lexical representation  stronger and/or faster than characteristic sounds. Likewise,  characteristic sounds will activate semantic representation  stronger and/or faster than spoken words. 
Method 
Participants 
Thirty monolingual English speakers participated in the  study. These participants were randomly assigned to the  characteristic sound condition (n = 15, 14 female) or the  spoken word condition (n = 15, 13 female). Eye-tracking  data for one participant in the characteristic sound condition  was lost due to equipment error. The remaining participants  in the sound and word conditions did not differ in age, non verbal IQ scores (Wechsler Abbreviated Scale of  Intelligence; WASI, PsychCorp, 1999), phonological  memory scores (digit span and nonword repetition subtests  of the Comprehensive Test of Phonological Processing;  CTOPP, Wagner, Torgesen, & Rashotte, 1999), or English  receptive vocabulary scores (Peabody Picture Vocabulary  Test; PPVT, Dunn, 1997).  
Materials 
Fifteen sets of stimuli were created for each competitor  type, lexical and semantic. The 15 lexical sets included three  critical items: A target (e.g., clock), a phonological onset  competitor (e.g., cloud) whose name overlapped with the  target, and a control (e.g., lightbulb) that did not overlap.  The 15 semantic sets also included three critical items: a  target (e.g., chicken), an associative semantic competitor  (e.g., egg), and a control (e.g., snowman). In each group of  15 sets, the target word, the lexical/semantic competitor, and  the control did not differ from each other in word frequency  (SUBTLEXUS; Brysbaert & New, 2009), phonological and  orthographic neighborhood size (CLEARPOND; Marian,  Bartolotti, Chabal, & Shook, 2012), familiarity,  concreteness, or imageability (MRC Psycholinguistic  Database; Coltheart, 1981). 
These sets were used to create 240 trials; in 50% of these  trials, the target picture was absent from the display. Sixty  of these target-absent trials comprised the set of  experimental trials; analyses were limited to target-absent  trials to ensure that competitor activation was caused by the  auditory stimulus itself, instead of only the pictures on the  screen (see Chabal & Marian, 2015). In 30 competitor trials,  each competitor (e.g., cloud or egg) appeared in a display  with three unrelated pictures. In 30 control trials, the  competitor was replaced with a control object (e.g.,  lightbulb or snowman) in the same location. The 180 filler  trials were designed to mask the experimental manipulation  and to balance the number of times each picture was viewed  in the experiment (i.e., targets, competitors, controls, and  other unrelated items). 
Pictures were black and white line drawings from the  International Picture Naming Database (Bates et al., 2000) or independently normed by 20 English monolinguals using  Amazon Mechanical Turk (http://www.mturk.com). These  pictures were positioned in the four corners of a 3 x 3  invisible square grid. Pictures in the same display were  similar in saturation (i.e., none of the pictures were darker  than the others) and line thickness. Participants were seated  approximately 80 cm away from a computer screen (2560 x  1440 resolution) while their eye-movements were tracked  using an Eyelink 1000 eye-tracking system recording at 250  Hz. The words representing the 30 target items were  recorded by a Midwestern female speaker of Standard  American English. Word and sound stimuli were amplitude  normalized and played through closed-back headphones.  Spoken word durations (M = 731.7 ms, SE = 4.93, Range =  [502, 1066]) were shorter than characteristic sounds (M =  1545.4 ms, SE = 28.53, Range = [329, 3868]), t(29) = 5.33,  p < .001, due to the fact that many continuous sounds do not  have a fixed ending point, as words do. Note that duration  was not correlated with response times (R2 = .001, n.s.). To  account for any potential effects of auditory recording  length on visual fixations, duration was included as an  additional predictor in all models. 
Procedure 
A fixation cross was shown on the screen for 1500 ms,  followed by the four-object display. The display was shown  for 500 ms before the participants heard either a  characteristic sound or a spoken word. After the onset of the  auditory input, the objects remained onscreen for 4500 ms  before they disappeared. Participants were instructed to  click on the target picture as quickly as possible if the target  was present, and to click on the fixation cross in the center  of the screen if the target picture was absent. Before the  experiment, participants completed a set of practice trials.  
Data Analysis 
Accuracy was analyzed using linear mixed effects  regression. By-subject and by-item averaged models were  created; with fixed effects of Auditory-input (word, sound),  Condition (lexical, semantic), and Competition (competitor,  control) and their interactions, as well as a random intercept  of either subject or item (mixed effects logistic regression  with subject and item random effects was not possible due  to multicollinearity of fixed effects). Response times were  analyzed for correct trials only, and outliers (greater than the  condition mean plus two standard deviations) were replaced  with M+2SD (4.72% of trials). The RT model included  fixed effects of Auditory-input, Condition, and Competition  plus their interactions, as well as random intercepts of both  subject and item. Significance of fixed effects were obtained  using t-tests and the Satterthwaite approximation for degrees  of freedom. Follow-up pairwise comparisons used the  Tukey correction for multiple comparisons. 
The time course of visual fixations to semantic and lexical  competitors was analyzed using growth curve analysis  
1471
(Mirman, Dixon, & Magnuson, 2008). Visual fixations were  analyzed in 25 ms bins for correct trials only, averaged by  items. Fixations were analyzed from 200 ms post-word  onset (the time required to plan and execute an eye  movement, Viviani, 1990) until each condition’s average  RT. Level-1 models used fourth-order orthogonal  polynomials to capture the rise and fall of visual fixations  over time. Level-2 models included all time terms and  random effects of item on all time terms, plus additional  fixed effects of each variable of interest. The difference  between fixations to competitors and controls was analyzed  separately for each combination of Auditory-input (word,  sound) and Condition (lexical, semantic). All models  included each item’s auditory duration (scaled score) on all  time terms, as adding auditory duration significantly  improved each model’s fit (ps < .001). Parameter p-values  were obtained using the Satterthwaite approximation for  degrees of freedom. 
Results 
Eye movements 
Competitor fixations. We found a significant effect of  lexical competition in response to spoken words on the  intercept (β = -0.030, SE = 0.005, t(1245) = -6.39, p < .001),  linear (β = 0.077, SE = 0.031, t(1245) = 2.47, p < .05), and  cubic terms (β = -0.161, SE = 0.031, t(1245) = -5.17, p < .001). These effects captured a larger, earlier fixation peak 
for lexical competitors compared to controls (Figure 1, top left), indicating rapid lexical access by spoken words.  
Duration interacted with competition on the intercept (β =  0.013, SE = 0.005, t(1245) = 2.85, p < .01) and quadratic  terms (β = -0.119, SE = 0.031, t(1245) = -3.81, p < .001); 
longer words activated lexical information less strongly. There was a significant effect of lexical competition in  response to characteristic sounds on the intercept (β = - 0.023, SE = 0.005, t(1200) = -5.19, p < .001), quadratic (β = 0.174, SE = 0.029, t(1200) = 5.92, p < .001), and quartic (β = -0.131, SE = 0.029, t(1200) = -4.47, p < .001) terms.  These effects captured a late divergence between competitor  and control fixations (Figure 1, top-right), indicating delayed lexical access by sounds. Duration interacted with  competition on the intercept (β = 0.013, SE = 0.005, t(1200)  = 2.95, p < .01) and quadratic terms (β = -0.060, SE = 0.029,  t(1200) = -2.04, p < .05). As with words, sounds with longer  durations activated lexical information less strongly. There was a significant effect of semantic competition in  response to spoken words on the intercept (β = -0.023, SE =  0.004, t(1305) = -5.57, p < .001), quadratic (β = 0.155, SE =  0.028, t(1305) = 5.54, p < .001), and quartic (β = -0.067, SE = 0.028, t(1305) = -2.39, p < .05) terms. These effects  captured a large competitor peak above a steady control  baseline in the middle of the analysis window (Figure 1,  bottom-left), indicating late semantic access by spoken  words. Duration had a significant effect on the cubic term (β  = -0.040, SE = 0.017, t(25.1) = -2.34, p < .05), and  interacted with competition on the intercept (β = 0.054, SE =  0.004, t(1305) = 12.93, p < .001), linear (β = 0.056, SE =  0.028, t(1305) = 2.00, p < .05), and quadratic 
  

Figure 1: Activation of lexical and semantic competitors in response to spoken words and characteristic sounds. Lines  represent model fits for fixations to competitors (color) and controls (black). Dots and vertical lines indicate observed values  and standard error, respectively. 
1472
terms (β = -0.100, SE = 0.028, t(1305) = -3.58, p < .001).  These effects captured decreased semantic activation in  response to longer words. 
Finally, there was a significant effect of semantic  competition in response to characteristic sounds on the  intercept (β = -0.024, SE = 0.004, t(1185) = -5.86, p < .001),  linear (β = -0.076, SE = 0.027, t(1185) = -2.84, p < .01), and  quartic (β = 0.058, SE = 0.027, t(1185) = 2.15, p < .05)  terms. These effects captured a small, steady increase in  competitor fixations early on, followed by a large late  divergence between competitor and control fixations (Figure  1, bottom-right), which indicates sustained access to  semantic information by sounds. Duration had a significant  effect on the quadratic term (β = 0.065, SE = 0.029, t(15) =  2.27, p < .05), and interacted with competition on the  intercept (β = 0.036, SE = 0.004, t(1185) = 8.72, p < .001), linear (β = -0.110, SE = 0.027, t(1185) = -4.09, p < .001),  and cubic (β = -0.115, SE = 0.027, t(1185) = -4.30, p < .001)  terms. These effects capture a large effect of duration across  the observed time window. Longer sounds only activate  semantics at a very late stage, whereas shorter sounds  activate semantic information at both early and late stages. 
Comparing word and sound access to lexical and  semantic information. To facilitate comparisons across  conditions, difference curves were calculated by subtracting  control fixations from competitor fixations for each of the  four levels of Auditory-input by Condition. A linear mixed  effects regression model was designed, including fixed  effects of Auditory-input (word, sound), Condition (lexical,  semantic), and duration plus their interactions on all time  terms, as well as a random effect of item. Crucially, there  was an interaction between Auditory-input and Condition  on the quadratic term (β = 1.015, SE = 0.16, t(2021) = 6.35,  p < .001), which is followed up in two analyses, one  comparing activation of lexical information by words vs.  sounds, and the other comparing activation of semantic  information by words vs. sounds.  
For lexical activation, Auditory input had a significant  effect on the intercept (β = -0.073, SE = 0.02, t(1119) = , p <  .001) and quadratic terms(β = -0.271, SE = 0.12, t(816) = , p < .05); duration had an effect on the intercept (β = -0.076,  
SE = 0.02, t(1042) = -4.60, p < .001) and quadratic terms (β  = -0.324, SE = 0.11, t(689) = -2.99, p < .01), and interacted  with auditory input on the intercept (β = -0.121, SE = 0.03,  t(1194) = -4.10, p < .001) and quadratic (β = -0.815, SE =  0.19, t(943) = -4.19, p < .001) terms. The combined effects  captured the earlier peak of lexical activation for words  (Figure 2, left, dark red) compared to sounds (Figure 2, left,  light orange). These results suggest that words access lexical  information faster than sounds.  
For semantic activation, Auditory input also had a  significant effect on the intercept (β = -0.086, SE = 0.02,  t(1090) = -5.58, p < .001) and quadratic terms(β = 0.713, SE = 0.10, t(1034) = 6.94, p < .001); duration had an effect on  the intercept (β = -0.072, SE = 0.01, t(1060) = -5.74, p <  .001) and quadratic (β = 0.704, SE = 0.08, t(995) = 8.43, p <  .001) terms, and interacted with auditory input on the  intercept (β = -0.129, SE = 0.02, t(1138) = -5.48, p < .001)  and quadratic terms (β = 1.162, SE = 0.16, t(1079) = 7.43, p < .001). The combined effects manifested differently than  lexical activation: For semantic information, words resulted  in a peak in the middle of the window (Figure 2, right, dark  blue), and sounds peaked closer to the offset of the window  (Figure 2, right, light green). While words and sounds  activated semantic information at the same rate, sounds had  stronger access to semantics with higher peak activation.  
Accuracy.  
We found a significant three-way interaction (by-subjects,  t(81) = 3.38, p < .001; by-items, t(84) = 2.74, p < .01).  Follow-up pairwise comparisons indicated that the  Semantic-Sound Competitor had lower accuracy (86.5%)  than all other conditions (all higher than 97.6%, ps < .001,  by-subjects and by-items); no other comparisons were  significant. Most errors (83.3%) in the Semantic Sound  condition were caused by clicks on the semantic competitor.  
Response time.  
There was a significant main effect of Competition (β = - 118.72, SE = 19.22, t(1635.3) = -6.18, p < .001) indicating  that the presence of a competitor slowed down participants’  assertion that the target was not present. The 
  

Figure 2: Effect of auditory input on lexical/semantic activation. Left: Words (red) activate lexical information earlier than  sounds (orange). Right: Sounds (green) activate semantic information more strongly than words (blue). Curves represent  predicted model values when auditory duration is set to a constant value (median word duration). 
1473
interaction between Condition and Competition was also  significant (β = 86.83, SE = 38.44, t(1635.3) = 2.26, p <  .05). Follow-up pairwise comparisons showed that RTs  during trials with semantic competitors were 163.56 ms  slower than trials with matched controls, t(1636) = 5.96, p <  .001 and that RTs during trials with lexical competitors  were 77.19 ms slower than trials with matched controls,  t(1634.8) = 2.86, p < .05. 
Discussion 
The current study examined relative activation of word-form  (i.e., lexical) knowledge and meaning (i.e., semantic)  knowledge while listening to spoken words or characteristic  sounds using eye-tracking in a visual world paradigm.  While sounds and words are processed similarly in many  ways: both are influenced by context, familiarity, and  frequency (Ballas, 1993, Edmiston & Lupyan, 2014, Stuart  & Jones, 1995), and both are similarly influenced by noise  degradation (Aramaki, Marie, Kronland-Martinet, Ystad, &  Besson, 2010, Gygi, Kidd, & Watson, 2004), models of  auditory processing propose that words and sounds may  access conceptual knowledge via different routes (Chen &  Spence, 2011). Our aim was to determine whether sounds  and words vary in how they provide access to lexical and  semantic knowledge. We found different patterns for words  and sounds in their access to lexical/semantic information.  Specifically, spoken words were found to access lexical  information earlier than sounds, but with similar intensity.  In contrast, characteristic sounds were found to access  semantics more strongly than words, but at a similar rate. 
By comparing the shape and timecourse of visual  fixations to lexical and semantic competitors, we discovered  privileged access by spoken words to lexical information,  and by characteristic sounds to semantic information. While  lexical competition can be activated by both a spoken word  and a characteristic sound, participants fixated the lexical  competitor several hundred milliseconds earlier when cued  by a word compared to a sound. This result supports the  auditory processing model of Chen and Spence (2011),  which states that spoken words first activate a lexical  representation, whereas sounds first activate a semantic  representation, which then spreads to the lexicon. These  direct and indirect lexical pathways are reflected in the  staggered timing of activation peaks observed in our study.  Our results also demonstrate that non-linguistic sounds  alone can provide fast access to lexical information,  potentially via the concepts they activate. 
A different pattern was observed for activation of  semantic information. Once again, both a spoken word and a  characteristic sound created semantic competition. This  finding is consistent with results from cortical processing of  semantic violations, where words and sounds were found to  evoke similar cortical responses using event-related  potentials (Hendrickson, Walenski, Friend, & Love, 2015).  However, while both words and sounds started to increase  semantic activation at the same rate, words reached an  earlier and lower peak than sounds did. Chen and Spence’s  
model proposes that characteristic sounds first activate  semantic representations, which then feed forward to lexical  representations. Our results partially support this proposal,  as we find stronger activation of semantics by sounds, but  we do not find a sound advantage in rate – in fact, words  reach earlier peak activation than sounds.  
This apparent departure from the model may be resolved  when we consider differences in the nature of the semantic  representation that is primarily accessed by words and  sounds. Words, particularly concrete nouns as used in the  current study, activate prototypical sematic concepts: “bird”  typically makes one think of a songbird animal, rather than  an ostrich or penguin (Hampton, 2016). Characteristic  sounds, on the other hand, are closely linked to their original  source and specific matching referents (Edmiston &  Lupyan, 2013, 2015). In the context of the current study, the  spoken word cue may have first accessed a lexical  representation, followed by a prototypical semantic concept,  which spread activation to related semantic concepts. The  characteristic sound cue may have first accessed a  representation for a specific referent that closely matched  the source sound; this specific representation then spread to  the prototypical semantic concept, and from there to related  semantic concepts (i.e., the semantic competitor). This  additional specific-to-general step for sounds may have  contributed to the slower rate of competitor activation.  
Our results also demonstrate the influence of the duration  of an auditory signal on information access. Changes in the  duration of either words or sounds had the same effects,  where shorter durations increased lexical and semantic  activation relative to longer durations. This consistent  duration effect may be related to continuous auditory input  processing. Speech processing models posit that as a spoken  word unfolds, all lexical items that are consistent with the  partially received input become activated, and start to  decline as they diverge from the input (McClelland &  Elman, 1986; Shook & Marian, 2013). During the partially produced stage, activation is spread diffusely among  multiple representations, which decreases the level of any individual item. It is possible that non-speech sound  processing follows a similar pattern where multiple  representations are initially activated and then pruned,  leading to the same duration effect for sounds that we  observe for words. 
We elected to use separate targets to examine lexical and  semantic competition in order to minimize priming effects,  and due to the constraints inherent in selecting identifiable picture pairs with recognizable characteristic sounds. Now  that distinct lexical and semantic effects have been  established, it will be informative to directly compare them  using target – lexical competitor – semantic competitor  triplets (e.g., clock-cloud-radio). In addition, the issue of  different word and sound durations should be controlled in  future work. Note that the longer sound durations likely  increased the ecological validity of this study, as many  environmental sounds are continuous, compared to spoken  words’ fixed ending points. 
1474
In conclusion, we have identified similarities and  differences in how humans process two types of auditory  input – linguistic spoken words and non-linguistic  characteristic sounds. The observed preferential access to  lexical information by spoken words, and to semantic  information by non-speech sounds, reveals features of the  cognitive architecture used to process sounds. These results  highlight the interconnectivity of the mind, with interactions  observed among linguistic and non-linguistic processing,  auditory and visual processing, and lexical and semantic  processing.  
Acknowledgments 
The authors thank the members of the Northwestern  University Bilingualism and Psycholinguistics Research  Group for helpful comments and input. This work was  supported in part by grant NICHD 2R01 HD059858. 
References  
Allopenna, P., Magnuson, J. S., & Tanenhaus, M. K. (1998).  Tracking the time course of spoken word recognition  using eye movements: Evidence for continuous mapping  models. Journal of Memory and Language, 38(4), 419– 439.  
Bates, E., Andonova, E., D’Amico, S., Jacobsen, T.,  Kohnert, K., Lu, C., … Pleh, C. (2000). Introducing the  CRL international picture naming project (CRL-IPNP).  Center for Research in Language Newsletter, 12(1). 
Brysbaert, M., & New, B. (2009). Moving beyond Kucera  and Francis: A critical evaluation of current word  frequency norms and the introduction of a new and  improved word frequency measure for American English.  Behavior Research Methods, 41(4), 977–990.  
Chabal, S., & Marian, V. (2015). Speakers of different  languages process the visual world differently. Journal of  Experimental Psychology: General, 144(3), 539–550. 
Chen, Y.-C., & Spence, C. (2011). Crossmodal semantic  priming by naturalistic sounds and spoken words  enhances visual sensitivity. Journal of Experimental  Psychology: Human Perception and Performance, 37(5),  1554–1568.  
Chen, Y.-C., & Spence, C. (2013). The time-course of the  cross-modal semantic modulation of visual picture  processing by naturalistic sounds and spoken words.  Multisensory Research, 26(4), 371–386.  
Coltheart, M. (1981). The MRC psycholinguistic database.  Quartely Journal of Experimental Psychology, 33(4),  497–505.  
Connolly, J. F., & Phillips, N. A. (1994). Event-Related  Potential Components Reflect Phonological and Semantic  Processing of the Terminal Word of Spoken Sentences.  Journal of Cognitive Neuroscience, 6(3), 256–266.  
Dunn, L. M. (1997). Examiner’s Manual for the PPVT-III:  Peabody Picture Vocabulary Test-Third Edition. Circle  Pines, MN: American Guidance Service. 
Edmiston, P., & Lupyan, G. (2013). Verbal and nonverbal  cues activate concepts differently, at different times.  
Proceedings of the 35th Annual Conference of the  Cognitive Science Society, 2243–2248. 
Edmiston, P., & Lupyan, G. (2015). What makes words  special? Words as unmotivated cues. Cognition, 143, 93– 100.  
Glaser, W. R., & Glaser, M. O. (1989). Context effects in  stroop-like word and picture processing. Journal of  Experimental Psychology. General, 118(1), 13–42.  
Hampton, J. A. (2016). Categories, prototypes, and  exemplars. In N. Reimer (Ed.), Routledge Handbook of  Semantics (pp. 125–141). New York: Routledge. 
Hendrickson, K., Walenski, M., Friend, M., & Love, T.  (2015). The organization of words and environmental  sounds in memory. Neuropsychologia, 69, 67–76. 
Huettig, F., & McQueen, J. M. (2007). The tug of war  between phonological, semantic and shape information in  language-mediated visual search. Journal of Memory and  Language, 57(4), 460–482.  
Iordanescu, L., Grabowecky, M., Franconeri, S., Theeuwes,  J., & Suzuki, S. (2010). Characteristic sounds make you  look at target objects more quickly. Attention, Perception  & Psychophysics, 72(7), 1736–1741.  
Marian, V., Bartolotti, J., Chabal, S., & Shook, A. (2012).  CLEARPOND: Cross-Linguistic Easy-Access Resource  for Phonological and Orthographic Neighborhood  Densities. PloS One, 7(8), e43230.  
McClelland, J. L., & Elman, J. L. (1986). The TRACE  Model of Speech Perception. Cognitive Psychology, 18,  1–86. 
Mirman, D., Dixon, J. A., & Magnuson, J. S. (2008).  Statistical and computational models of the visual world  paradigm: Growth curves and individual differences.  Journal of Memory and Language, 59(4), 475–494.  
Mirman, D., Magnuson, J. S., Graf Estes, K., & Dixon, J. A.  (2008). The link between statistical segmentation and  word learning in adults. Cognition, 108(1), 271–280.  
PsychCorp. (1999). Wechsler abbreviated scale of  intelligence (WASI). San Antonio, TX: Harcourt  Assessment. 
Shook, A., & Marian, V. (2013). The Bilingual Language  Interaction Network for Comprehension of Speech.  Bilingualism (Cambridge, England), 16(2), 304–324.  
Van Petten, C., Coulson, S., Rubin, S., Plante, E., & Parks,  M. (1999). Time course of word identification and  semantic integration in spoken language. Journal of  Experimental Psychology: Learning, Memory, and  Cognition.  
Viviani, P. (1990). Eye movements in visual search:  Cognitive, perceptual and motor control aspects. Reviews  of Oculomotor Research, 4, 353–93.  
Wagner, R. K., Torgesen, J. K., & Rashotte, C. A. (1999).  The comprehensive test of phonological processing.  Austin, TX: Pro-Ed. 
Yee, E., & Sedivy, J. C. (2006). Eye movements to pictures  reveal transient semantic activation during spoken word  recognition. Journal of Experimental Psychology:  Learning, Memory, and Cognition, 32(1), 1–14. 
1475
Optimal face recognition performance involves a balance between global and local  information processing: Evidence from cultural difference 
Zhijie Cheng (chengzj@hku.hk) 
Department of Psychology, The University of Hong Kong, Pokfulam Road, Hong Kong 
William G. Hayward (whayward@hku.hk) 
Department of Psychology, The University of Hong Kong, Pokfulam Road, Hong Kong 
Department of Psychology and ARC Centre of Excellence in Cognition and Its Disorders, The University of Auckland,  Private Bag 92019, Auckland, New Zealand 
Antoni B. Chan (abchan@cityu.edu.hk) 
Department of Computer Science, City University of Hong Kong, Tat Chee Avenue, Hong Kong 
Janet H. Hsiao (jhsiao@hku.hk) 
Department of Psychology, The University of Hong Kong, Pokfulam Road, Hong Kong 
Abstract 
In face recognition, eye gaze to the eye region is reported to  be associated with better performance than to the center of a  face. Nevertheless, Caucasians and Asians differ in how much  they look at the eyes when they scan a face, but have  comparable identification performance. To resolve this issue,  here we test the hypothesis that optimal face recognition  performance involves a balance between global and local face  processing. Thus, Asians may benefit from enhancement of  local processing and vice versa for Caucasians. We showed  that local attention priming using hierarchical letter stimuli  led to more eye-focused eye movement patterns compared to global attention priming in both Asians and Caucasians.  However, Asians had better performance after local priming  than global priming, whereas Caucasian showed the opposite  effect. These results suggest that engagement of global/local  attention leads to face-center/eye biased eye movements  respectively, and optimal recognition performance involves  both global and local processing/gaze transitions between the  face center and eyes.  
Keywords: eye movement, face recognition, cultural  difference, hidden Markov model, EMHMM 
Introduction 
Humans have a remarkable ability to recognize individual  faces. Nevertheless, it remains unclear what kind of  information use can lead to optimal face recognition  performance. Recent studies have reported substantial  individual differences in eye movement patterns in face  recognition (Peterson & Eckstein, 2013), which may reflect  individual differences in information use and recognition  performance. To account for these individual differences in  eye movement data analysis, Chuk, Chan, and Hsiao (2014) proposed the Eye Movement analysis with Hidden Markov  Models (EMHMM) approach, in which they modeled each  participant’s eye movement pattern in face recognition with  a hidden Markov model (HMM, a type of machine learning  model for time series data), including personalized regions  of interest (ROIs) and transition probabilities among the  ROIs. Through clustering these individual models according  
to their similarities, they discovered two common patterns:  “holistic” pattern, in which observers mainly looked at the  face center; and “analytic” pattern, in which observers  looked at the eye region in addition to the face center.  Interestingly, analytic patterns were associated with better  recognition performance (Chan, Chan, Lee, & Hsiao, 2018;  Chuk, Chan, & Hsiao, 2017; Chuk, Crookes, Hayward,  Chan, & Hsiao, 2017) and higher activations in brain  regions important for top-down visual attention control such  as the frontal eye field and the intraparietal sulcus (Chan,  Wong, Chan, Lee, & Hsiao, 2016). In contrast, holistic  patterns were associated with cognitive decline in older  adults (Chan et al., 2018). Miellet, Caldara, and Schyns  (2011) found that during face viewing, looking at the  nose/face center was associated with global information  processing, whereas looking at the eyes was associated with  local information processing. Accordingly, since analytic  patterns involve looking at both the face center and the eyes,  their advantage in recognition performance may be due to  the use of both global and local information. In other words,  optimal face recognition performance may require both  global and local information processing. Consistent with  this speculation, while global/configural information is  believed to be essential for face processing (e.g., Galton,  1883; Richler, Cheung, & Gauthier, 2011; Tanaka & Farah,  1993), recent studies have suggested the importance of  local/featural information in addition to global information (e.g., Burton, Schweinberger, Jenkins, & Kaufmann, 2015;  Cabeza & Kato, 2000).  
Recent research has suggested that East Asians and West  Caucasians differ in cognitive style: Asians are more likely  to attribute the cause of an event to the context (holistic  cognition), whereas Caucasians are more likely to attribute  the cause of an event to isolated objects (analytic cognition;  e.g., Nisbett & Miyamoto, 2005). This cultural difference is  reflected in their eye movements in scene viewing: Asians  looked at the background more often and are more attuned  to contextual information, whereas Caucasians pay more  
1476
attention to salient foreground objects and are less sensitive  to contexts (e.g., Masuda, Ishii, & Kimura, 2016; Miyamoto,  Nisbett, & Masuda, 2006; Nisbett, Choi, Peng, &  Norenzayan, 2001). Some studies have reported that this  cultural difference could also be observed in eye movements  in face recognition: at the group level, Asians are shown to  predominantly fixate on the face center, whereas Caucasians  fixated mainly on the eyes and the mouth (Blais, Jack,  Scheepers, Fiset, & Caldara, 2008; Caldara, Zhou, &  Miellet, 2010; Kelly, Miellet, & Caldara, 2010; Miellet, He,  Zhou, Lao, & Caldara, 2012). This phenomenon suggests  that during face recognition, Asians may rely more on  global information processing, whereas Caucasians engage  more local information processing. Consistent with this  speculation, when information outside central vision was  restricted, Asians fixated at the eyes and mouth much like  Caucasians, whereas when central vision was masked and  peripheral vision was preserved, Caucasians started to look  at the face center (Caldara, Zhou, & Miellet, 2010; Miellet,  He, Zhou, Lao, & Caldara, 2012). Note however that  regardless of this cultural difference in eye movement  pattern and information use, the two cultural groups did not  differ in face recognition performance (Blais et al., 2008;  Caldara et al., 2010; Miellet et al., 2012). Since previous  studies have suggested that optimal face recognition  performance may involve both global and local information  processing, Asians may benefit from enhancement of local  face processing, whereas Caucasians may benefit from  enhancement of global face processing. 
To examine this possibility, here we used the Navon  stimuli (Navon, 1977) to induce global and local face  processing in Asian and Caucasian participants during face  recognition. Navon stimuli are hierarchical stimuli with a  global figure composed of local components and have been  widely used to prime global or local attention biases (e.g.,  Hübner, 2000; Large & McMullen, 2006; Shedden,  Marsman, Paul, & Nelson, 2003; Ward, 1982). We  predicted that local priming may lead to better face  recognition performance than global priming in Asians, and  this effect may be associated with increased eye fixations to  the eye region. In contrast, Caucasians may have better  recognition performance after global priming than local  priming, and this effect may be associated with increased  eye fixations towards the face center. 
Methods 
Participants 
35 Chinese participants (12 male, mean age 22.1, SD = 4.23)  from the University of Hong Kong and 24 Caucasian  participants (13 male, mean age 24.04, SD = 3.98) from the  University of Auckland were recruited. All had normal or  corrected-to-normal vision and reported right-handed except  for 2 left-handers (1 Asian and 1 Caucasian).  
Materials 
Navon stimuli Sixteen hierarchical letters were created  using letters D, E, F, and H in bold Helvetica font. They  were white in color and presented on a black background.  Each local letter subtended 0.8° × 1.2°visual angle under a  60 cm viewing distance. Each global letter consisted of 13  to 17 local letters and subtended 5.5°×6.7° (Fig. 1). 
  

Figure 1: Samples of Navon letters. 
Face stimuli Images of 120 Chinese faces and 120  Caucasian faces with neutral expressions were used (half  male and half female in each race). External features such as  hair and ears were removed. All faces were grey-scaled with  equal luminance and scaled and aligned with standard eye 
to-eye and eye-to-mouth distances. Each face subtended 6°  × 8°of visual angle. Chinese faces were used for Asian  participants and Caucasian faces were used for Caucasian  Participants. 
Design and Apparatus 
The design consisted of a between-subject variable group  (Asian vs. Caucasian) and a within-subject variable priming level (baseline vs. global vs. local). Stimuli were shown on  a 22’’ monitor with 1024 x 768 resolution. Participants sat  in front of the screen with a chinrest to limit their head  movement. Eye movements were recorded with an Eyelink  1000 eye tracker (sampling rate 1000 Hz). Participants  viewed the stimuli with binocular vision, but only the  dominant eye was tracked. A nine-point calibration  procedure was used before the task. Drift correction was  performed in the beginning of each trial. The calibration  procedure was repeated when drift correction error was  larger than 1° of visual angle. 
Procedure 
Asian and Caucasian participants performed the same face  recognition task with face stimuli of their own race. Each  participant performed three blocks of old-new judgment  task: baseline, global priming, and local priming blocks. In  the baseline block, during the study phase, 20 faces were  shown on the screen one at a time, for 5 s each. Participants  were asked to view and remember the faces. After a 5- minute break, in the test phase, the 20 old faces together  with 20 new faces were presented one at a time in a random  order. The position of each face was randomly assigned to  be either at the upper or lower center of the screen. Participants made old/new judgments using a keyboard.  Each face was presented until response. 
In the global and local priming blocks, participants  performed the same study phase as the baseline block.  Afterwards, instead of having a 5-minute break, participants  
1477
performed a 5-minute Navon task (162 trials) between the  study and test phases1. In each trial, 2 Navon stimuli were  presented simultaneously on the left and right of the screen,  each at 5° of visual angle away from the center. In the  global priming block, participants judged whether the global  form of the stimuli were the same, whereas in the local  priming block, they judged whether the local letters of the  stimuli were the same. Participants made responses through  a keyboard. The test phase procedure was similar to that in  the baseline block, except that in each trial, participants  performed a trial of the Navon task before the presentation  of the face in order to maintain the priming effect. The order  of the 3 blocks was counterbalanced across participants. 
Results 
Performance of the Navon task in the priming blocks  We conducted a 2 x 2 mixed ANOVA with group (Asian vs.  Caucasian) and level (global vs. local) as the independent  variables. In the 5-minute Navon task prior to the test phase,  participants responded faster in matching global forms (M =  634.78 ms, SE = 16.20) than local letters (M = 786.99 ms,  SE = 17.18), F(1, 57) = 80.24, p < .001. In the Navon trials  during the test phase, participants were more accurate, F(1,  57) = 4.974, p = .030, and faster, F(1, 57) = 75.91, p < .001,  in matching global forms (response time: M = 822.08 ms,  SE = 17.09; accuracy: M = 98.81%, SE = 0.23) than local  letters (response time: M = 960.20 ms, SE = 19.40; accuracy:  M = 98.01%, SE = 0.42). No other main effect or interaction  was found (ps > .12). These results reflected the global  precedence effect in visual perception. 
Face recognition performance 
Face recognition performance was measured by d’. A mixed  ANOVA showed a significant interaction between group  and priming level, F(2, 114) = 3.714, p = .027 (Fig. 2). Post hoc comparisons showed that in Asians, local priming led to  better performance than global priming, t(34) = 2.21, p = .025, whereas in Caucasians, a marginal effect indicated  better performance after global priming than local priming, t(24) = -1.95, p = .055. This result was consistent with our  hypothesis: Asians benefited more from enhancement of  local face processing whereas Caucasians benefited more  from enhancement of global face processing. There was no  main effect of group or priming level, and no significant  effect was found in response time (ps > .17). 
  
1 Chuk, Chan, and Hsiao (2017) found that face recognition  performance did not correlate with the similarity between the eye  movement patterns during face learning and recognition, and it was  only correlated with eye movement patterns during recognition but  not learning, suggested that eye movement patterns during  encoding does not play an important role, but the retrieval of  diagnostic information during recognition is essential for  recognition. Giving that test phase play an important role and to exclude the confounding of encoding, we only manipulated the eye  movement during test phase. 
Eye movement pattern analysis 
We used the EMHMM approach (Chuk et al., 2014. See  http://visal.cs.cityu.edu.hk/research/emhmm/ for details) to quantitatively assess eye movement pattern changes due to  priming. Following previous studies (Chuk et al., 2014,  2017; Chan et al., 2018), we used the first three fixations in  each trial in the analysis since these fixations were shown to  be particularly relevant to recognition performance (Chuk et  al., 2017). Chan et al., (2018) identified representative  holistic and analytic eye movement patterns for face  recognition from a large sample across a large age span (34  young and 34 older adults) through clustering using the  EMHMM approach (Fig. 3). The holistic pattern focused at  the face center, whereas the analytic pattern focused at the  eye region in addition to the face center. They assessed the  similarity of an individual’s eye movement pattern to the  representative holistic/analytic pattern as the log-likelihood  of the individual pattern being generated by the HMM of the representative pattern. They then developed the Holistic 
Analytic scale (H-A scale) to quantitatively assess one’s eye  movement pattern along the holistic-analytic dimension: H A scale = (holistic log-likelihood – analytic log-likelihood)  /(|holistic log-likelihood| + |analytic log-likelihood|). Higher  score indicated higher similarity to the holistic pattern. They  found that participants’ H-A scale was correlated with their  cognitive performance. Interestingly, the two representative  models could be used to assess new participants’ H-A scale  of eye movement patterns and show similar correlations.  This result demonstrated the robustness of the representative  models in quantifying one’s eye movement patterns. Since  here we used the same face recognition task and image size  as Chan et al. (2018), we used their two representative 
  
Fig. 2. Face recognition performance 
Fig. 4. H-  A Scale of Asians’ and Caucasians’ eye  movement patterns in face recognition. 
1478
  

Fig. 3. Representative holistic (top) and analytic (bottom)  patterns discovered in Chan et al. (2018) with both young  and older Asian adult participants. The three ellipses on the  large image on the left show the regions of interest (ROIs).  Small images on top show corresponding raw fixations and  fixation heat map respectively. The table shows transition  probabilities among the ROIs; priors indicate the probability  of the first fixation lands on the given ROIs. 
Holistic pattern Analytic pattern 
Representative  
HMMs 
Asians Caucasians 
Baseline 
H-A: -0.009 (0.0035) H-A: -0.021 (0.0038) 
Global  
priming 
H-A: -0.008 (0.0035) H-A: -0.018 (0.0043) 
Local  
priming 
H-A: -0.014 (0.0038) H-A: -0.023 (0.0040) 
Fig.5. Heat maps of Asians’ and Caucasians’ eye  fixations in the baseline, global priming, and local priming  conditions (Mean HA scale score and standard error in each  condition are shown on the bottom). The representative  holistic and analytic eye movement patterns from Chan et al.  (2018) are shown on the top for comparison reasons. 
models to calculate our participants’ H-A scales in different  conditions to better quantify their eye movement patterns  along the analytic-holistic dimension. 
Results of mixed ANOVA on H-A scale showed a  significant main effect of group, F(1, 57) = 42.992, P = .001:  
Caucasians were less holistic than Asians. We also observed  a marginal effect of priming level, F(2, 114) = 2.713, P = .071 (Fig. 4). When we directly compared the global and  local priming conditions in the posthoc analysis,  participants’ H-A scale was significantly higher after global  priming than after local priming, t(58) = 2, p = .01,  suggesting that priming level difference significantly  influenced participants’ eye movement patterns. There was  no interaction between group and priming level, F(2, 114) =  0.241, P = .786, suggesting that the priming tasks had  similar influence on Asians’ and Caucasians’ eye movement  patterns (see Fig. 5 for corresponding group fixation heat  maps for visualization purposes). 
Note that in the baseline condition, Caucasians’ eye  movement patterns were more analytic than Asians, t(57) =  2.313, p = .024 (Fig. 4). However, the two groups did not  differ in recognition performance, t(57) = .153, p = .879 
(Fig. 2). This finding was consistent with the literature (e.g.,  Blais et al., 2008; Kelly et al., 2010). Our results further  showed that for both Caucasians and Asians, global priming  led to more holistic eye movement patterns than local  priming. Nevertheless, global priming led to better  recognition performance in Caucasians, whereas local  priming resulted in better recognition performance in Asians.  These results were consistent with our hypothesis,  suggesting that optimal face recognition performance 
involves a balance between global and local information  processing. 
Discussion 
Here we tested the hypothesis that optimal face recognition  performance involves a balance between global and local  information processing through comparing Asians’ and  Caucasians’ recognition performance and eye movement  pattern changes in response to global and local attention  priming using Navon stimuli. We first showed that without priming, Asians showed more face-center-focused, holistic  eye movement patterns than Caucacians; nevertheless, the  two groups did not differ in recognition performance. This  result was consistent with previous findings in the literature  (e.g., Blais et al., 2008). We then showed that global  priming elicited more face-center-focused, holistic patterns,  whereas local priming elicited more eye-centered, analytic  patterns. Although this effect was consistent among Asians  and Caucasians, local priming led to better face recognition  performance than global priming in Asians, whereas  Caucasians had better recognition performance after global  priming than local priming. This result was consistent with  our hypothesis, suggesting that optimal face recognition  performance involves a balance between global and local  face processing. 
Chuk et al., (2017) examined cultural differences in face  recognition with similar paradigm and eye movement data analysis methods (EMHMM) to the current study. In  contrast to our results, they did not find strong evidence  suggesting cultural difference in eye movement patterns.  
1479
More specifically, they recruited 24 Asian and 24 Caucasian  young adult participants and discovered three representative  eye movement patterns through clustering: holistic, left-eye biased analytic, and right-eye-biased analytic. They found  that the two race groups did not differ either in the log likelihood or in the frequency of adopting the three patterns.  Note however that their representative patterns were directly  discovered from the 48 young adults, whose eye movement  patterns in face recognition are shown to be more eye focused than older adults (Chan et al., 2018). In contrast, the  representative patterns used in the current study were  developed from a larger sample with both young and older adults and captured better the difference between eye focused and face-center-focused eye movement patterns  (Chan et al., 2018). Also, their face images subtended 8°of  visual angle horizontally, larger than the ones used here (6°). Thus, their representative patterns tend to be more eye focused in general as compared with the ones used here (Fig.  6). Indeed, previous studies have shown that image  size/viewing distance is an important factor influencing holistic face processing, as the effect declined sharply at  viewing distances shorter than 2 meters (McKone, 2009;  Ross & Gauthier, 2015). Since the image size used in Chuk  et al. (2017) resembled the size of a real face under a  viewing distance of 1 meter, both Asian and Caucasian  observers might engage less global face processing, and  consequently the cultural difference in eye movement  patterns diminished. Thus, image size/viewing distance may  be an important factor to consider in the examination of  cultural difference in eye movements. 
  
Fig. 6. The representative holistic and analytic eye  movement patterns discovered in (left) Chan et al. (2018),  and (right) Chuk et al. (2017) respectively. 
In the current study, we found that global/local attention  priming using Navon stimuli had similar effects on eye  movement patterns in Asians and Caucasians: Participants’  eye movement patterns were more holistic (face-center 
focused) after global priming and more analytic (eye focused) after local priming. This result is consistent with  Miellet et al. (2011) and Lemieux, Collin and Nelson (2014),  suggesting a strong link between engagement of local/global  attention and eye movements in face recognition.  Nevertheless, regardless of the consistent direction of change in eye movement pattern, Asians and Caucasians  showed contrasting priming effects on recognition  performance: Asians performed better after local priming  than global priming, whereas Caucasians performed better  after global priming than local priming. Since Asians’ eye  
movement patterns were more holistic whereas Caucasians’ were more analytic in the baseline condition, local priming  may have helped Asians to direct attention to the eyes and  global priming helped Caucasians to better process global  information to facilitate recognition. This result suggests  that optimal face recognition performance involves a  balance between global and local information processing,  consistent with recent studies suggesting the importance of both featural and configural information in face recognition 
(Burton et al., 2015; Cabeza & Kato, 2000). This result also  suggests an inverted-U shape relationship between face  recognition performance and eye movement patterns, where  the optimal performance may be observed somewhere  between the two extremes along the holistic-analytic  dimension. Nevertheless, with the current sample, we did  not observe a significant quadratic relationship between  recognition performance and H-A scale of eye movements.  We speculate that the variance in our current sample may be  inadequate to reveal this potential relationship, since all  participants were young adults, whose eye movements tend  to be more analytic than older adults (Chan et al., 2018).  Indeed, the range of H-A scale scores of the older adults in  Chan et al. (2018) was from -.06 to .06, whereas in the  current study with young adults, it was from -.08 to .02.  With this H-A scale score range, we also failed to replicate  the negative correlation between face recognition  performance and H-A scale observed in Chan et al. (2018)  (in the current Asian sample, r(103) = -.030, p = .764). Future work will examine this potential inverted-U shape  relationship between face recognition performance and eye  movement patterns with a larger, more representative  participant sample. 
Note that for both Asians or Caucasians, the priming  procedure adopted here did not significantly improve  recognition performance when compared with the baseline condition. This may be because in the priming blocks  participants had to perform both the Navon and face  recognition tasks, and thus their face recognition  performance was interfered by the Navon task. Thus, it  remains unclear whether it is possible to use attention  priming to improve one’s face recognition performance.  Indeed, recent studies have suggested that adults have  limited plasticity for face recognition ability due to  abundant experience with faces that may have led to the  maximum level of capacity (Tree, Horry, Riley, & Wilmer,  2017). Future work will examine possible attention priming  procedures that may improve face recognition performance. 
In conclusion, here we showed that in face recognition,  global and local attention priming could induce holistic  (face-center-focused) and analytic (eye-focused) eye  movement patterns respectively across cultures, suggesting  a link between eye movement patterns and global/local  information use. Nevertheless, Asians’ face recognition  performance benefited more from local than global attention  priming due to their tendency to adopt a holistic perceptual  style, whereas Caucasians, who were more analytic, showed the opposite effect. These results suggest that optimal face  
1480
recognition performance involves a balance between global  and local information processing through gaze transitions between the face center and the eye region. 
Acknowledgments 
We thank Tim Chuk for help in the EMHMM methods. We  are grateful to the RGC (#17402814 to Hsiao; CityU110513  to Chan). 
References  
Blais, C., Jack, R. E., Scheepers, C., Fiset, D., & Caldara, R.  (2008). Culture shapes how we look at faces. PloS One, 3,  e3022. 
Burton, A. M., Schweinberger, S. R., Jenkins, R., &  Kaufmann, J. M. (2015). Arguments against a configural  processing account of familiar face recognition. Perspect  Psychol Sci, 10, 482–496. 
Cabeza, R., & Kato, T. (2000). Features are Also Important:  Contributions of Featural and Configural Processing to  Face Recognition. Psychol Sci, 11, 429–433. 
Caldara, R., Zhou, X., & Miellet, S. (2010). Putting culture  under the “spotlight” reveals universal information use  for face recognition. PloS One, 5, e9708. 
Chan, C. Y. H., Chan, A. B., Lee, T. M. C., & Hsiao, J. H.  (2018). Eye-movement patterns in face recognition are  associated with cognitive decline in older adults. Psychon  Bull Rev. 
Chan, C. Y. H., Wong, J. J., Chan, A. B., Lee, T. M. C., &  Hsiao, J. H. (2016). Analytic Eye Movement Patterns in  Face Recognition are Associated with Better Performance  and more Top-down Control of Visual Attention – an  fMRI Study, In Proceedings of the 38th Annual  Conference of the Cognitive Science Society.. 
Chuk, T., Chan, A. B., & Hsiao, J. H. (2014).  Understanding eye movements in face recognition using  hidden markov models. J of Vision, 14, 1–14. 
Chuk, T., Chan, A. B., & Hsiao, J. (2017). Is having similar  eye movement patterns during face learning and  recognition beneficial for recognition performance ?  Evidence from hidden Markov modeling. Vision Res, 141,  
204-216. 
Chuk, T., Crookes, K., Hayward, W. G., Chan, A. B., &  Hsiao, J. H. (2017). Hidden Markov model analysis  reveals the advantage of analytic eye movement patterns  in face recognition across cultures. Cognition, 169, 102– 117. 
Galton, F. (1883). Inquiries into human faculty and its  development. Reading. doi:10.1037/10913-000 Hübner, R. (2000). Attention shifting between global and  local target levels: The persistence of level-repetition  effects. Vis Cogn, 7, 465–484. 
Kelly, D. J., Miellet, S., & Caldara, R. (2010). Culture  shapes eye movements for visually homogeneous objects, Front Psychol, 1, 1–7. 
Large, M.-E., & McMullen, P. a. (2006). Hierarchical  attention in discriminating objects at different levels of  specificity. Pecept Psychophys, 68, 845–860. 
Lemieux, C. L., Collin, C. a., & Nelson, E. a. (2014).  Modulations of eye movement patterns by spatial filtering  during the learning and testing phases of an old/new face  recognition task. Attention, Pecept Psychophys, 77, 536– 550. 
Masuda, T., Ishii, K., & Kimura, J. (2016). When Does the  Culturally Dominant Mode of Attention Appear or  Disappear? Comparing Patterns of Eye Movement During  the Visual Flicker Task Between European Canadians  and Japanese. J Cross Cult Psychol, 47, 997–1014. 
McKone, E. (2009). Holistic processing for faces operates  over a wide range of sizes but is strongest at identification  rather than conversational distances. Vision Res, 49, 268– 283. 
Miellet, S., Caldara, R., & Schyns, P. G. (2011). Local  Jekyll and global Hyde: the dual identity of face  identification. Psychol Sci, 22, 1518–26. 
Miellet, S., He, L., Zhou, X., Lao, J., & Caldara, R. (2012).  When East meets West: gaze-contingent Blindspots  abolish cultural diversity in eye movements for faces. J  Eye Mov Res, 5, 1–12. 
Miyamoto, Y., Nisbett, R. E., & Masuda, T. (2006). Culture  and the physical environment holistic versus analytic  perceptual affordances. Psychol Sci, 17, 113–119. 
Navon, D. (1977). Forest before trees: The precedence of  global features in visual perception. Cognitive Psychol, 9,  353–383. 
Nisbett, R. E., Choi, I., Peng, K., & Norenzayan, A. (2001).  Culture and systems of thought: Holistic versus analytic  cognition. Psychol Rev, 108, 291–310. 
Nisbett, R. E., & Miyamoto, Y. (2005). The influence of  culture: Holistic versus analytic perception. Trends in  Cogn Sci, 9, 467–473. 
Peterson, M. F., & Eckstein, M. P. (2013). Individual  differences in eye movements during face identification  reflect observer-specific optimal points of fixation.  Psychol Sci,24, 1216–25. 
Richler, J. J., Cheung, O. S., & Gauthier, I. (2011). Holistic  processing predicts face recognition. Psychol Sci, 22,  464–471. 
Ross, D. A., & Gauthier, I. (2015). Holistic processing in  the composite task depends on face size. Visual Cognition,  23, 533–545. 
Shedden, J. M., Marsman, I. A., Paul, M. P., & Nelson, A.  (2003). Attention switching between global and local  elements: Distractor category and the level repetition  effect. Vis Cogn, 10, 433–470. 
Tanaka, J. W., & Farah, M. J. (1993). Parts and wholes in  face recognition. Q J Exp Psychol A, 46, 225–245. Tree, J. J., Horry, R., Riley, H., & Wilmer, J. B. (2017). Are  portrait artists superior face recognizers? Limited impact  of adult experience on face recognition ability. J Exp  Psychol Hum Percept Perform, 43, 667–676. 
Ward, L. M. (1982). Determinants of attention to local and  global features of visual forms. J Exp Psychol Hum  Percept Perform, 8, 562–581. 
1481
Improving predictions of polite and frustrated speech using linguistic features associated with different cognitive states in children 
Cindy Chiang (cindyc@usc.edu) 
Department of Psychology, 3620 South McClintock Ave. 
Los Angeles, CA 90089 
Jacqueline Brixey (brixey@usc.edu) 
USC Institute for Creative Technologies 
12015 E Waterfront Dr, Los Angeles, CA 90094 
James Gibson (jjgibson@usc.edu) 
Viterbi School of Engineering, 3650 McClintock Ave. 
Los Angeles, CA 90089 
Morteza Deghani (mdehghan@usc.edu) 
Department of Psychology, 3620 South McClintock Ave. 
Los Angeles, CA 90089 
Abstract 
Childrens poor emotional self-regulation is associated with poor mental health outcomes. This study presents methods that improve prediction rates of polite and frustrated speech using linguistic cues. These improvements can be used to help auto matically identify characteristics of poor self-regulation in fu ture studies. This work adds to previous research by consider ing existing computer science, psychology, and psycholinguis tics methodologies and findings. More specifically, features associated with childrens cognitive control capacities across age groups are considered to investigate acoustic, semantic, and syntactic features in speech. The current analyses indi cate that the features most predictive for polite and frustrated speech differ, a combination of features work best for predict ing both speech types, and the predictive quality of features do not vary substantially by age. Further work should be con ducted to clarify how well these findings transfer to general and clinical populations as well as to consider the developmental norms of different age groups. 
Keywords: self-regulation; linguistic features; machine learn ing 
Introduction 
Approximately 13 percent of children and adolescents have been estimated to have clinically significant mental health problems that impair daily life functioning (Jellinek et al., 1999; Semansky, Koyanagi, & Vandivort-Warren, 2003). Many of these mental health problems have been linked to poor emotional self-regulation (Forbes & Dahl, 2005; Hin shaw, 2002; Kuntsche, Knibbe, Engels, & Gmel, 2007; Wyman et al., 2009) and difficulty with regulating emotion during higher levels of distress. Interventions developed to target self-regulation (Wyman et al., 2010) have been shown to be effective in decreasing rates of problematic behavior in schools, and in improving some aspects of functioning in the classroom (Wyman et al., 2010). 
Despite the large number of youths affected and the effi cacy of these targeted interventions, there is not an automated way to identify children with poor emotional self-regulation. To take initial steps towards developing such a method, this 
paper works to identify methods of improving prediction of polite and frustrated speech using linguistic features present in child speech. 
Several linguistic features have been used to identify emo tional states in adults, specifically prosodic and semantic fea tures. These features were linked to various psychological and emotional states and were used to automatically catego rize these states. Features akin to semantic cues have been used, and word count methods, such as Linguistic Inquiry and Word Count (LIWC) (Pennebaker, Boyd, Jordan, & Black burn, 2015), linked emotional states to word distributions in a range of categories. Researchers participating in the annual Interspeech Challenge have also sought to determine acous tic features related to emotional speech (Schuller, Steidl, & Batliner, 2009). 
Additionally, some linguistic features have been used to identify polite or frustrated speech. For example, polite speech class prediction performance improved through a fu sion of acoustic, lexical, and contextual features for chil dren’s speech (Yildirim, Narayanan, & Potamianos, 2011). Boril, Sadjadi, Kleinschmidt,and Hansen (2010) used results of tasks measuring cognitive load to improve prediction rates of frustrated speech in drivers. The study improved predic tion rates of frustrated speech using subjects performance on cognitive tests and the acoustic features of their speech. 
However, literature in psychology indicated that there are additional linguistic features that differentiate polite and frus trated speech. These features have also been observed to change through development and cognitive load. Develop mental changes in language included the comprehension and production of more complex sentences (Gaer, 1969). Cog nitive changes across development included a larger capacity to overcome cognitive load difficulties (Hsu & Jaeggi, 2013). The potential impacts of these changes in the linguistic cues of both polite and frustrated speech are further detailed below. In both an observational (Gleason, Perlmann, & Greif, 
1482
1984) and experimental study, Greif and Gleason (1980) found that children use polite speech in structured and for mulaic manners. Polite speech was also found to be couched in routine, often prompted by parents, and reinforced by par ents (Gleason et al., 1984). As a result, even when polite speech was deliberately elicited in children aged two to five, the frequency of polite speech was very low (Greif & Glea son, 1980). Two-year-olds in this study, thought to be too young to even understand the nuances of polite speech, still produced polite speech. The researchers of both studies at tributed this phenomenon to the rote and formulaic nature of speech. The frequency in these studies illustrated the link be tween the directedness of polite speech and its frequency in adult speech. The authors in both studies hypothesized that these differences were also linked to socioeconomic status and parenting styles, as these types of speech were reflective of input and directions from parents, rather than developmen tal factors. 
Developmental differences in these areas may not have been as evident, since children as young as two formu laically produced polite speech. The scripted nature of polite speech should have impacted the semantic and syntactic cues present. As such, these studies indicated that there were lit tle variability in the types of words, contexts, and language structures that were utilized when producing polite speech. Changes in cognitive load similarly should not influence po lite speech. 
Linguistic cues in frustrated speech, in contrast, have been found to influence cognitive load. Boril and colleagues (Boril et al., 2010) improved rates of categorizing frustrated speech in adults when cognitive factors and acoustic cues were con sidered. Cognitive factors should similarly influence chil drens frustrated speech. 
Previous literature dealing with cognitive factors influ ence on speech found that poor cognitive control influenced peoples ability to accurately interpret complex sentences (MacDonald, Just, & Carpenter, 1992) and impacted lexical associations (Boudewyn, Long, & Swaab, 2012). These fac tors could be further modulated by changes in cognitive de velopment, as some aspects of cognitive development con tinue past young childhood (Munakata, Snyder, & Chatham, 2012). 
The current study looks to improve prediction processes of polite and frustrated speech by considering the existing literature in computer science and psychology. Based on the reviewed literature, several factors could improve predic tion processes of polite and frustrated speech: using a subset of linguistic features, using combined linguistic characteris tics, and using linguistic features known to co-occur with the cognitive load children experience while calm or frustrated. These pieces are addressed in the experimental methods out lined below. 
Methods 
Corpus 
The Children’s Interactive Multimedia Project (ChIMP) database (Narayanan & Potamianos, 2002) was utilized for this work. ChIMP is a corpus of child-machine spoken dia logues in a Wizard-of-Oz game setting. Participants played ”Where in the USA is Carmen Sandiego?” and located a car toon criminal by communicating commands to game agents. Approximately 100 subjects, both male and female, between the ages of 7 and 14 participated (Table 1). Subjects formed three age groups: 7-9 years old (young), 10-11 y/o (middle), and 12-14 y/o (old). 
Table 1: Distribution of subjects and number of utterances for each emotional class (neutral, polite, frustrated) for each gender-age group. 
Group N Neutral Polite Frustrated Total 7-9 y/o 38 3966 977 796 5739 10-11y/o 35 4004 1078 360 5442 12-14 y/o 30 3005 694 705 4404 Female 48 5035 1513 800 7438 Male 55 5940 1236 1061 8237 Total 103 10975 2749 1861 15585 
The recorded spontaneous utterances were manually la beled with an emotional tag: polite, neutral, or frustrated (Ta ble 1). The corpus contained over 15,000 labeled utterances, with approximately 700 unique words. The data set showed notable variation in age and gender behaviors. The middle group was more polite and less frustrated than the other two age groups during the game. The younger and older groups were nearly twice as frustrated as the middle age group. The frustration age trend was partially driven by subjects’ exac erbation with the game’s level of challenge or ease. Addi tionally, frustrated expressions occurred more often in losing games than in winning instances. By gender, girls were more polite and less frustrated overall in their interactions during the game than boys (Arunachalam, Gould, Andersen, Byrd, & Narayanan, 2001). 
ChIMP was used previously to investigate polite and frus trated speech in children. Prior work utilized latent semantic analysis (LSA) for discourse topics and explored emotional salience in lexical features to predict polite and frustrated speech (Yildirim et al., 2011). Variations of this set have been used to improve uncertainty predictions and to hone ways to improve machine coding validity (Black, Chang, & Narayanan, 2008). This work expands on prior research by exploring LIWC categories, part-of-speech (POS), and word embeddings (WEs) as features to predict cognitive mecha nisms. 
1483
Extracted Features 
Feature extraction was motivated by the analysis of the child machine interaction dialogues from ChIMP. Thus, acoustic, lexical, and syntactic features are proposed. 
Acoustic 384 low-level descriptors (LLD) - such as such as pitch frequency, formant frequency, root mean square (RMS) energy, and zero-crossing-rate (ZCR) - were extracted using openSMILE (Eyben, Wollmer, & Schuller, 2009). These ex- ¨ traction measures build upon the work of (Yildirim et al., 2011), and were combined with new lexical and syntactic fea tures for analyses, described below. 
Lexical Two separate features measure lexical variation - LIWC and word embeddings (WEs). LIWC version 2007 was used to generate the LIWC feature set. LIWC provides in formation about an utterance’s psychological dimension and will measure semantic word choice variation. All LIWC cat egories were considered, and Pearson’s correlation was cal culated to determine which categories have more predictive power for determining polite and frustrated speech. 
WEs were mappings of words in the vocabulary of the data set to vectors of real numbers. This feature captured mean ing, semantic relationships, and context for words in ChIMP. Thus, vocabulary in frustrated utterances were represented in a feature space that were separate from polite utterances. 
Syntactic Part of speech (POS) tags were gen erated by the Stanford Part of Speech Tagger (https://nlp.stanford.edu/software/tagger.shtml). We hy pothesized that variation in POS should occur as a product of cognitive control. Hence, decreased complexity, repre sented as shallower trees, will align with the childs level of frustration. 
Analysis 
To determine how cognitive and developmental measures cor respond to politeness and frustration expression by children, we conducted three analyses. 
Analysis 1 We conducted five-fold cross validation exper iments to compare feature sets. Two machine learning tech niques were used: feed forward neural network (FFNN) for utterance level feature sets (acoustic and LIWC features); long short term memory (LSTM) for sequence based features (WEs and syntactic features). As a previous study has pre viously explored a Bayesian classifier to predict polite and frustrated speech (Yildirim et al., 2011), we expand on those findings by implementing two new state-of-the-art models in our study. Five speaker-independent cross validation folds, approximately balanced across age and gender, were created. 
Each neural network was implemented and trained using Keras (Chollet et al., 2015) with Theano (Theano Develop ment Team, 2016) as the back-end. All the systems were trained using categorical cross-entropy loss and optimized using the Adam algorithm (Kingma & Ba, 2014). The loss function was weighted by class according to the inverse fre 
quency of each class (to account for class imbalance). Within each fold the utterances of approximately 10% of the speak ers were separated as a validation set. They were trained for a maximum of 30 epochs with an early stopping strategy to terminate if the validation loss did not decrease after three consecutive epochs, and only the model with the lowest vali dation loss was retained. Each model was trained in 10 trials using different random initializations and the reported results were averaged across these trials. 
The feed forward networks consisted of two hidden layers where the first layer was of equal dimension to the input fea ture vector and the second hidden layer dimension was 10% of the first layers’. Both hidden layers had sigmoid activation. The LSTM networks consisted of an embedding layer at the input followed by a bidirectional layer of dimension 50 for the word embedding features and 26 for the POS sequences (the number of unique POS tags). All the networks had a softmax activation at the output layer. 
Analysis 2 We used decision level fusion to combine the feature sets to compare the power of multimodality in pre dicting emotional states. We used the average fusion algo rithm (Yildirim et al., 2011) to combine the computed pos terior probabilities of each single feature set classifier in or der to estimate the posterior probability of a combined classi fier. We hypothesized that the fusion of acoustic, lexical, and syntactic would provide more predictive power than single feature models alone. 
Analysis 3 We executed training on one age group and test ing as well as experiments on the others for age specific per formance on politeness and frustration. We expected that the expression of politeness was age independent, whereas frus tration was age dependent. 
Results 
We presented results to detect frustrated and polite attitudes in childrens speech using the selected features for three-way classification tasks: single feature evaluation, fused features evaluation, and age specific performance. 
Analysis 1 
The results for this analysis are shown in the solid bars to the left in Figure 1. For single-feature classifications, WEs pro vided the best predictive power, while acoustic features dis played the worst performance. To understand how well each feature predicted the three emotional classes, F1 scores were determined for each emotion class (Figure 2). It is clear that acoustic features performed poorly overall due to the poor predictive power for neutral and polite classes, despite be ing the best at classifying frustrated utterances. Both LIWC and POS exhibited good predictive power for neutral and po lite speech, but performed poorly for frustrated. The mod els trained on WEs features were the most successful, but performed the best at correctly classifying polite utterances. Overall, the features tended to be more predictive of polite speech. 
1484
  

Figure 1: Results from Experiments 1 and 2. Word embed dings (WE) performed the best as a single feature (experiment 1) while the fusion of all features produced the highest pre diction rate overall (experiment 2). 
  

Figure 2: F1 scores for emotional states for each features from Experiment 1. 
Analysis 2 
The results for fusing the selected features are shown in the striped bars to the right in Figure 1. The majority of the fused features were more successful at predicting than the single-feature classification, with the combination of all the features performing the best (Unweighted average recall of 63.4%) of all the models. Both the WEs+POS and Acous tic+LIWC+WEs+POS fused feature models showed a signifi cant improvement with respect to the best single system result in Analysis 1 (p<0.01). 
Analysis 3 
We conducted experiments where each age-group was used for training and the system was tested on the remaining age groups to determine age specific prediction for each feature, summarized in Table 2. In general, systems trained on the middle group were the least successful at predicting frustra 
tion, but the inverse trend at predicting politeness. The results indicated that systems trained on WEs for the middle group produce the best scores for politeness, while systems trained on either the acoustic or WEs features for the young or old groups are able to predict frustration the best. This seemed to be reflective of the large difference in the politeness and frustration distributions for the middle group versus the other two groups rather than reflecting age-related differences. 
Table 2: Leave-one-out train versus test F1 score by age group for each feature for politeness and frustration emo tional states. 
Emotion Age group Young Middle Old Acoustic 
Politeness Young - 51.2 37.6 Middle 51.0 - 37.3 
Old 36.5 40.2 - 
Frustration Young - 19.0 34.2 Middle 31.0 - 32.1 
Old 32.6 16.3 - 
LIWC 
Politeness Young - 66.8 61.2 Middle 64.2 - 57.4 
Old 61.1 70.0 - 
Frustration Young - 11.6 31.5 Middle 28.7 - 28.6 
Old 28.3 14.9 - 
WE 
Politeness Young - 75.8 62.7 Middle 66.6 - 59.9 
Old 65.6 71.5 - 
Frustration Young - 15.7 32.9 Middle 30.0 - 29.1 
Old 34.6 15.5 - 
Part of Speech 
Politeness Young - 56.4 52.2 Middle 60.8 - 52.9 
Old 53.3 53.0 - 
Frustration Young - 9.9 32.0 Middle 10.6 - 7.3 
Old 29.7 12.3 - 
Feature analysis 
Finally, we analyzed acoustic and LIWC features. First, we calculated Pearson correlations to determine the top features with predictive power with respect to politeness and frustra tion (Table 3). 
The top LLD values for the acoustic feature set showed positive correlations with frustration, and negative correla tions with politeness. The features positively correlated with frustration deal with signal frame energy, while the features most correlated with politeness relate to zero-crossing rate of time signal. 
1485
Table 3: Pearson’s correlation (rho) for acoustic and LIWC top features. 
Feature rho 
Acoustic politeness zcr linregerrQ -0.204 zcr stddev -0.199 
RMSenergy de minPos -0.198 
Acoustic frustration RMSenergy amean 0.165 RMSenergy stddev 0.157 
RMSenergy range 0.147 
LIWC politeness you 0.432 posemo 0.402 
affect 0.393 
LIWC frustration inhib 0.123 i 0.118 
verb -0.078 
For LIWC features, politeness showed negative correlation with affective processes (e.g. affect). The correlations illus trated that the LIWC category inhibition (e.g. inhib) was pos itively correlated with frustration. 
Correlations for age groups were next reviewed for both features, which can be seen in Table 4. For acoustic fea tures, the top acoustic features were consistent across age for frustration but not politeness. So, frustration was more con sistent in its acoustic expression across age groups whereas politeness was not. The top LIWC features were consistent across age for politeness but not frustration. So, expression of politeness was more uniform across age with respect to language whereas expression of frustration through language varies more across age groups. 
Discussion 
The experiments in this work produced several interesting findings. First, WEs was the best predictor when looking at the single-feature systems. Second, the best single fea ture predictor differed across types of speech, as LIWC and WEs were more successful when predicting polite speech, while acoustic features were more successful with predicting frustrated speech. Third, training the models by age group yielded differing levels of success. Fourth, several features correlated well with polite or frustrated speech. Overall, it ap peared that several factors influenced predictiveness the most, mainly semantic features and age groups. 
The overall contribution of the WEs could be attributed to its success in predicting polite speech and the number of polite speech in the corpus, as these represented double the number of frustrated utterances. It is possible that WEs and LIWC categories were the most successful as a result of the formulaic nature of polite speech (e.g. “thank you”). The re sult that “you” was the LIWC category most correlated with polite speech supported this possibility. Additionally, there were negative correlations with cognitive mechanism words, which could be attributed to the scripted rather than engaged 
Table 4: Correlations for utterance level features with polite ness and frustration for each age group. 
Politeness 
Acoustic 
Young RMSenergy de minPos (-0.212), 
Middle zcr stddev (-0.222), 
Old fftMag mfcc[1] linregerrQ (-0.203) LIWC 
Young you (0.478), 
Middle you (0.462), 
Old you (0.328), 
Frustration 
Acoustic 
Young RMSenergy amean (0.200), 
Middle RMSenergy amean (0.113), 
Old RMSenergy amean (0.178), 
LIWC 
Young inhib (0.202), 
Middle social (0.097), 
Old i (0.149), 
and thoughtful speech. 
In contrast, acoustic cues were most predictive of frustrated speech. It may be that semantic cues were not reliable and that there was a lot of variability within the words used. Pre vious research found that poor cognitive control was associ ated with different levels of sensitivity to lexical associations (Boudewyn et al., 2012). Participants who performed poorly on cognitive tests, particularly those with difficulty on the suppression tasks, in the study were more sensitive to lexical associations. It was possible that sensitivity to lexical associ ations produced speech that was less characteristic with some of the categories within the LIWC dictionary. An alternative hypothesis is that frustrated speech was more variable, irre spective of the lexical associations that children might make when frustrated. Nozari, Freund, Breining, Rapp, and Gor don (2016) described the use of cognitive control on different stages of language production, one of which required moni toring and revising word choice errors. 
Across age groups and speech categories, there were some differences in the predictive strength for certain features. In polite speech, LIWC and WEs were very good in training classification models across time groups. These features did not change across time. It may be the case that polite speech in this corpus did vary across age groups. Studies by Gleason, Perlmann, and Greif (1984) and Greif and Gleason (1980) found this trend in their studies and attributed it to the scripted nature and acquisition of the speech. 
In frustrated speech, the F-1 scores of the middle age group in all linguistic feature categories were lower than the other two age groups. This might result from the smaller number of frustrated utterances in this age group. 
While these findings conform to the trends reported by pre 
1486
vious literature, the age groups investigated in the current study was different. Previous studies have generally inves tigated children in a younger age group (e.g. two to five years old). There may be additional features that have not been cap tured by the literature and factors that were not considered by the classification experiments conducted in this study. It will be important to further consider the developmental norms of older age groups, especially if such classification models are used in clinical and practical settings. 
References 
Arunachalam, S., Gould, D., Andersen, E., Byrd, D., & Narayanan, S. (2001). Politeness and frustration language in child-machine interactions. In Seventh european confer ence on speech communication and technology. 
Black, M., Chang, J., & Narayanan, S. (2008). An empir ical analysis of user uncertainty in problem-solving child machine interactions. In First workshop on child, computer and interaction. 
Boril, H., Omid Sadjadi, S., Kleinschmidt, T., & Hansen, J. H. (2010). Analysis and detection of cognitive load and frustration in drivers’ speech. Proceedings of INTER SPEECH 2010, 502–505. 
Boudewyn, M. A., Long, D. L., & Swaab, T. Y. (2012). Cognitive control influences the use of meaning relations during spoken sentence comprehension. Neuropsycholo gia, 50(11), 2659–2668. 
Chollet, F., et al. (2015). Keras. https://github.com/fchollet/keras. GitHub. Eyben, F., Wollmer, M., & Schuller, B. (2009). Openearintro- ¨ ducing the munich open-source emotion and affect recog nition toolkit. In Affective computing and intelligent inter action and workshops, 2009. acii 2009. 3rd international conference on (pp. 1–6). 
Forbes, E. E., & Dahl, R. E. (2005). Neural systems of posi tive affect: relevance to understanding child and adolescent depression? Development and psychopathology, 17(3), 827–850. 
Gaer, E. P. (1969). Children’s understanding and produc tion of sentences. Journal of Verbal Learning and Verbal Behavior, 8(2), 289–294. 
Gleason, J. B., Perlmann, R. Y., & Greif, E. B. (1984). What’s the magic word: Learning language through politeness rou tines. Discourse Processes, 7(4), 493–502. 
Greif, E. B., & Gleason, J. B. (1980). Hi, thanks, and good bye: More routine information. Language in Society, 9(2), 159–166. 
Hinshaw, S. P. (2002). Process, mechanism, and expla nation related to externalizing behavior in developmental psychopathology. Journal of Abnormal Child Psychology, 30(5), 431–446. 
Hsu, N. S., & Jaeggi, S. M. (2013). The emergence of cogni tive control abilities in childhood. In The neurobiology of childhood (pp. 149–166). Springer. 
Jellinek, M. S., Murphy, J. M., Little, M., Pagano, M. E., Comer, D. M., & Kelleher, K. J. (1999). Use of the pe diatric symptom checklist to screen for psychosocial prob lems in pediatric primary care: a national feasibility study. Archives of Pediatrics & Adolescent Medicine, 153(3), 254–260. 
Kingma, D. P., & Ba, J. (2014, December). Adam: A Method for Stochastic Optimization. ArXiv e-prints. 
Kuntsche, E., Knibbe, R., Engels, R., & Gmel, G. (2007). Drinking motives as mediators of the link between alcohol expectancies and alcohol use among adolescents. Journal of Studies on Alcohol and Drugs, 68(1), 76–85. 
MacDonald, M. C., Just, M. A., & Carpenter, P. A. (1992). Working memory constraints on the processing of syntactic ambiguity. Cognitive psychology, 24(1), 56–98. 
Munakata, Y., Snyder, H. R., & Chatham, C. H. (2012). De veloping cognitive control: Three key transitions. Current directions in psychological science, 21(2), 71–77. 
Narayanan, S., & Potamianos, A. (2002). Creating conversa tional interfaces for children. IEEE Transactions on Speech and Audio Processing, 10(2), 65–78. 
Nozari, N., Freund, M., Breining, B., Rapp, B., & Gordon, B. (2016). Cognitive control during selection and repair in word production. Language, cognition and neuroscience, 31(7), 886–903. 
Pennebaker, J. W., Boyd, R. L., Jordan, K., & Blackburn, K. (2015). The development and psychometric properties of liwc2015 (Tech. Rep.). 
Schuller, B., Steidl, S., & Batliner, A. (2009). The inter speech 2009 emotion challenge. 
Semansky, R. M., Koyanagi, C., & Vandivort-Warren, R. (2003). Behavioral health screening policies in medicaid programs nationwide. Psychiatric Services, 54(5), 736– 739. 
Theano Development Team. (2016, May). Theano: A Python framework for fast computation of mathematical expres sions. arXiv e-prints, abs/1605.02688. Retrieved from http://arxiv.org/abs/1605.02688 
Wyman, P. A., Cross, W., Brown, C. H., Yu, Q., Tu, X., & Eberly, S. (2010). Intervention to strengthen emotional self-regulation in children with emerging mental health problems: Proximal impact on school behavior. Journal of abnormal child psychology, 38(5), 707–720. 
Wyman, P. A., Gaudieri, P. A., Schmeelk-Cone, K., Cross, W., Brown, C. H., Sworts, L., . . . Nathan, J. (2009). Emo tional triggers and psychopathology associated with sui cidal ideation in urban children with elevated aggressive disruptive behavior. Journal of abnormal child psychology, 37(7), 917–928. 
Yildirim, S., Narayanan, S., & Potamianos, A. (2011). De tecting emotional state of a child in a conversational com puter game. Computer Speech & Language, 25(1), 29–44. 
1487
Shaping Perceptions by Hand: The Influence of Motor Fluency on Face Judgment  
Julia Chirls (jchirls@fandm.edu) 
Department of Psychology, Franklin & Marshall College 
415 Harrisburg Ave, Lancaster, PA, 17603, USA 
Maddy Kaplan (mkaplan1@fandm.edu) 
Department of Psychology, Franklin & Marshall College 
415 Harrisburg Ave, Lancaster, PA, 17603, USA 
Yosan Gebre-Ab (ygebreab@fandm.edu) 
Department of Psychology, Franklin & Marshall College 
415 Harrisburg Ave, Lancaster, PA, 17603, USA 
Mia Ortiz (mortiz@fandm.edu) 
Department of Psychology, Franklin & Marshall College 
415 Harrisburg Ave, Lancaster, PA, 17603, USA 
Lauren H. Howard (lauren.howard@fandm.edu) 
Department of Psychology, Franklin & Marshall College 
415 Harrisburg Ave, Lancaster, PA, 17603, USA 
Abstract 
Research has shown that individual variation in our bodies,  such as differential hand dominance, can influence the way that  we interact with and perceive the world (Casasanto, 2009). For  example, right-handed individuals are more likely to associate  their right spatial plane as more positive than their left, an effect  that is switched in left-handed individuals. Here, we explored  whether asking participants to use their dominant (“good”)  versus nondominant (“bad”) hand on a motor task influenced  subsequent valanced face judgment. Results demonstrate that  simply asking a participant to use their right or left hand to  complete a task can have a significant effect on the perceived  valence of neutral faces. These findings add to the evidence  that the way we physically interact with our world may have  important consequences for our perceptions of social stimuli. 
Keywords: embodiment; handedness; body specificity; social  cognition; face judgment 
Introduction 
In our everyday life, we interact with physical objects in ways  that both create and shape our perception of the world. Often  referred to as “embodied cognition” (see Goldinger, Papesh,  Barnhart, Hansen, & Hout, 2016 for review), the study of the  relationship between our bodies, the environment, and  thought has demonstrated remarkable interconnections  between these systems. More recently, the “body specificity  hypothesis” has focused on the nuanced ways that natural  variation in our bodies (e.g., handedness) can influence our  cognitive processes (e.g., Casasanto, 2009; 2014). In the  current series of studies, we ask whether the use of one's  
dominant or nondominant hand on a motor task impacts later  social perception. 
According to the body specificity hypothesis, people  implicitly associate emotional valence (positive or negative)  with certain spatial planes, an association that is mediated by  motor fluency (Casasanto, 2009; Casasanto, 2014). For  example, right-handed individuals associate the right spatial  plane with positivity or goodness, and the left spatial plane  with negativity or badness. This association is reversed in  left-handed individuals, who associate left with positive and  right with negative (Casasanto, 2009). Such a systematic  difference across right vs. left handed individuals likely  results from systematically different perceptuomotor  experience, involving extended interaction with the dominant  side of the body. Over time, one side becomes the easiest to  use, increasing motor fluency and creating an association  with more positive thoughts (Oppenheimer, 2008). 
While valenced associations have been found across a  number of modalities (e.g., sound; McFarland & Kennison,  1989) and body parts (e.g., feet; de la Vega, Graebe, Härtner,  Dudschig, & Kaup, 2015), the most common focus in the  body hypothesis literature is on the connection between  handedness and emotions. For example, research has shown  that individuals prefer and rate objects more positively that  are presented on the side aligning with their dominant hand  (Casasanto, 2009), are quicker to react to positive vs.  negative stimuli using their dominant hand (de la Vega, De  Filippis, Lachmair, Dudschig, & Kaup, 2012), and are more  likely to use their dominant hand when gesturing about ideas  construed as positive (Casasanto & Jasmin, 2010). Similarly,  participants rate words typed on the right side of a QWERTY  keyboard as more positive than those on the left side, even if  
1488
those words are not used in the English language (Jasmin &  Casasanto, 2012). This suggests that, from a correlational  perspective, the dominant side of the body is more strongly  associated with positive conceptualizations. 
The body specificity hypothesis is further supported by  intervention studies, where a previously dominant or ‘fluid’  hand is handicapped and subsequent cognitive effects are  assessed. For example, Casasanto and Chrysikou (2011)  found that the right-positive association in naturally right 
handed individuals flipped when participants were asked to  wear a bulky ski glove over their dominant hand while  completing a fine-motor task. These effects were similar to  those found in stroke patients, who had previously utilized a  dominant hand that they could no longer control. In fact, even  imagining wearing a ski glove while completing a motor task  is enough to elicit valenced reactions in line with a real  handicap (de la Fuente, Casasanto, & Santiago, 2015).  Therefore, both real and visualized motor experiences can  have a marked effect on later perceptions. 
While the previous literature has made a strong argument  for a connection between handedness and general valenced  appraisal, the influence of such an effect on social cognition  is still generally unknown. Studies have focused primarily on  how participants rate the valence of unrelated words (Jasmin  & Casasanto, 2012), sort objects/pictures (e.g., Casasanto,  2009), or naturalistically gesture in accordance with their  dominant versus nondominant planes (Casasanto & Jasmin,  2010). However, few studies to date have explored whether  the hand we use influences the way we perceive social  partnersin our world, particularly in relation to the perception  of emotional expression.  
As may be evident, it is crucial for humans to be able to  effectively detect the emotions of those around them.  Emotional facial expressions provide a wealth of social  information, including but not limited to communicative  intentions (see Russell & Fernández-Dols, 1997) and social  rejection or acceptance (de Gelder, 2009). In fact, failure to  adequately identify facial emotions is often a hallmark of  those with other social deficits, such as Autism spectrum  disorder (e.g., Loth et al., 2018) or Borderline personality  disorder (e.g., Meyer, Pilkonis, & Beevers, 2004),  highlighting the adaptive importance of this ability. 
Even within normative populations, emotional perception  appears particularly influenced by context and priming. For  example, pairing neutral face pictures with either positive or  negative sentences (Wieser et al., 2014) or positive or  negative backgrounds (Lee, Choi, & Cho, 2012) influences  the valence reports of participants. Therefore, if handedness  can influence the perceived valence of neutral words, there is  reason to believe it would also influence the perceived  valence of neutral faces. 
 Some previous research has suggested a connection  between handedness and certain social judgments in-the moment. For example, individuals are more likely to place  highly vs. weakly valenced faces on the extreme ends of a  continuous horizontal line (Freddi, Brouillet, Cretenet,  Heurley, & Dru, 2016), with the location of these  
placements (left or right) corresponding to both participant  handedness and stimulus valence. Similarly, right-handed  individuals are faster to respond to positive socio-emotional  stimuli with their right hands (Kong, 2013), tend to prefer  social partners presented on their right side (Zhao et al.,  2016), and are more likely to rate faces presented on their  right side as positive vs. negative (Brookshire & Casasanto,  2013), the reverse of which is true for left-handed  participants. Together, this body of work suggests a clear  connection between handedness, locations in space (right  versus left), and valenced social judgments. However, it  fails to explore whether dominant vs. non-dominant hand  use could later influence neutral face judgments regardless  of their spatial presentation. 
 The current study builds upon the literature on motor  fluency, handedness, and social cognition to explore the  connection between dominant (vs. nondominant) hand use  and face judgment. In Study 1, participants completed a  timed jigsaw puzzle task using either their dominant or  nondominant hand, before rating the emotional valence of  facial stimuli. In Study 2, participants completed a timed  domino placing task with their dominant or nondominant  hand before completing the same face judgment task.  Across both studies, it was hypothesized that participants  who used their dominant hand would perceive the neutral  face pictures as more positive compared to those who used  their nondominant hand.  
Study 1 
Participants 
Data from 46 participants between the ages of 17 and 21 years  (M = 19.22, 11 males, 35 females) were analyzed for Study  1. Participants were recruited via introductory psychology  courses at a liberal arts college. Students voluntarily signed  up using the online SONA Systems software (Fidler, 2002),  receiving class credits for participation. Participants were  randomly assigned to one of two conditions: the dominant  hand condition or nondominant hand condition. All  participants consented to the study and were right-handed  according to self-report.  
Materials & Procedure 
Each participant completed two tasks during the experiment  in the following order: a puzzle completion task and a facial  expression rating task.  
During the puzzle task, participants were asked to complete  one of two 25-piece children’s jigsaw puzzles (one depicting  the four seasons, one depicting various occupations, see  Figure 1) while seated at a table. Each participant was  randomly assigned to either the dominant (right, N = 22) or  nondominant (left, N = 24) hand condition, and was  instructed to complete as much of the puzzle as possible in  one minute using only their assigned hand. Participants were  video recorded with a handheld Canon Vixia HFR 72 video  camera mounted on a tripod, and pointed at the table,  recording only the participant’s hands and the puzzle. After  
1489
one minute, the experimenter asked the participant to stop,  even if all of the puzzle pieces were not placed.  
  

Figure 1. Stimuli from Study 1 including a) a photo of the 25-piece  puzzle used in the puzzle task and b) an example of a neutral face  emotion picture and associated Likert scale.  
After the puzzle task, participants completed a face  judgment task. During this phase, participants viewed a series  of 20 randomly presented faces on a laptop computer, one by  one (see Figure 1). The face stimuli (2.5 by 2.5 inches) were  presented on a white background and were taken from the  Chicago Face Database (Ma, Correll, & Wittenbrink, 2015).  Individuals in the database wore grey shirts, and photos were  taken from the shoulders up. Faces varied according to both  race and gender. Participants viewed 8 neutral faces (4 white,  4 African American), 6 happy faces (3 white, 3 African  American), and 6 unhappy faces (3 white, 3 African  American). Half of the faces were female (10) and half were  male (10). The happy and unhappy stimuli were used as  distractors, thus necessitating fewer of these faces in the set.  All stimuli and responses were presented with Qualtrics  research software (Qualtrics, Provo, UT).  
After viewing each picture, participants were asked to  verbally rate how positive or negative the facial expression of  each picture was on a scale of -4 (very negative) to +4 (very  positive; adapted from Wieser et al., 2014), with positive  ratings on the right. Verbal responses were recorded by an  experimenter so as not require participants to use their hands  when making responses, which might interfere with influence  of the previous puzzle task (Casasanto & Chrysikou, 2011).  
Results 
Preliminary analyses found no effect of gender, age, or race  on face judgment ratings (all ps > .54), so subsequent  analyses were collapsed across these factors. 
A 2 (condition: dominant, nondominant) by 3 (facial  expression: happy, unhappy, neutral) repeated measures  ANOVA with condition as a between-subjects factor was run  to explore the influence of hand use on emotional face  judgment. Results demonstrate a main effect of facial  expression (F(2, 43) = 585.14, p < .001; partial η2 = 0.93).  As might be expected, planned contrast reveal that  participants rated the happy faces as significantly more  
positive than the negative faces (happy: M = 3.21, SD=.49;  unhappy: M = -2.54, SD = .71, t(46) = 43.52, p < .001), the  happy faces as significantly more positive than the neutral  faces (happy: M = 3.21, SD = .49; neutral: M = -.24, SD = .43,  t(46) = 41.71, p < .001), and the neutral faces as significantly  more positive than the unhappy faces (neutral: M = -.24, SD  = .43, unhappy: M = -2.54, SD = .71; t(46) = 24.37, p < .001).  There were no other main effects or interactions, suggesting  condition did not have a significant effect on face judgment. 
On average, participants correctly placed under one third  of the potential 25-pieces in the puzzle task correctly (M =  7.24 pieces, SD = 3.14, range = 1-15), suggesting that the 1  minute time limit made this task particularly challenging  regardless of which hand was used. An independent sample  t-test found no significant effect of hand condition on the  number of puzzle pieces correctly placed (dominant: M =  8.09, SD = 3.07; nondominant: M = 6.46, SD = 3.08, t(46) =  1.78, p = .078), though the patterns suggest that those using  their dominant hand may have been at a slight advantage.  There were no significant correlations between the number of  puzzle pieces correctly placed and facial expression ratings  within either condition (all ps > .189). 

Figure 2. Emotional ratings of neutral faces based on condition  (dominant, nondominant) in Studies 1 & 2. Error bars represent  standard error. 
Discussion 
Our results showed no significant connection between hand  usage (dominant vs. nondominant) and face judgment.  Though participants were able to adequately distinguish and  rate the facial expression present on the stimulus pictures,  these ratings were not influenced by the previous motor task. 
It is possible that the difficulty of the puzzle task was so  high that participants were unable to reach their desired level  of motor fluency even when using their dominant hand.  Indeed, with only one minute to complete the task and 25  pieces to correctly place (absent a reference picture), no  
1490
participants were able to finish the puzzle and often appeared  distressed by this fact. These negative emotions may have  been more pronounced in the dominant hand condition,  where participants knew they were using their “good” hand,  than in the nondominant condition, where poor performance  could be more easily attributed to hand constraints.  
Though a byproduct in the current study, such ‘mood  priming’ has been correlated with negative biases on emotion  recognition tasks in the past (Schmid & Mast, 2010).  Therefore, Study 2 we changed the motor task to more  directly align with the previous body specificity literature  (Casasanto and Chrysikou, 2011) and to allow adequate time  for participants to complete the task across conditions.  
Study 2 
Participants 
Data from 59 participants between the ages of 18 and 21 years  of age (M = 19.28, 25 males, 34 females) were analyzed for  Study 2. All were recruited and compensated as in Study 1  and were similarly assigned to either the dominant hand  condition or nondominant hand condition. All participants  consented to the study and were right-handed according to  self-report 
Materials & Procedure 
The procedure was the same as Study 1, with the following  exceptions: In lieu of the puzzle task, all participants  completed a domino placing motor task (adapted from  Casasanto & Chrysikou, 2011) before the face judgment task.  
During the domino task, participants were instructed to  arrange approximately 60 dominos onto two laminated  pictures according to the order and color of the dots on the  stimuli. One of the laminated pictures depicted an ampersand  symbol and the other depicted a star (see Figure 3). The  shapes were outlined with colored dots in rainbow order (red,  orange, yellow, green, blue, black), and the colors of the  dominoes corresponded to the colors of the dots. The  dominoes were to be placed vertically and parallel to one  another. In the case that any dominoes fell over, participants  were instructed to realign them before continuing (as per  Casasanto & Chrysikou, 2011). 

Figure 3. Action shot of participant placing dominos onto one  diagram (the ampersand) next to a picture of the second diagram (a  star). 
As opposed to Study 1, where participants were only given  1 minute to complete a difficult motor task, Study 2 allowed  participants 10 minutes to place all the dominos. This time  allowed almost all participants (N = 57) to complete the task  with time to spare, while also assuring significantly more  motor priming before moving onto the second task. 
After ten minutes, participants stopped the domino task and  were asked to complete the face judgment task as in Study 1.  Furthermore, in Study 2, the “0” was excluded on the  emotional rating Likert scale in order to reduce a central  tendency bias (Garland, 1991). 
Results 
Preliminary analyses found no effect of gender, age, or race  on face emotion ratings (all ps > .28) so subsequent analyses  were collapsed across these factors. 
A 2 (condition: dominant, nondominant) x 3 (facial  expression: happy, unhappy, neutral) repeated measures  ANOVA with condition as a between-subjects factor was run  to determine if there was an interaction between hand use and  subsequent face judgment. As in Study 1, results demonstrate  significant main effect of facial expression (F(2, 57) =  1505.66, p < .001; partial η2 = 0.964), but also a significant  condition x facial expression interaction (F(2, 57) = 3.01, p =  .05, partial η2 = 0.066). There were no other significant main  effects. A series of planned independent t-tests revealed that  participants in the dominant condition rated neutral faces as  more positive (M = 0.24, SD = 0.62) than participants in the  nondominant condition (M = -0.07, SD = 0.58), t(57) = 2.00,  p = 0.05, see Figure 2). There were no significant differences  between ratings of happy (p = .19) or unhappy (p = .41) faces  across conditions.  
Discussion 
In Study 2, we built upon the findings of Study 1 by creating  a motor task that was more similar to previous body  specificity paradigms (Casasanto & Chrysikou, 2011) while  also allowing adequate time for participants to complete the  task. Consistent with our hypotheses, participants who  completed the domino task with their dominant (right) hand  rated neutral faces more positively compared to participants  who used their nondominant (left) hand on the domino task.  This finding aligns with previous research demonstrating that  people associate their dominant side with more positive  thoughts and ideas (Casasanto, 2009; Casasanto &  Chrysikou, 2011; Casasanto & Jasmin, 2010; Casasanto &  Jasmin, 2012), suggesting that the use of one hand over  another can have real ramifications for social stimulus  perception. 
General Discussion 
The current set of studies was motivated by findings  demonstrating that handedness can influence the perception  
1491
of traditionally non-motor stimuli (e.g., Casasanto, 2009).  We found that restricting a participant to use either their  dominant or nondominant hand during a motor task had  cascading effects, resulting in differential face judgments.  Interestingly, this effect only appeared if participants were  given a task that was possible to complete, and with enough  time to conceivably complete it (as in Study 2). Under  extreme time restraints on a more difficult task (Study 1),  participants using their dominant hand did not show any  positivity effects, and in fact trended towards a negativity  bias. 
 Though not a central aim of the study, the differences in  response to the motor tasks of Study 1 and Study 2 highlight  the important influence that task length and difficulty can  have on face judgment. In Study 1, even those in the  dominant hand condition were faced with a task they could  not complete in time, potentially negating any internal  positive feedback they may have received from doing well on  a motor paradigm. In Study 2, allowing dominant condition  participants more time to experience motor fluency and a  chance at completing the task resulted in a more positive  evaluation of subsequent neutral face pictures. Future  research could better disentangle the relative influences of  priming time and task difficulty in order to create a more  comprehensive picture of their respective effect on face  judgment. 
 It is also possible that the puzzle task used in Study 1 was  not motor-specific enough to create the type of motor fluency  effects seen in previous research (e.g., Casasanto, 2009;  2011; 2014). Indeed, jigsaw puzzles are most commonly used  to study spatial perception, and not motor abilities, in the  psychological literature (e.g., Richardson & Vecchi, 2002).  Furthermore, as our puzzle did not come with a map or  picture to follow, participants often spent a large portion of  task time simply trying to determine the structure and spatial  layout of the complete picture. This may have decreased the  time spent moving puzzle pieces, and also distracted them  from any conscious or unconscious feelings of motor fluency.  Using a task that more closely aligned with the previous  research remedied this issue (Study 2), allowing for a more  motor-specific (vs. spatial) task. 
 It is also important to mention that without a matched  sample of left-handed participants, it is difficult to determine  whether the current findings are due to relative body fluency,  or due to the effects of using one's right hand when right 
handed. Indeed, previous research has suggested that the left hemisphere of the brain, which controls the right side of the  body, might be particularly relevant for processing  positively-valenced emotions (e.g., Adolphs, Jansari, &  Tranel, 2001). If this is the case, it's possible that individuals  using their dominant (right) hand were simply tapping  underlying neural structures that support positive  perceptions. However, more recent research suggests that  such findings may be better interpreted as body fluency  effects as opposed to hemispheric specialization for emotions  (Brookshire & Casasanto, 2013). In the future, utilizing both  right- and left-handed individuals will help in better  
disentangling the relative influence of handedness on face  judgment. 
 Taken together, our findings underscore the influence that  our bodies can have on judgments of social stimuli. These  results have implications not only for future research, but also  for everyday life interactions. If using one’s dominant hand  can alter the interpretation of social stimuli, it is worth  pondering the ramifications this might have for those forced  to use a certain hand due to injury, disease, or environmental  constraints. Future work is necessary to explore the  ecologically valid bounds of this body-specific effect, further  elucidating the connections between movement and mind. 
Acknowledgements 
The authors would like to thank all of the individuals who  volunteered to help with this study, along with Geoff  Brookshire and Daniel Casasanto for their helpful comments  and suggestions prior to data collection. The first four authors  would also like to thank Dr. Lauren H. Howard for her  continued support, guidance, and encouragement throughout  the research process. Funding to support data collection was  provided by Franklin & Marshall College’s COG Student  Research Grants to the first four authors.  
References  
Adolphs, R., Jansari, A., & Tranel, D. (2001). Hemispheric  perception of emotional valence from facial expressions.  Neuropsychology, 15, 516-524. 
Brookshire, G., & Casasanto, D. (2013). Brief motor  experience reverses visual hemifield effects for emotional  faces. Poster presented at the 25th annual convention of the  Association for Psychological Science, Washington, D.C. 
Casasanto, D. (2009). Embodiment of abstract concepts:  Good and bad in right- and left-handers. Journal Of  Experimental Psychology: General, 138, 351-367. 
Casasanto, D. (2011). Different bodies, different minds: The  body-specificity of language and thought. Current  Directions in Psychological Science, 20(6), 378–383 
Casasanto, D. (2014). Bodily relativity. The Routledge  Handbook of Embodied Cognition, 108-117. 
Casasanto, D., & Chrysikou, E. G. (2011). When left is  “right”: Motor fluency shapes abstract concepts.  Psychological Science, 22, 419-422. 
Casasanto, D., & Gijssels, T. (2015). What makes a metaphor  an embodied metaphor? Linguistics Vanguard, 1, 327- 337. 
Casasanto, D., & Jasmin, K. (2010). Good and bad in the  hands of politicians: Spontaneous gestures during positive  and negative speech. PLoS ONE, 5, 1-5.  
De Gelder, B. (2009). Why bodies? Twelve reasons for  including bodily expressions in affective neuroscience.  Philosophical Transactions of the Royal Society B, 364,  3475-3484 
de la Fuente, J. D., Casasanto, D., & Santiago, J. (2015).  Observed actions affect body-specific associations  between space and valence. Acta Psychologica, 156, 32- 36. 
1492
de la Vega, I., De Filippis, M., Lachmair, M., Dudschig C.,  & Kaup, B. (2012). Emotional valence and physical space:  Limits of interaction. Journal of Experimental Psychology:  Human Perception and Performance, 38, 375-385. 
de la Vega, I., Graebe, J., Härtner, L., Dudschig, C., & Kaup,  B. (2015). Starting off on the right foot: Strong  right-footers respond faster with the right foot to positive  words and with the left foot to negative  words. Frontiers In Psychology, 6. 
Fidler, J. (2002). Sona Systems [Computer software].  Estonia: Sona Systems, Ltd. 
Firth, C. (2009). Role of facial expressions in social  interactions. Philosophical Transactions of the  Royal Society B, 364, 3453-3458. 
Freddi, S., Brouillet, T., Cretenet, J., Heurley, L. P., & Dru,  V. (2016). A continuous mapping between space and  valence with left- and right-handers. Psychonomic Bulletin  & Review, 23, 865-870. 
Garland, R. (1991). The mid-point on a rating scale: Is it  desirable? Marketing Bulletin, 2, 66-73. 
Goldinger, S. D., Papesh, M. H., Barnhart, A. S., Hansen, W.  A., & Hout, M. C. (2016). The poverty of embodied  cognition. Psychonomic Bulletin & Review, 23, 959-978. 
Jasmin, K., & Casasanto, D. (2012). The QWERTY Effect:  How typing shapes the meanings of words.  Psychonomic Bulletin & Review, 19, 499-504. 
Kong, F. (2013). Space-valence associations depend on  handedness: Evidence from a bimanual output task.  Psychological Research, 77, 773-779. 
Lee, T. H., Choi, J. S., & Cho, Y. S. (2012). Context  modulation of facial emotion perception differed by  individual difference. PLoS One, 7(3), e32987. 
Loth, E., Garrido, L., Ahmad, J., Watson, E., Duff, A., &  Duchaine, B. (2018). Facial expression recognition as a  candidate marker for autism spectrum disorder: How  frequent and severe are deficits? Molecular Autism. 
Ma, D., Correll, J., & Wittenbrink, B. (2015). The Chicago  face database: A free stimulus set of faces and  norming data. Behavior Research Methods, 47, 1122- 1135. 
McFarland, R. A., & Kennison, R. (1989). Handedness  affects emotional valence asymmetry. Perceptual and  Motor Skills, 68, 435-441. 
Meyer, B., Beevers, C. G., & Pilkonis, P. A. (2004). What’s  in a (neutral) face? Personality disordrs, attachment styles,  and the appraisal of ambiguous social cues. Journal of  Personality Disorders, 18, 320-336. 
Oppenheimer, D. M. (2008). The secret life of fluency.  Trends in Cognitive Sciences, 12, 237-241. 
Richardson, J. T., & Vecchi, T. (2002). A jigsaw-puzzle  imagery task for assessing active visuospatial processes in  old and young people. Behavior Research Methods,  Instruments, & Computers, 34(1), 69-82. 
Russell, J. A., & Fernández-Dols, J. M. (Eds.). (1997). The  psychology of facial expression. Cambridge university  press. 
Smith, R. (2002). Qualtrics [Computer software]. Provo, UT:  Qualtrics. 
Wieser, M. J., Gerdes, A. B. M., Büngel, I., Schwarz, K. A.,  Mühlberger, A., & Pauli, P. (2014). Not so  harmless anymore: How context impacts the perception  and electrocortical processing of neutral faces.  NeuroImage, 92, 74-82. 
Williams, L. E., & Bargh, J. (2008). Experiencing physical  warmth promotes interpersonal warmth. Science, 322,  606-607. 
Yi, C. Y., Murry, M. E., & Gentzler, A. L. (2016). Perception  of emotional expressions in adults: The role of  temperament and mood. Journal Of Individual  Differences, 37, 16-23. 
Zhao, X., He, X., Zhang, W., Chen, G., Chen, Q., & Huang,  L. (2016). Interpersonal choice: The advantage on the left  or on the right? International Journal of Psychology. 
1493
Illusory causation and outcome density effects  with a continuous and variable outcome 
Julie Y. L. Chow (julie.chow@sydney.edu.au) 
Hilary J. Don (hdon7006@uni.sydney.edu.au) 
Ben Colagiuri (ben.colagiuri@sydney.edu.au) 
Evan J. Livesey (evan.livesey@sydney.edu.au) 
School of Psychology, Badham Building, University of Sydney, NSW 2006 AUS Abstract 
Illusory causation is a consistent error in human learning in  which people perceive two unrelated events as being causally  related. Causal illusions are greatly increased when the target  outcome occurs frequently rather than rarely, a characteristic  known as the outcome density bias. Unlike most experimental  designs using binary outcomes, real-world problems to which  illusory causation is most applicable (e.g. beliefs about  ineffective health therapies) involve continuous and variable  consequences that are not readily classifiable as the presence  or absence of a salient event. This study used a causal  learning task framed as a medical trial to investigate whether  outcome density effects emerged when using a continuous  and variable outcome that appeared on every trial.  Experiment 1 compared the effects of using fixed outcome  values (i.e. consistent low and high magnitudes) versus  variable outcome values (i.e. low and high magnitudes  varying around two means in a bimodal distribution).  Experiment 2 compared positively skewed (low density) and  negatively skewed (high density) continuous distributions.  These conditions yielded comparable outcome density effects,  providing empirical support for the relevance of the outcome  density bias to real-world situations in which outcomes are  not binary but occur to differing degrees. 
Keywords: illusory causation; outcome density; causal  learning; contingency learning 
Introduction 
Many of the decisions we make in everyday life are  motivated by beliefs about cause and effect. Based on the  perceived contingencies between events, humans act on the  environment in order to maximize desirable outcomes (e.g.  taking vitamin supplements to improve health) or prevent  undesirable ones (e.g. using insect repellent to prevent  mosquito bites). A strategy people often rely on to make  inferences about causal relationships is that the occurrence  of the potential cause should alter the probability of the  outcome (Jenkins & Ward, 1965). Simple contingency  learning experiments test this premise. Typically, they involve two binary events—one potential cause or cue (C)  and one outcome (O)—yielding four possible combinations  of cause and outcome, shown in Table 1.  
Manipulations of the covariation between cue and  outcome are possible by varying the relative frequency of  each trial type, with the resulting contingency conveniently  quantified using the ∆p metric (Allan, 1980), according to  Equation 1. 
Table 1: Contingency matrix showing the four different trial  types as a function of whether the cue and outcome are  present or absent. 
Outcome Present Outcome Absent 
Cue Present a b 
Cue Absent c d 
Equation 1: 
∆p = p(O|C) – p(O|~C) = [a/(a+b)] – [c/(c+d)] 
According to this rule, the contingency between two  events is dependent on the probability of the outcome  occurring when the cue is present and when the cue is  absent. If a cue generates the outcome, ∆p is positive,  whereas if a cue prevents the outcome from occurring, ∆p  has a negative value (i.e. the outcome is more likely to occur  when the cue is absent). Importantly, when a cue has no real  effect on the outcome, p(O|C) = p(O|~C), ∆p is zero. 
Although people are often accurate when assessing causal relationships (Wasserman, 1990), research has shown that  under certain conditions, we are misled to believe a causal  link between a potential (but ineffective) cause and an  outcome (Alloy & Abramson, 1979). Specifically,  judgments of causation consistently deviate from the ∆p rule  when there is no contingency between the cue and the  outcome (i.e. ∆p = 0) and, as described below, when the  frequency of a trial types is relatively high.  
The illusion of causality is an important phenomenon  because it represents a consistent error in human learning  that is thought to contribute to the development and  maintenance of superstitious beliefs and pseudoscientific  thinking (Matute, Yarritu & Vadillo, 2011).  Pseudoscientific beliefs are grounded in causal illusions,  whereby two unrelated events such as consuming echinacea  (i.e. an action or cue) and common cold prevention (i.e.  outcome) are believed to be related in some meaningful way  (Karsch-Völk, Barrett & Linde, 2015; Allan & Arroll,  2014). Despite the lack of supporting evidence for the  efficacy of certain complementary and alternative medical  treatments, many people still believe in their effectiveness  and may even prefer such treatments over those that are  scientifically validated (Lilienfeld, Ritschel, Lynn, Cautin,  & Latzman, 2014). 
1494
Illusory causation and event densities 
Manipulations that increase cue-outcome coincidences (i.e.  trial type a in Table 1) appear to be particularly effective in  inflating causal judgment, regardless of whether the two  events are actually causally associated with one another  (Wasserman, 1990; Blanco, Matute & Vadillo, 2013).  Many of the studies on illusory causation have thus  explored the frequency of the cue and outcome on  generating a false association. 
Outcome density (OD) bias refers to the tendency to  overestimate the relationship between cue and outcome  when the outcome occurs frequently. In a classic example,  Alloy and Abramson (1979) asked participants to determine  the degree of control they possessed over the onset of a  green light by pressing a button. In conditions where the  button press had absolutely no effect on the light,  participants were more likely to overestimate the action outcome relationship when the light frequently turned on  than when it rarely turned on. This outcome density effect  has now been replicated across a wide variety of learning  tasks with zero-contingency events (e.g. Blanco & Matute,  2015). A high outcome density increases the frequency of a 
and c trials relative to b and d trials (Table 1), even though  contingency remains zero. Similarly, when the probability  of the cue is high (inflating the frequency of a and b trials  relative to c and d trials), participants typically report  greater causal judgments than when the cue rarely occurs  (Allan & Jenkins, 1983). 
Causal learning about real-world outcomes Illusory causation is highly applicable to the formation and  maintenance of beliefs about alternative therapies for minor  illness. For example, complementary and alternative  medicine is regarded as the preferred treatment for back  pain in the United States (White House Commission on  Complementary and Alternative Medicine Policy, 2002); an  illness with a high rate of spontaneous remission analogous  to the light bulb spontaneously turning on frequently in outcome density experiments (e.g. Alloy & Abramson, 1979; Blanco & Matute, 2015). 
Not surprisingly, the assessment of treatment-outcome  relationships in the real world often proves to be more  difficult than when emulated in the laboratory. Complex,  continuous, and variable consequences experienced and  observed in the real world differ substantially from the  deliberately simple, unambiguous and binary outcomes used  in most contingency learning experiments (e.g. a light bulb  turning on or not). Real-world outcomes often involve continuous and variable changes that do not fit neatly into  the outcome-present versus outcome-absent dichotomy. 
This issue was highlighted by Marsh and Ahn (2009) in  the context of parsing ambiguous cues. They noted that the  task of parsing events into discrete categories is often not a  trivial problem, yet is ignored by simple covariation-based  models. Covariation-based models, including associative  learning models (e.g. Rescorla & Wagner, 1972), as well as  causal induction models (e.g. Cheng, 1997) anticipate  
illusory learning effects, and the outcome density bias in  particular, by assuming a certain mental representation of  cue-outcome coincidences. These models are usually  implemented by classifying, in a dichotomous fashion,  whether the cue and outcome are each present or absent, in  line with the four discrete trial types shown in Table 1. In  other words, each experience can be classified as supporting  or disconfirming the putative causal relationship.  
Most real-life situations are sufficiently complex, which presents a problem to the way in which these models are  applied. To illustrate, most medical treatments produce  some outcome in varying degrees (e.g. patient is still sick or  patient gets better), and rarely if ever produce no outcome at  all. However, it is unclear whether people readily parse their  experiences of continuous variable events into the presence  versus absence of a target outcome. As such, it is important  to test whether continuous outcomes produce lawful  variations in illusory causal judgments in the same way as a  simple binary outcome. Thus, we were interested in  measuring illusory causation and outcome density effects  using continuous and variable outcomes that are always  present to some extent and may be difficult to dichotomize. 
The current study 
The aim of the current study was to test whether illusory  causation and outcome density effects could be generated  using an outcome that always occurred but to a varying degree. Our study used a contingency learning task framed  as a medical trial for a new fictitious drug. Participants were  presented with a causal scenario and instructed to make  judgments about the relationship between a drug cue and  health improvements. Rather than using discrete outcome  events, an outcome was presented on every trial, but its  magnitude varied along a continuous scale. Participants then  observed a series of trials with and without the drug, with  the drug actually having no impact whatsoever on recovery  (∆p = 0 and precisely the same distribution of outcome  magnitudes for trials with and without the drug). In both  Experiments 1 and 2, participants were separated into Low  and High Outcome Density (OD) conditions, where the  outcome was improvement in patient's health. Low OD  participants observed outcomes that were predominantly  low in magnitude (i.e. little improvement in health) with  some high-magnitude outcomes (i.e. large improvement in  health), whereas High OD participants observed  predominantly high-magnitude with some low-magnitude  outcomes.  
In Experiment 1, we used distributions of outcomes  centered on a high (80) and low (20) mean value, and tested  whether the presence of variability in the outcome around  these mean outcomes affected illusory causation and  outcome density effects (Variable vs. Fixed outcomes). In  Experiment 2, all participants were presented with  continuous and variable outcomes sourced from a single  skewed distribution, with either a high or low modal value  (see Figure 1). As in most OD studies, the critical measure  was participants' causal judgments about the cue, in this  
1495
case measured using ratings of how effective the drug was  in treating the disease relative to no treatment. 
Figure 1: (a) Bimodal outcome distribution presented to  participants in Variable outcome and Low OD condition,  where 80% of outcomes were low in magnitude. (b)  Bimodal outcome distribution presented to participants in  the Variable outcome and High OD condition, where 80%  of outcomes were high in magnitude. (c) Continuous  outcome distribution presented to participants in the Low  OD condition, where 80% of outcomes were below an  outcome value of 50. (d) Continuous outcome distribution  presented to participants in the High OD condition, where  80% of outcomes were above an outcome value of 50. 
Experiment 1 
Method 
Participants. One hundred and twelve participants (78  female, Mage = 22.2, SD = 5.35) completed the study for  class participation or monetary reimbursement. Participants  were randomly allocated to one of four experimental  
Table 2: Proportion of total trials (per block and overall) in  low and high outcome density conditions. Blocks of 10  trials were presented 10 times, yielding 100 trials. Low OD group High OD group 
Cloveritol No  
treatment Cloveritol No  
treatment 
High  
Outcome 0.1 0.1 0.4 0.4 
Low  
Outcome 0.4 0.4 0.1 0.1 
Procedure. Participants were asked to imagine they were  a medical researcher investigating a new illness. They were  told a new experimental drug ‘Cloveritol’ had been created  to treat the disease. The objective of the study was to test  the drug’s efficacy in treating the disease. All participants  were told that patients usually take a long time to recover,  and a large improvement in health is indicative of rapid  recovery. 
During training, participants were presented with trials in  which they were asked to predict the level of improvement  in the patient’s health. Each trial represented a new patient  and participants were shown if the drug was administered  (represented by a picture of a pill bottle and drug name), or  not administered (‘No treatment’). Below this cue, a scale  was presented ranging from 0% (no improvement) to 100%  (full recovery), and participants were required to predict the  patient’s health in that trial by clicking on a point on the  scale. Once a prediction was made, an identical scale would  appear below with the actual observed health improvement  for that trial animated as a growing horizontal bar across the  scale. Task schematics are illustrated in Figure 2. 
X 
Cloveritol administered 
What level of improvement do you expect? 
conditions according to time of arrival (n = 28 in each). Design. The study used a 2 (OD: High vs. Low) x 2  (Outcome Variability: Fixed vs. Variable) between-subjects  design. For participants in the Variable outcome condition,  the observed outcome was sampled from a low distribution  (M = 20, SD = 5, Range = 13-27) or high distribution (M =  80, SD = 5, Range = 73-87) depending on OD group. In  contrast, participants in the Fixed outcome variability 
0% 
No  
improvement 
100% 
Full recovery 
X 
Cloveritol administered 
0% 
No  
improvement 
Press any key to continue 
20% 
X 
100% 
Full recovery 
condition were presented with an exact-value outcome of 80  
on 80% of trials and 20 on 20% of trials in the High OD  
20% 
0% 
No  
condition, and an exact-value outcome of 20 on 80% of  
Cloveritol administered 
100% 
Full recovery 
trials and 80 on 20% of trials in Low OD condition.  The experiment was programmed using Matlab and the  
improvement 
0% 
No  
improvement 
Observed health improvement: 80% 
100% 
Full recovery 
Psychophysics Toolbox extensions (Brainard, 1997; Pelli,  1997). All participants completed 100 training trials, 50  with and 50 without the treatment cue. Trials were presented  to participants in blocks of 10 such that each block was  representative of the total frequency of high and low  outcomes in the experiment (Table 2). 
Figure 2: Typical displays during the training phase in  Experiment 1 and 2. Participants are presented with either  the drug cue (Cloveritol administered) or no cue (No  Treatment) and asked to make a prediction on the patient’s  health improvement. Having done so, a second scale  appears with the observed outcome for that patient. 
1496
During test, participants were instructed to make  judgments about the treatment based on observations during  training. Participants were first presented with the drug cue  and no treatment cue separately with instructions to predict  the level of improvement they would expect on average if a  patient were given Cloveritol or No Treatment. Ratings  were made on the same scale as presented during training.  
Table 3: Average rating for Cloveritol and No Treatment at  test (SD) as a function of Outcome Density (Low vs High)  and Outcome Variability (Fixed vs Variable). 
Low OD High OD 
Cloveritol No 
treatment Cloveritol No  
treatment 
Subsequently, they were asked to rate how effective they  thought the treatment was relative to no treatment. Ratings  were made on a scale from 0 (Completely Ineffective) to 10  (Completely Effective). 
Exp 1  Fixed 
Exp 1  Variable 
34.1 
(13.7) 
34.8 
(14.7) 
32.6 
(14.0) 
32.8 
(16.1) 
68.0 
(13.8) 
71.5 
(15.2) 
59.3 
(17.4) 
67.5 
(15.4) 
Results 
Exp 2 44.1 (12.4) 
36.3 
(15.0) 
69.6 
(12.4) 
53.3 
(16.7) 
We focus first on the results of critical importance, namely  the effect of outcome density, and its interaction with  outcome variability, on judgments of treatment efficacy. Causal judgments of this nature have most consistently  produced OD effects in previous studies and we expected  these ratings to show the effect most reliably in our study.  These efficacy ratings are illustrated in Figure 3. As  predicted, we found a main effect of OD, F(1,108) = 11.3, p = .001, ηp2 = .094, such that participants in the High OD  condition (M = 4.73, SD = 2.69) reported significantly  greater efficacy ratings than participants in the Low OD  condition (M = 3.11, SD = 2.34). 
Critically, we found no significant interaction effect  between OD and outcome variability, F(1,108) = .004, p =  .953, ηp2 < .001, suggesting that the OD effect did not differ  significantly between groups. Indeed, significant OD effects  were found when participants were presented with Variable  outcomes, F(1,108) = 5.43, p = .022, ηp2 = .048, as well as  when they were presented with Fixed outcomes, F(1,108) =  5.83, p = .017, ηp2 = .051. 
Average outcome magnitude predictions for Cloveritol  and No Treatment at test are reported in Table 3. We found  a significant main effect of cue type on average predictions,  F(1,108) = 6.37, p = .013, ηp 2 = .056, with greater average  ratings for Cloveritol (M = 52.1, SD = 22.7) than No  Treatment (M = 48.1, SD = 22.1). This finding suggests an  illusory causation effect. Participants were predicting  greater health recovery when the drug was present than  when it was absent, despite there being no contingency  between cue and outcome. However, this effect of cue type  did not interact with outcome density, F(1,108) = 2.00, p = .160, ηp 2 = .018; this was true for both outcome variability  conditions, F < 1. This null interaction is not uncommon in  work on illusory causation, which tends to produce outcome  density effects on causal ratings rather than on direct  predictions of the outcome. 
Discussion 
Experiment 1 found a clear OD effect in efficacy  judgements at test using a variable outcome distribution.  
) 0
1
 
o
t
 
0 
:
e
la
c
s
( 
g
n
it
a
r
 
y
c
a
c
i
ff
E
 
1
 
p
x
E
7 
6 
5 
4 
3 
2 
1 
0 
Variable  
outcome 
Low Outcome Density 
High Outcome Density 
Fixed outcome Single  distribution 
50 40 30 20 10 0 
The High OD condition produced greater efficacy ratings  than the Low OD condition and this difference was  
E
x
consistent across Fixed and Variable outcome conditions.  
p
 
2
 
E
We consider this result to be highly consistent with other  
f
f
i
c
studies that have used discrete binary outcomes. Similar 
a
c
y
 
outcome density effects were not found when participants  
r
a
t
i
n
were asked to provide an average prediction for treatment  
g
 
(
s
c
cue and no treatment cue at test. This discrepancy in causal  
a
l
e
:
 
judgments as a function of question format has been  
-
1
0
0
extensively discussed elsewhere (see Vadillo & Matute,  
 
t
o
 
1
2007). Importantly for our purposes, previous research has 
0
0
) 
Experiment 1 Experiment 2 
Figure 3: Drug efficacy ratings at test (±SE) as a function of  OD and outcome variability in Experiment 1 (left) and for  Low vs High OD conditions in Experiment 2 (right).  Efficacy ratings were measured on a scale ranging from 0  (Completely Ineffective) to 10 (Completely Effective) in  Experiment 1, and from -100 (Effectively worsens recovery)  to 100 (Effectively improves recovery) in Experiment 2.  Negative efficacy judgments in Experiment 2 indicates that  the drug makes patients feel worse, whereas positive values  suggest the drug improves patient recovery. 
consistently found this discrepancy, which supports our  claim that the adopted experimental design did not  significantly differ from traditional binary-outcome  contingency learning paradigms. 
In this experiment, the outcome magnitudes were  sampled from two distinct and non-overlapping  distributions, each with relatively low variance. Thus categorizing them as two discrete outcomes (high and low)  may still be relatively straightforward for the participant. It  is still important to demonstrate that OD biases generalize to  situations in which the distribution is not so distinctly  partitioned. Experiment 2 examined the same outcome  
1497
density effects by using a continuous outcome distribution,  in which all participants experience a full range of outcome  values. 
Experiment 2 
Experiment 2 was identical to Experiment 1 in all respects  except the way in which outcomes were distributed. Instead  of a bimodal distribution with values centered around 20  and 80, participants experienced outcome values sourced  from a single distribution, and ranging from 1 to 99. Similar  to Experiment 1, participants in the Low OD condition  experienced a majority of low-magnitude outcomes with  some high-magnitude outcomes, and this was reversed for participants in the High OD condition. All outcomes  presented were independent of the cue. 
A central difference between the distribution used in  Experiment 2 and that of Experiment 1 is the addition of  ambiguous outcome values around the mid-range of the  scale that are less readily classifiable as low-magnitude or  high-magnitude outcomes. We were interested in  determining whether we could still obtain outcome density 
biases in a trial-by-trial contingency learning task with the  use of continuous and variable outcomes sampled from a  complete range of values, some of which are more  decipherable to the participants (low vs. high) than others.  
Ratings of treatment efficacy presented at test were  modified to capture greater variance in responses, with  values ranging from -100 (Effectively worsens recovery) to  100 (Effectively improves recovery) with a midpoint of 0  (Completely ineffective). This modification in Experiment 2 
allowed meaningful comparisons to be made between the  group means and zero, the midpoint of the scale, whereas  zero represented an extreme end of the scale in Experiment  1. It is also possible that some participants judge that the  drug actually makes health improvement less likely since  the base rate of recovery without the drug was quite high. 
Method 
Participants. 56 participants (35 female, Mage = 22.9, SD = 4.44) completed the study for either class participation or  monetary reimbursement and were randomly allocated to  one of two experimental conditions (n = 28 in each). 
Design. The study used a between-subjects design, with  outcome density (Low vs. High OD) as the only  manipulation. For the Low OD condition, the sample of  observed outcomes O was positively skewed, created using  a truncated ex-gaussian distribution with a higher proportion 
of low-magnitude outcomes (Distribution parameters: µ=  10, σ = 5, τ = 25, Range = 1-99, yielding sample Mean = 32,  SD = 20). For the High OD condition, we took the  complement of this same distribution (i.e. 100 – O) to  produce a negatively skewed distribution with a higher  proportion of high-magnitude outcomes (sample Mean = 68,  SD = 20). Outcome values were further constrained by the  proportion of trials with an outcome-value below 50:  participants in the Low OD condition experienced .75 of  trials with outcomes below 50, whereas participants in the  
High OD condition only experienced .25 of trials with  outcomes below a value of 50. All participants received  identical causal instructions and the procedure of the study  was identical to that of Experiment 1.  
Results 
Efficacy ratings at test are shown in Figure 3. As predicted,  we found a main effect of OD, F(1,54) = 4.54, p = .038, ηp2 = .078, such that participants in the High OD condition (M =  33.6, SD = 31.7) reported significantly greater efficacy  ratings than participants in the Low OD condition (M =  16.5, SD = 28.3). A comparison of the group means against  zero found a significant difference for both Low OD, t(27) =  3.09, p = .005, d’ = .584, and High OD, t(27) = 5.62, p <  .001, d’ = 1.06. These findings indicate illusory causation  effect in both Low and High OD condition, with greater  difference found in the High OD group. As in Experiment 1,  we did not find an interaction between cue type and  outcome density in average outcome magnitude ratings,  F(1,54) = 3.06, p = .086, ηp2 = .054.  
Discussion 
In Experiment 2, we found support for the use of continuous  and variable outcomes in generating an outcome density  effect, with significantly greater judgments of treatment  efficacy in the High OD relative to the Low OD condition.  
Together, the results from Experiments 1 and 2 indicate  that reliable OD effects emerge even when the outcomes are presented in a continuous and variable manner, mirroring  some important properties of real-world outcomes.  Importantly, the OD effect obtained using a continuous and  variable outcome was not significantly different to that from  a fixed-value outcome (analogous to binary events),  suggesting that the current experimental paradigm is a  reliable measure of the effect in generating illusory  causation. 
To our knowledge, this is the first study showing OD  effects with outcomes that are potentially ambiguous. That  is, the magnitude of these outcomes provides information that is not always readily classifiable as confirming or  disconfirming the learner’s current causal hypothesis. There  are two opposing explanations for this finding. Firstly,  individuals may parse ambiguous outcome information into  discrete categories and use this information to form  judgments about causal relationships. This ability is  potentially important for accurate contingency learning, but  may also be instrumental in producing the errors of  judgment leading to OD effects. Increasing the frequency of  outcome-present trials, in this case high-magnitude  outcomes, creates an over-representation of cue-outcome  coincidences, resulting in stronger causal judgments. This  interpretation of our findings parallels previous studies with  ambiguous cue information, which has shown learners to  spontaneously categorize ambiguous intermediate  observations in a discrete fashion and use them in  subsequent contingency judgments (Marsh & Ahn, 2009).  
1498
A second interpretation of these findings is that the OD effect emerges even when the learner is not able to  categorize events into discrete classes. If so, these results  lend themselves to Bayesian models of causal judgment  (Griffiths & Tenenbaum, 2005) in which continuous  representation of the expected outcome magnitude can be  implemented relatively easily. Associative learning models  like the Rescorla-Wagner model can also accommodate  continuous outcomes by assuming that the "teaching signal"  that represents the experienced outcome can take on values  proportional to the outcome magnitude. These theoretical  approaches may be able to account for OD effects and  illusory causation without assuming discrete categorization 
of events by the learner. Testing the capabilities of these  models is thus an important future endeavor. 
The results from our experiments provide empirical  support for the use of continuous and variable outcomes that  mimic real-world events in obtaining an outcome density  bias. This is particularly relevant for researchers interested  in investigating false causal beliefs in medicine and public  health, where the consequences of choosing the wrong  treatment could have detrimental effects.  
Conclusion 
Across both experiments, we found a reliable outcome  density effect, where participants who frequently observed high levels of health improvement judged a fictitious drug  to be more efficacious than participants who observed levels  of health improvement that were frequently low, even when  there was no real contingency between drug and health  outcome. This finding is compelling given the novel  experimental paradigm using continuous and variable  outcomes that occur on every trial but vary in degree. This  approach also produced effects that were not significantly  different from fixed-value outcomes analogous to binary  events adopted in previous contingency learning  experiments. The experimental approach we present here,  that is, representing treatment outcomes in a continuous and  variable fashion that mimic real-world medical  consequences, may be an important stepping-stone to  bridging the gap between experimental research and real world experience. 
Acknowledgments 
This research was supported by Discovery Grant  DP160102871 from the Australian Research Council. 
References  
Allan, L. G. (1980). A note on measurement of contingency  between two binary variables in judgment tasks. Bulletin  of the Psychonomic Society, 15, 147-149 
Allan, L. G., & Jenkins, H. M. (1983). The effect of  representations of binary variables on judgment of  influence. Learning and Motivation, 14, 381-405. 
Allan, G. M., & Arroll, B. (2014). Prevention and treatment  of the common cold: making sense of the  
evidence. Canadian Medical Association Journal, 186,  190-199. 
Alloy, L. B., & Abramson, L. Y. (1979). Judgment of  contingency in depressed and nondepressed students:  Sadder but wiser? Journal of experimental psychology:  General, 108, 441.  
Blanco, F., Matute, H., & Vadillo, M. A. (2013). Interactive  effects of the probability of the cue and the probability of  the outcome on the overestimation of null  contingency. Learning & Behavior, 41, 333-340. 
Blanco, F., & Matute, H. (2015). Exploring the Factors That  Encourage the Illusions of Control. Experimental  psychology, 62, 131.  
Brainard, D. H. (1997). The psychophysics toolbox. Spatial  vision, 10, 433-436.  
Cheng, P. W. (1997). From covariation to causation: a  causal power theory. Psychological review, 104, 367. Griffiths, T. L., & Tenenbaum, J. B. (2005). Structure and  strength in causal induction. Cognitive psychology, 51,  334-384. 
Jenkins, H. M., & Ward, W. C. (1965). Judgment of  contingency between responses and outcomes.  Psychological Monographs: General and Applied, 79, 1. 
Karsch-Völk, M., Barrett, B., & Linde, K. (2015).  Echinacea for preventing and treating the common  cold. Jama, 313, 618-619. 
Lilienfeld, S. O., Ritschel, L. A., Lynn, S. J., Cautin, R. L.,  & Latzman, R. D. (2014). Why ineffective  psychotherapies appear to work: A taxonomy of causes of  spurious therapeutic effectiveness. Perspectives on  Psychological Science, 9, 355-387. 
Marsh, J. K., & Ahn, W. K. (2009). Spontaneous  assimilation of continuous values and temporal  information in causal induction. Journal of Experimental  Psychology: Learning, Memory, and Cognition, 35, 334. 
Pelli, D. G. (1997). The VideoToolbox software for visual  psychophysics: Transforming numbers into movies.  Spatial vision, 10, 437-442 
Vadillo, M. A., & Matute, H. (2007). Predictions and causal  estimations are not supported by the same associative  structure. Quarterly Journal of Experimental Psychology,  60, 433-447. 
Wasserman, E. (1990). Detecting response-outcome  relations: Toward an understanding of the causal texture  of the environment. Psychology of Learning and  Motivation, 26, 27-82.  
White House Commission on Complementary and  Alternative Medicine Policy. (2002). White House  Commission on Complementary and Alternative Medicine  Policy. Final report (WHCCAM Report). Available at:  http://www. whccamp.hhs.gov/pdfs/fr2002_document.pdf  [accessed January 23, 2018]. 
1499
Mechanistic Knowledge Generalizes Differentially 
Aaron Chuey, Mark Sheskin, Frank Keil (aaron.chuey@yale.edu, mark.sheskin@yale.edu,  frank.keil@yale.edu) 
Yale Department of Psychology, 2 Hillhouse Ave. New  Haven, CT 06511 USA 
Abstract: When inferring the extent of others’ knowledge from samples of what they know, certain kinds of samples imply richer content. One candidate kind is knowledge of  causal mechanism. In the current study, we investigate  whether children and adults think that knowledge about  mechanism generalizes more broadly than non-mechanistic  factual knowledge. We find an early-emerging assumption  that mechanistic knowledge about a basic level category  implies greater knowledge about a superordinate category, compared to factual knowledge about the same basic level  category. Even young children have a sophisticated sense of  how causal mechanisms generalize across categories, despite  possessing little mechanistic knowledge themselves. These  intuitions likely support the epistemic inferences we make  from early childhood onward. 
Keywords: mechanism, causal reasoning, knowledge,  category learning, epistemic inference 
Introduction 
To benefit from a world full of information, we must make a  variety of inferences about who possesses useful  information and how much of it they possess. To do so, we  also need a sense of how broadly knowledge and  information generalize. For example, if someone knows  how tractor engines work, should I assume that she is also  knowledgeable about cars? Airplanes? Iguanas? Even  children demonstrate sophisticated intuitions about how  knowledge (Keil et al, 2008) and explanations (Johnston et  al, 2017) generalize across kinds and domains, but it is  unclear what underlies these intuitions. Here, we argue that  a sense of shared understandings of causal mechanisms plays an important role in the way we generalize knowledge 
from one kind to another, and that this sense is evident in  children’s early emerging epistemic intuitions. A growing literature supports the importance of  mechanism in young children’s strategies for evaluating and  structuring knowledge. For example, preschoolers judge that  someone who can fix an object has more causal knowledge  about it than someone who knows its name (Kushnir  Vredenburgh, & Schneider, 2013). Because children  associate mechanistic knowledge with an ability to fix  (Lockhart et al, under review), intuitions about causal  mechanisms may influence children’s expectations about  who possesses useful information. 
Young children also show an appreciation for mechanism  when reasoning about the way knowledge is structured  beyond particular knowers. By age 5, children group  biological and psychological processes separately based on  
a notion of shared causal mechanisms (Erickson, Keil &  Lockhart, 2010). As they get older, children develop a  stratified sense of difficulty for the sciences (Keil, Lockhart,  & Schlegel, 2010), suggesting that intuitions about causal  mechanisms affect not only the way children organize  knowledge, but also their attitudes towards it.  
Causal mechanism also pervades children’s explanatory  preferences. When requesting information, young children  are often not satisfied with statements of fact or circular  reasons, instead preferring causal explanations (Corriveau &  Kurkul, 2014; Frazier, Gelman & Wellman, 2009; 2016).  Children’s desire for rich information increases with age;  requests for causally rich explanations take up an  increasingly large proportion of children’s questions as they  reach elementary school (Chouinard et al., 2007). For  example, one study found that “how” questions make up  only 3.5% of 3-year-olds’ questions, but 19.8% of 5-year  olds’ questions (Callanan & Oakes, 1992). Furthermore, young children remember a larger number of causally  relevant features when they explain phenomena, as opposed  to merely reporting on them (Legare & Lombrozo, 2014;  Walker et al, 2017). Preschoolers likewise privilege “deep” properties in their own explanations, favoring more  inductively powerful features of a system when engaging in  explanation (Walker et al, 2014).  
Although there is clear evidence that notions of mechanism influence children’s reasoning about knowledge  and explanations, what accounts for its utility? Here, we  argue that one critical feature is mechanism’s 
generalizability across related kinds. Trouche et al. (2017)  found that mechanism-focused instruction shifted  elementary school children’s complexity intuitions more  than factual non-mechanistic instruction, suggesting that  mechanism cues children to certain properties that other  forms of factual information do not. Children’s mechanism 
induced complexity intuitions also propagate to related  entities (Trouche et al, under review), suggesting that  children may expect knowledge of mechanism-related  properties to apply broadly. For example, if someone knows  how a car works, we might expect that knowledge to apply,  at least in part, to tractors because most vehicles work in a  broadly similar way utilizing broadly similar internal  components. Here we investigate whether children think  that mechanistic knowledge about basic level categories  (e.g. cars, clocks, and smartphones) generalizes to  knowledge about their superordinate level categories (e.g.  vehicles, machines, and electronics).  
In the current study, children (ages 6 to 9 years old) and  adults were presented with two twins, one possessing  mechanistic knowledge about a basic level category (e.g., clocks) and another possessing factual non-mechanistic  knowledge about that category. Participants were then asked  which twin knew more about its superordinate category  (e.g., machines), a subordinate category (e.g., grandfather  clocks), and an unrelated basic level category (e.g., tulips). A superordinate category was included as the key measure: differentially generalizing mechanistic knowledge from a  
1500
basic level category to a superordinate level category would  suggest that mechanistic knowledge applies more broadly to  kinds within that superordinate category compared to non mechanistic knowledge about the same basic level category. A subordinate category was included to assess the scope of  the generalization inferences: differentially generalizing  mechanistic knowledge from a basic level category to a  subordinate level category would imply that mechanistic  knowledge applies to specific kinds within that basic level  category, an inference that might appeal to young children if  they see mechanistic knowledge as generalizing more  strongly to all instances of the category. Not selectively  generalizing mechanistic knowledge to the subordinate level  would reveal a more nuanced view about mechanistic  knowledge’s generalizability. An unrelated basic level  category was included as a control: not generalizing  mechanistic knowledge to an unrelated category ensures that  participants did not perceive the mechanistic knower as  more intelligent or knowledgeable in general.  
Our hypotheses were: 
1. Participants across all ages would judge the twin  possessing mechanistic knowledge as more knowledgeable about the superordinate category than the twin possessing  factual non-mechanistic knowledge, showing that  mechanistic knowledge about a basic level category is generalized to its superordinate level category more than  factual knowledge. 
2. Participants across all ages would not judge the  mechanistic twin to know significantly more about the  unrelated basic level category. This result would clarify  evidence in support of our first hypothesis, demonstrating 
that mechanistic knowledge does not simply imply greater  knowledge in general, but rather about the superordinate  category in particular, and also that participants do not  perceive the mechanistic twin as merely more intelligent or  knowledgeable in general. 
 3. Participants across all ages would not judge the  mechanistic twin to know significantly more about the  subordinate level category. This result would further clarify  evidence in favor of our first hypothesis, demonstrating that  participants perceived the mechanistic and non-mechanistic 
twins as possessing approximately the same amount of knowledge about categories below the basic level category.  
Experiment 
Method 
Participants Twenty-one 6- and 7-year-olds (11 male,  mean age 85 months) and twenty 8- and 9-year-olds (8  male, mean age 106 months) participated in the experiment  via TheChildLab.com online platform (Sheskin & Keil,  under review). On this platform, researchers can engage in  online videoconferences with participants on a web-enabled device. The study stimuli are presented as a PowerPoint 
presentation shared within the videoconference, and  sessions begin with simple warm-up activities (e.g.  following a ball through a tube), established as easy for  
1501
most participants in previous research. Thirty-nine adults  participated in the experiment via Amazon Mechanical Turk  for $.50 payment; thirty-one adults (20 male, mean age 32  years) passed all attention checks and were included in the  final sample. 
Materials Three stimulus categories were used, with each  matched to a different superordinate category, subordinate  category, and unrelated basic level category. The three  stimulus/superordinate/subordinate/unrelated sets were  clocks, machines, grandfather clocks, tulips; cars, (wheeled) 
vehicles, racecars, sharks; smartphones, electronics,  iPhones, tigers. Each category was presented with an image  depicting the category, consisting of 6 category exemplars  in a white square (to emphasize kind rather than token).  These categories were chosen because they represented a  broad sample of artifacts familiar to most children.  
Design Each question was focused on which one of two  cartoon children knew more about a kind. Each pair were  introduced as twins, and looked nearly identical except one  twin wore blue clothes and the other wore green clothes.  The twins were referred to by the color of their clothes (i.e.,  as “Blue” or as “Green”). The blue twin was always  described first for each stimulus category, but the blue  twin’s knowledge type (mechanistic or non-mechanistic)  was counterbalanced across participants. The test categories (superordinate, subordinate, side) were presented in a  consistent order for a given participant across all three 
stimulus items, but order was counterbalanced with either  the superordinate category being presented first and the  subordinate category last, or vice-versa. The order of the  stimulus items was randomized across participants. The  study took approximately 8 minutes for children and 5  minutes for adults. 
Procedure At the start of the activity, children were  presented with a training example that introduced the  concept of a yellow equal sign, which would be used in the  activity. They were then trained on how to give an answer in  the activity, saying “blue” if they chose the blue twin  (always on the left), “green” if they chose the green twin  (always on the right), and “yellow” if something applied to  both twins the same (the yellow equals sign was always  shown between the twins). A “same” choice was included  because it could reflect a genuine preference, especially for the unrelated category. Adults did not complete this training  and were instead instructed “in this survey, you are going to  hear about pairs of twins who both read a book about the  same topic, so they both learn a lot of information about the  same thing. However, the books they read are different, so  they each learn different information about the same thing.  You will hear about the kind of things that each twin learns,  and then your job is to decide who knows more about some  different things.” Participants were then introduced to both  twins and told that each twin read a corresponding colored  book, which were identical besides the color of their covers.  Both books were about the same stimulus category, so they  both learned lots of things about that category, but the books  were different so each twin learned different kinds of things  
about it. Participants were told one twin learned about how  the category works (the mechanistic twin), and the other  twin learned facts about the category (the non-mechanistic  twin). They were also given two examples of each twin’s  mechanistic or non-mechanistic knowledge (see Table 1).  
Categorical Level Age Group 
Subordinate Superordinate Unrelated 
Participants were then presented with a test category and  asked which twin knew more about the category, or whether  they thought the twins knew the same amount about it. For  example, children might be asked: “Here are some  machines. Who do you think knows more about machines?  Blue who knows about how clocks work, Green who knows  
1 (6-7) M = .619 p = .056 
2 (8-9) M = .1 p = .748 
3 (adults) M = .452 p = .232 
M = .857 p = .007* 
M = .9 
p = .014* 
M = 1.645 p < .001* 
M = -.714 p = .015* 
M = -.4 
p = .104 
M = .032 p = .813 
facts about clocks, or Yellow, do you think they know about  the same amount?” Children were asked about each test  category in sequence, with each test category consisting a  different slide. Adults were presented with all three test  categories in sequence on the same page. Participants  completed this procedure for each stimulus category. 
Stimulus Mechanistic Knowledge Non-mechanistic Knowledge 
Table 2: Mean scores and p-values for one sample t-tests  comparing knowledge attributions across each category level and age group to chance (* indicates p < .05) 
Bonferroni-adjusted post hoc comparisons were also conducted to investigate how knowledge attributions at each  categorical level differed from each other for each age  group. For age group 1, mechanistic scores differed  
Clocks For example, she learned what  makes the parts of the clock  
move. As another example, she  
learned how clocks can keep  
working for years without  
stopping. 
Cars For example, he learned how car  engines make the car move. As  
another example, he learned how  
cars’ brakes make the tires stop  
spinning. 
Smartphones For example, he learned how  smartphones’ screens recognize  
your fingerprint. As another  
example, he learned how  
smartphones are able to make  
many different kinds of sounds. 
For example, she learned where  clocks were first invented. As  another example, she learned  how many clocks are made  every year. 
For example, he learned where  the first car engines were built.  As another example, he learned  how many different companies  make tires for cars.  
For example, he learned what  kinds of glass smartphones’  screens are made out of. As  another example, he learned  how many different ringtones  are available for smartphones. 
significantly between the superordinate and unrelated  categories (p = .011), and between the subordinate and  unrelated categories (p = .023), but not between the  superordinate and subordinate categories (p = 1.0). For age  group 2, mechanistic scores differed significantly between  the superordinate and unrelated categories (p = .025) and  marginally between the superordinate and subordinate  categories (p = .085), but not between the subordinate and  side categories (p = .785). For age group 3, mechanistic  scores differed significantly between the superordinate and  unrelated categories (p < .001), and between the  superordinate and subordinate categories (p = .045), but not  
Table 1: Knowledge examples 
Results 
No children were excluded, yielding a sample of 21 6-7 year  olds (age group 1), 20 8-9 year olds (age group 2), and 31  adults (age group 3). For purposes of analysis, choosing the  mechanistic twin was coded as 1, the non-mechanistic twin  as -1, and knowing the same as 0. Scores were aggregated  
between the subordinate and unrelated categories (p = .821). 
2.5  
2  
1.5  
  
e
r
1  
o
c
across stimulus items for each categorical level, yielding a  
S
 
c
i
0.5  
superordinate level score, a subordinate level score, and an  
t
s
i
unrelated category score, which could range from -3 to 3. A  
n
a
0  
h
repeated measures ANOVA was conducted with category 
c
e
-0.5  
level (superordinate, subordinate, and unrelated) as repeated  M
measures factors and age as a between subjects. A main  
-1  
effect of category level, F(2, 138) = 19.04, p < .001, η2 =  
.21 was found, along with an effect of age, F(2, 69) = 3.26,  
-1.5  
p = .045, η2 = .086. There was no significant interaction  
6 & 7  
8 & 9  
Adults  Subordinate Superordinate Unrelated  
Categorical level  
between age and category level. One sample t-tests (two tailed) were therefore conducted to compare knowledge  attributions at each categorical level to a chance value of 0 for each age group (see Table 2). 
Figure 1: Study 1 mechanistic scores by categorical level and age group. Error bars indicate standard error. 
Discussion 
In support of our first hypothesis, each age group judged the mechanistic twin as more knowledgeable than the non mechanistic twin about the superordinate level categories. In  
1502
support of our second hypothesis, no age group judged the  mechanistic twin as more knowledgeable than the factual  twin about the unrelated category. Our third hypothesis was  mostly supported: no age group judged the mechanistic twin  as more knowledgeable than the factual twin about the  subordinate level category, although the youngest age group  approached significance. Knowledge attributions at the  superordinate and subordinate levels also did not differ  significantly from each other for the youngest age groups,  although scores at the superordinate level still exceeded those at the subordinate level for both age groups. In short,  children and adults recognized that knowing how a category  works implies broader knowledge within the same domain,  but not greater knowledge about unrelated categories or, to  an extent, the category itself. 
Preferences for the unrelated category shifted across age  groups, with the youngest children holding a non mechanistic preference, the older children a comparatively  weaker non-mechanistic preference, and adults possessing  no preference. While children may have chosen the non mechanistic twin strategically to balance knowledge  attributions (i.e., from a fairness motivation to choose each  twin some of the time), this might also reflect a legitimate  preference. 
There was also a developmental trend in subordinate level  preferences. Younger children chose the mechanistic twin  for the subordinate level almost as much as they did for the  superordinate level. In contrast, older children and adults  demonstrated a sizably weaker mechanistic preference at the  subordinate level compared to the superordinate level. Thus,  young children appear more optimistic about generalizing  broad mechanistic knowledge to more specific instances of  a category, an assumption that declines with age. This may  reflect young children’s lack of experience with specific  mechanisms. As they begin to encounter them in school,  children’s intuitions about causal mechanisms become  increasingly detailed and diverse, potentially weakening the  perceived similarity between broad mechanistic knowledge  and the mechanisms than obtain in specific kinds.  Alternatively, this developmental trend might be due to  children starting off with relatively less confidence that  factual non-mechanistic information generalizes downward,  and then an increasing awareness of this type of  generalization with age. 
In sum, the goal of this study was to investigate whether even young children appreciate that mechanistic knowledge  generalizes beyond its immediate scope. We found evidence  that children and adults selectively generalize mechanistic  knowledge to immediate superordinate level categories, while still being aware of its limits within and across domains. 
General Discussion 
The current study provides evidence that children and adults  attribute more knowledge about related kinds to individuals possessing mechanistic knowledge compared to factual non mechanistic knowledge. In particular, we find evidence that  
knowing how a basic level category works implies more  superordinate category knowledge than simply knowing  facts about it. This pattern suggests that a sense of shared  mechanism among members of a kind guides children and  adults’ inferences about the knowledge others possess. 
The study compared broadly mechanistic knowledge with  factual non-mechanistic knowledge. One concern is that  mechanistic knowledge was pitted against a particularly  weak or idiosyncratic variety of non-mechanistic  knowledge. To that end, the example pieces of knowledge  varied widely and were chosen to give a broad coverage of  factual non-mechanistic knowledge. Particular care was  taken to avoid knowledge of surface features (such as color  or size) that could be learned through mere observation.  Instead, the facts concerned unobservable traits like history  (e.g. where the first car engines were built) and constitution  (e.g. what kinds of glass smartphones’ screens are made out  of). Also, if a particular component or topic was mentioned  in a mechanistic example, it was also mentioned in the  corresponding non-mechanistic example to minimize a mere  bias for knowledge about internal parts. Importantly, the  vignettes were explicitly labeled as example pieces of knowledge, and were meant to broadly indicative the kind  of knowledge each twin possessed rather than specify it  exactly. This was further implied when each twin was  directly stipulated to have learned ‘a lot’ about the same  basic level category. 
A related concern is that mechanistic and non-mechanistic  information is not truly dichotomous, either theoretically or  cognitively. However, contemporary philosophers of  science have offered broadly converging accounts (Bechtel,  2011; Craver & Darden, 2013). These accounts lay out  several key features of mechanistic explanation in the  sciences, including: a phenomenon being explained (e.g. 
how does it work?); a division of components, often  functional, that underlie the phenomenon; a set of causal  relations that obtain between the components, forming a  bounded system; hierarchical organization of components  via constitution, such that components can be unpacked into  constituents and their interactions at a lower level. More  colloquially, mechanistic explanations typically answer  “how” and “why” questions and are compactly described as  “how something works”. 
A selective preference for the mechanistic at the  superordinate level shows that children and adults are  capable of making distinct judgments about mechanistic  knowledge when pitted against non-mechanistic factual  knowledge, and that these judgments are reasonably  bounded. Importantly, however, the distinction between  mechanistic and non-mechanistic information is not always  clear in everyday life. The two occur together in most  contexts, and it is difficult to fully isolate a mechanism from  relevant non-mechanistic specifications. For example, the  size of a jet engine’s parts directly impact how it works.  However, the consistent mechanistic preferences at the  superordinate level, despite an option to judge both  knowledge types as equal, suggests that distinguishing  
1503
mechanistic from non-mechanistic knowledge is relatively  easy and consequential when accompanied by a minimal  level of supporting detail.  
One limitation of these studies is that all stimulus items  were artifacts, narrowing our findings to that domain.  Artifacts were used for two reasons: first, mechanism is  particularly salient in artifacts, often envisioned as the inner  “clockworks” of objects (Dolnick, 2011); second, no  colloquial phrase exists in English that signifies mechanistic  knowledge about biological entities, in contrast to “how it  works” for artifacts. Although, there is reason to suspect  children generalize mechanistic knowledge across biological  entities as well. Even infants believe the insides of living  things have privileged causal powers (Newman et al, 2008),  and by age six, children are able to make a variety of  abstract inferences about an object’s internal features (Ahl  & Keil, 2017). Taken together, these findings suggest some  form of awareness of internal biological mechanisms by the  early elementary school years. However, the biological  domain may suffer from specific limitations; in particular,  children’s, and even adults’, intuitions about plants are  notably weak and delayed compared to their intuitions about  animals (Stavy & Wax, 1989; Hatano & Inagaki, 1994).  This might lead children to undervalue mechanistic  knowledge about plants. Further research should test  children’s mechanistic intuitions across a broad range of  familiar biological entities, including animals, plants, and  organs. 
A final limitation is that, given children’s limited  attention, the current studies did not feature multiple  superordinate level categories for a single stimulus. As a  result, it is unclear exactly how distant two categories within  a domain need to be for mechanistic knowledge about one  to no longer generalize to the other.  
The scope of mechanistic knowledge is likely determined  by the same intuitions that lead to it generalizing to related  kinds, namely a sense of shared mechanism amongst those  kinds. But what are these mechanistic intuitions like? How  concrete and detailed are they? How do we acquire them  and how do they change over time? The current studies do  not address these questions, since the aim was to show that  mechanistic knowledge generalizes, not what our  representations of mechanisms are like. To some extent,  these representations are idiosyncratic by nature, dependent  on one’s concrete mechanistic knowledge and experiences  with particular instances of a kind. However, given the  relatively consistent pattern of responses in this study across  all age groups, these representations may share fundamental  features or structure in common. Future study of these  common features could shed light on the nature of epistemic  inference and conceptual cognition more broadly. 
Conclusion 
The current study examined how children and adults  generalize mechanistic knowledge. The results suggest that  even young children recognize that mechanistic knowledge  about a basic level category implies greater knowledge  
about its superordinate level category compared to factual  non-mechanistic knowledge about the same basic level  category. This provides one account of why intuitions about  causal mechanism have such a strong influence on children  and adults’ epistemic inferences. Impressively, children are  able to make systematic inferences about mechanistic  knowledge despite possessing little to none themselves,  suggesting that abstract features of mechanism, rather than  concrete mechanistic details, are chiefly responsible for  mechanism’s influence on children and adults’ intuitions. 
Acknowledgments 
The authors thank the National Science Foundation who  funded this research (proposal DRL 1561143 to F. Keil) as  well as the members of the Yale Cognition and  Development Lab for their helpful feedback. 
References  
Ahl, R. E., & Keil, F. C. (2017). Diverse effects, complex  causes: children use information about Machines'  functional diversity to infer internal complexity. Child  Development, 88(3), 828-845. 
Bechtel, W. (2011). Mechanism and biological explanation.  Philosophy of Science, 78, 533- 557. 
Callanan, M. A., & Oakes, L. M. (1992). Preschoolers'  questions and parents' explanations: Causal thinking in  everyday activity. Cognitive Development, 7(2), 213-233. 
Chouinard, M. M., Harris, P. L., & Maratsos, M. P. (2007).  Children's questions: A mechanism for cognitive  development. Monographs of the Society for Research in  Child Development, i-129. 
Corriveau, K. H., & Kurkul, K. E. (2014). “Why does rain  fall?”: Children prefer to learn from an informant who  uses noncircular explanations. Child Development, 85(5),  1827-1835. 
Craver, C. F., & Darden, L. (2013). In search of  mechanisms: Discoveries across the life sciences.  University of Chicago Press. 
Dolnick, E. (2011). The Clockwork Universe: saac Newto,  Royal Society, and the Birth of the Modern World I.  Harper Collins. 
Erickson, J. E., Keil, F. C., & Lockhart, K. L. (2010).  Sensing the coherence of biology in contrast to  psychology: Young children’s use of causal relations to  distinguish two foundational domains. Child  Development, 81(1), 390-409. 
Frazier, B. N., Gelman, S. A., & Wellman, H. M. (2016).  Young children prefer and remember satisfying  explanations. Journal of Cognition and Development,  17(5), 718-736. 
Hatano, G., & Inagaki, K. (1994). Young children's naive  theory of biology. Cognition, 50(1-3), 171-188. Johnston, A. M., Sheskin, M., Johnson, S. G., & Keil, F. C.  (2017). Preferences for Explanation Generality Develop  Early in Biology But Not Physics. Child Development. Keil, F. C., Lockhart, K. L., & Schlegel, E. (2010). A bump  on a bump? Emerging intuitions concerning the relative  
1504
difficulty of the sciences. Journal of Experimental  Psychology: General, 139(1), 1. 
Keil, F.C., Stein, C., Webb, L., Billings, V.D., & Rozenblit,  L. (2008). Discerning the Division of Cognitive Labor:  An Emerging Understanding of How Knowledge is  Clustered in Other Minds. Cognitive Science, 32(2), 259- 
300. 
Kushnir, T., Vredenburgh, C., & Schneider, L. A. (2013).  “Who can help me fix this toy?” The distinction between  causal knowledge and word knowledge guides  preschoolers' selective requests for information.  Developmental Psychology, 49(3), 446. 
Legare, C. H., & Lombrozo, T. (2014). Selective effects of  explanation on learning during early childhood. Journal  of Experimental Child Psychology, 126, 198-212. 
Lockhart, K. L., Chuey, A., Kerr, S., Keil, F. C. (under  review). The Privileged Status of Knowing Mechanistic  Information: An Early Epistemic Bias. 
Newman, G. E., Herrmann, P., Wynn, K., & Keil, F. C.  (2008). Biases towards internal features in infants’  reasoning about objects. Cognition, 107(2), 420-432. 
Sheskin, M. & Keil, F. C. (under review).  TheChildLab.com: A video chat platform for  developmental research. 
Stavy, R., & Wax, N. (1989). Children’s conceptions of  plants as living things. Human Development, 32(2), 88- 94. 
Trouche, E., Chuey, A., Lockhart, K. L., & Keil, F. C.  (2017). Why Teach How Things Work? Tracking the  Evolution of Children’s Intuitions about Complexity.  Proceedings of the 39th Annual Conference of the  Cognitive Science Society (pp. 3368-3373). London,  England, UK: Cognitive Science Society. 
Trouche, E., Chuey, A., Lockhart, K. L., & Keil, F. C.  (under review). Why Teach How Things Work? Hidden  Benefits of Exposure to Mechanistic Explanation 
Walker, C. M., Lombrozo, T., Legare, C. H., & Gopnik, A.  (2014). Explaining prompts children to privilege  inductively rich properties. Cognition, 133(2), 343-357. 
Walker, C. M., Lombrozo, T., Williams, J. J., Rafferty, A.  N., & Gopnik, A. (2017). Explaining constrains causal  learning in childhood. Child Development, 88(1), 229- 246. 
1505
Multiple anchors and the MOLE: Benefits for elicitation  
Marianne H. Clausen (marianne.clausen@adelaide.edu.au)  University of Adelaide, SA 5005, Australia  
Matthew B. Welsh (matthew.welsh@adelaide.edu.au)  
University of Adelaide, SA 5005, Australia  
Abstract  
Anchoring is a well-known, robust effect causing estimates to  be biased towards previously seen values – regardless of their  relevance. Reducing anchoring bias is important for optimizing  estimation. Herein, we tested the MOLE (More-Or-Less  Elicitation) tool’s ability to limit the impact of anchors on  estimates. In a direct elicitation task, 62 participants’ best  estimates correlated with anchor values at 0.27 whereas, when  using the MOLE, this relationship disappeared (r = .02).  Results also showed, however, that expertise reduces the  impact of anchoring (r = -0.46). We conclude that use of the  MOLE assists in avoiding anchoring and that this will be most  helpful in areas of high uncertainty.  
Keywords: anchoring; elicitation; accuracy; expertise;  decision making; repeated judgement, MOLE.  
In the face of uncertainty, industry and government often rely  on estimates made by experts to guide decisions – converting  their beliefs into useable, numerical forms via a range of  processes labelled ‘elicitation’ (Wolfson, 2001). Such  estimates are useful, but also prone to systematic errors,  known as biases (Kahneman, Slovic, & Tversky, 1982). One  particularly robust bias arises from ‘anchoring’: the tendency  for people to base estimates on numbers to which they have  recently been exposed, regardless of their relevance (Tversky  & Kahneman, 1974). Tversky and Kahneman (1974)  famously demonstrated this by asking people to estimate the  percentage of African countries in the United Nations, after  judging whether it was higher or lower than a supposedly  randomly selected value (of 10 or 65%). The group who saw  the initial value of 10 provided a median response of 25,  while those seeing 65 provided a median response of 45. This  and numerous other studies have demonstrated the effect of  initial values on subsequent estimates (for a recent review,  see Furnham & Boo, 2011).  
There are two well-known theories for mechanisms  underlying anchoring. First, Kahneman and Tversky (1974)  describe it as an anchoring and adjustment process: the  anchor providing a starting value from which a person adjusts  until they reach a value that they believe is reasonable.  Therefore, the process results in people selecting amongst  those values closest to the anchor - from the range they  consider feasible. The second theory of anchoring is  ‘selective accessibility’, or priming, where the initial  approach is to consider whether or not the anchor itself is the  true value (Mussweiler & Strack, 1999). If the anchor is  determined not to be the true value, a person will search their  memory for relevant clues as to what the true answer might  be, but the starting point of this search is determined by the  
region in which the anchor falls. Both theories have support  in the literature and both may contribute to the robust  tendency for estimates to be skewed towards an initial value  (see, e.g., Furnham & Boo, 2011).  
Anchoring and Expertise  
Given our reliance on expert opinion to reduce our  uncertainty in most industries and fields of endeavor, the  extent to which anchoring is reduced by particular expertise  is, logically, of considerable importance; biases are likely to  reduce the accuracy of expert estimates and thus erode the  quality of decisions made based on their opinions.  
While it might seem reasonable that people with greater  expertise would produce more accurate estimates - with the  effect of anchoring minimised as they rely on experience and  knowledge in preference to the anchor - there is mixed  evidence regarding anchoring and expertise. Some studies  have found that people higher in knowledge (Wilson et al,  1996) or those more certain of their responses (Chapman &  Johnson, 1994) were less affected by anchoring but others  have demonstrated anchoring in subject matter experts  (Mussweiler, Strack & Pfeiffer., 2000; Northcraft & Neale,  1987). That is, the common observation is that expertise may  reduce but does not eliminate bias arising from anchoring.  
More-or-Less Elicitation  
More-or-Less Elicitation (MOLE) is a computerised  elicitation tool, which produces a range for the values an  unknown parameter might take, in a manner designed to  decrease both overconfidence in those ranges (i.e.,  ‘overprecision’ as per Moore & Healy, 2008) and the effects  of anchoring (Welsh & Begg, 2018; Welsh, Lee, & Begg,  2008, 2009). The basic, MOLE process (described in detail  in the Method selection, below) presents users with two  random values. The user indicates which of these they  believe is closer to the true value, with the process being  repeated numerous times. The MOLE incorporates four key  insights to achieve reduced overconfidence and anchoring.  
First, instead of making absolute judgments, the MOLE  allows people to make relative judgements, which have long  been known to be both easier and more accurate (see, e.g.,  Stroop, 1932; Miller, 1956). Instead of directly asking for  estimates, respondents simply select which of two presented  values they believe is more likely/closer to the truth.  
Second, the MOLE uses repeated judgements, the average  of which have been shown to be more accurate than single  estimates (see, e.g., Herzog & Hertwig, 2009, Vul & Pashler,  
1506
2008). This allows multiple pairs of values to be presented  for each value being estimated, enabling repeated judgements  of the same value while preventing participants simply  repeating answers, thus meeting the criteria for useful,  repeated judgements from an individual (see, e.g., Herzog &  Hertwig, 2009, Vul & Pashler, 2008).  
Third, the MOLE retains portions of the possible range that  users are uncertain of. Values are only excluded when the  respondent’s judgements indicate they are certain that values  are not possible. This contrasts with more typical elicitation  processes where the respondent decides which values to  include – a process that is likely to result in them stopping  their search for values once they move beyond those they are  certain should be included and is thus likely to underestimate  their true uncertainty (an explanation that echoes the  explanation for anchoring invoked by Kahneman, 2011).  
Fourth (and key herein), the process is expected to reduce  the effect of any ‘anchor’ seen prior to the elicitation process  – simply because the MOLE requires that the respondent  consider many randomly selected values, thereby preventing  them from focusing on that anchor – in line with the advice  on best practice for avoiding anchoring (see, e.g., Kahneman,  Lovallo & Sibony, 2011). Additionally, the values presented  by the MOLE – being randomly selected – are likely to  include values that would not otherwise have been considered  by the user. This strategy, of considering alternative or  contradictory information has been shown to reduce  anchoring (Mussweiler et al., 2000; Russo & Schoemaker,  1992). When considered in light of the two theories of  anchoring introduced earlier, it appears that the MOLE  should, therefore, overcome the effect of anchoring,  regardless of the underlying mechanism. That is, the MOLE  provides many different values from which a user could begin  adjusting, and all of these values may prime the user to  consider them as the true value. The presence of numerous  potential anchors should, therefore, prevent bias caused by a  focus on any singular value.  
Previous studies have demonstrated the benefits of the  MOLE in reducing overconfidence and improving estimates  in perceptual, epistemic and forecasting tasks (Clausen,  2017; Welsh & Begg, 2018; Welsh et al., 2008, 2009). Welsh  and Begg (2018) also demonstrated that the initial values  presented by the MOLE itself do not act as anchors.  However, the tool’s efficacy in preventing anchoring on a  specific, prior value has, as yet, not been directly tested.  
Aims  
The primary aim of this research is to expand upon previous  studies (Welsh & Begg, 2018; Welsh et al., 2008, 2009),  which have investigated the MOLE with regards to accuracy  and overconfidence, by testing the MOLE’s ability to  overcome the effect of anchoring. Previous work (e.g., Welsh  & Begg, 2018) has made the logical assumption that the  MOLE overcomes the effect of anchors, yet this theory has  not been directly tested. Secondly, the effect that anchoring  has on the best estimates participants produce will be  assessed - to confirm whether anchoring is having a  
detrimental effect. While a detrimental effect seems logical,  anchors may provide clues and improve performance in the  case of low subject matter knowledge. Finally, the  relationship between participant knowledge and anchoring  will be examined – adding to the literature on differential  effects of anchoring resulting from subject matter expertise.  
Method  
Participants  
The study recruited N = 62 participants (38 females and 24  males) aged between 18 and 65 years (M = 31.15, SD =  12.81). One participant was excluded from the study due to  inappropriate responding. Participants were recruited within  the University of Adelaide and from the wider population.  Participation was, initially, encouraged by course credit (1st Year Psychology students, n = 14) or the opportunity to win  a $100 gift card (n = 17). To speed recruitment, later  participants were offered a $20 gift card (n = 31).  
Materials  
The study consisted of the various measures and elicitation  tasks (described below) incorporated into a single program,  created by the first author using Visual Basic for Applications  (VBA) in Microsoft Excel.  
Demographics Participants were asked to provide  information about themselves, including age, gender and  their engagement with Australian Rules Football (ARF).  Engagement with ARF was measured using four questions,  such as how often participants played ARF or watched ARF  matches. Participants rated their frequency of engagement  for each of four questions on a four-point Likert scale, with a  response of one corresponding to ‘rarely or never’ and four  corresponding to ‘more than once a week’. Possible scores  for engagement with ARF thus ranged between 4 and 16 with  higher scores indicating greater engagement. 
Individual Differences A number of measures were  included in the study as possible covariates of performance  on the task or anchoring susceptibility. Specifically: Need for  Closure (Webster & Kruglanksi, 1994; linked to range width  in elicitation tasks by Kaesler, Welsh & Semmler, 2016); the  2-item Openness and Conscientiousness measures from the  Ten Item Personality Inventory (TIPI; Gosling, Rentfrow &  Swann, 2003; Openness having been linked to anchoring  susceptibility by McElroy & Dowd, 2007); and the full-scale  Openness and Conscientiousness measures from the NEO 
FFI (Costa & McCrae, 1992) for comparison. However, no  significant results were obtained for these and, as a result,  they are not discussed further herein.  
Experimental Tasks The study utilised forecasting  questions, specifically related to Australian Football League  (AFL) match results (NB: Australian Rules Football – ARF -  is the sport and AFL is the national league). AFL results were  chosen as the forecasting measure because they provided a  
1507
sufficiently large set of numerical values of similar difficulty  to predict. Participants were asked to consider the total points  that a team would score in a specific game occurring within  the next two rounds of the competition. For example: “What  will be the total number of points that the St Kilda Saints  score when they play the North Melbourne Kangaroos on  Sunday, the 20th of August?”. Given the study was  conducted in Adelaide, the Adelaide-based AFL teams were  excluded to limit the impact of specialist knowledge. The  study included two methods of eliciting both a best estimate  and the range that the participant was confident that the true  value would fall within – the MOLE and a direct elicitation  task (described below). The study elicited responses from  participants for five questions under each condition.  
Anchoring Questions In each case, participants were  initially presented with an anchoring question. The anchoring  question asked if a specific team would score more/less than  a given value (20, 90, 160, 230 or 300), which were linearly  distributed over the MOLE’s initial range of 0-300. For  example, “Will the Collingwood Magpies score less than 160  total points when they play the Geelong Cats on Saturday, the  19th of August?”, where ‘160’ is the anchoring value.  
Only after responding to this anchoring question would the  participant be asked to answer the corresponding question:  (e.g.) “What will be the total number of points that the  Collingwood Magpies score when they play the Geelong Cats  on Saturday, the 19th of August?” using one of the two  methods described below.  
MOLE At each of 10 iterations within the MOLE process,  participants were presented with two values randomly  selected from a pre-determined widest possible range (for this  study, 0 to 300 points). Participants were then asked to  indicate which they believed would be closer to the true value  by adjusting a slider from the default centre position  (indicating the participant believed the values presented were  equally likely as per the user interface shown in Figure 1).  Adjusting the slider to the extreme left indicated that the  value displayed on the right was not deemed possible and  vice versa, whereas positions between the centre and the end 
points were mapped onto levels of confidence between 50%  and 100% in the selected value being closer to the true value.  If a participant indicated maximum (100%) confidence in  one of the values, the possible range was truncated at the  unselected value. For example, given an initial range of 0 to  300, if the values 95 and 240 were presented and the  participant selected 95 with maximum confidence, then this  would result in the range being truncated at 240 and the  options displayed in later iterations being drawn from the 0- 240 range rather than 0-300. This follows, logically, from the  descriptions within the GUI, where the ends of the confidence  scale explicitly rule out the alternative value. (NB - previous  studies using the MOLE truncated the range at the midpoint,  rather than the unselected value. This study adopted a more  conservative approach to range truncation in order to lessen  the chance of errors resulting in unrecoverable, overly narrow  
ranges as had sometimes been observed in previous work;  Welsh & Begg, 2018.)  
Where a participant’s confidence in their selected value  was less than complete, however, the range remained  unchanged as this was taken as evidence that the participant  believed that the alternative value still had some chance of  being closer to the true value.  
The remaining range after ten iterations of the MOLE was  recorded as the participant’s 100% confidence range – that is,  the range that they could be assumed to be 100% confident  would contain the true value. At each iteration, the  participant’s estimate of the true value was calculated from  their confidence and the difference between the values. For  example, equal (50%) confidence in the two values 100 and  200 would produce a best estimate of 150 whereas a 90%  confidence that 100 was closer to the true value than 200  produced an estimate of 110. A participant’s overall best  estimate was calculated by the MOLE simply as the average  of these estimates from each iteration.  
Figure 1: MOLE user interface  
Direct Elicitation The direct elicitation task required  participants enter a minimum value, a maximum value and a  best estimate for the number of points they believed the  specified team would score. The program checked that the  minimum value was lower than the best estimate, which in  turn had to be lower than the maximum; failure to meet these  requirements resulted in the program displaying an error  message, prompting the participant to revise their estimates.  Otherwise, these direct estimates of the end-points of the  range and the best estimate were simply recorded.  
Procedure  
After participants had registered their interest in the study,  they were emailed a copy of the VBA program. The program  included questions about the next two (weekly) rounds of  AFL matches. Therefore, the specific questions included in  the program changed on a weekly basis. Results were  collected between the 5th of June 2017 and the 19th of  
1508
August 2017. The study used a within-participants design,  with participants completing both elicitation tasks, allowing  for comparison of performance across the tasks.  
Participants were randomly assigned to complete either the  MOLE or direct elicitation task first, with the intention of  preventing order effects. While efforts were made to allocate  participants equally to the conditions, the number of  participants varied due to uptake and completion rates, with  the end result that 33 participants completed the MOLE task  first and 29 completed the direct elicitation task first.  
The survey began with an information page, where  participants indicated their agreement to participate. The  survey progressed through, in order, demographic questions  and individual difference measures, prior to beginning the  elicitation tasks and concluded with instructions to save the  file and return it to the researcher by email. Participants  could only move forwards through the tasks, once a response  was submitted it could not be altered.  
Results  
Dependent Measures  
The dependent measures of the study were defined and or  calculated for each participant within each elicitation  condition as follows, with descriptive statistics presented in  Table 1 alongside the independent measures age and  Engagement (described in the Demographics section above).  
Best Estimate. As described above, this was directly  estimated by participants for each of the five questions in the  Direct condition and calculated from their choices and  confidence in the five MOLE questions.  
Anchoring Score. The correlation between a participant’s  best estimates and the anchoring values across the five  questions within each condition.  
Accuracy Score. The correlation between a participant’s best  estimates and the true values across the five questions within  each condition.  
Error. The difference between a participant’s best estimates  and the actual value averaged across the five questions within  each condition.  
MOLE and Anchoring Given the typical anchoring effect,  it was expected that there would be a positive correlation  between anchor values and the best estimates participants  produced via direct elicitation. Repeated measures  correlation (rrm) was used due to there being five data points  for each participant. (Repeated measures correlation is  equivalent to Pearson’s correlation; however, it  accommodates multiple data points per participant,  increasing power, without violating the assumption of  independence. It evaluates overall intra-individual  relationships and can be calculated using a form of ANCOVA  using the rmcorr package in R; see Bakdash & Marusich,  
2017.) Analysis confirmed a small to medium, positive  correlation, rrm(247) = .27, p < .001, suggesting that higher  anchor values did, in fact, result in higher best estimates.  
Direct MOLECorrelation (RM) with 95% CI 
Table 1: Descriptive statistics  
Variable Mean Median Min Max SD  Age 31.2 26.5 18 65 12.8 Engagement 7.6 8 4 16 2.9 Anchoring Score 
 Direct .14 .24 -.96 .99 .50  MOLE .01 .01 -.96 .93 .51 Accuracy Score 
 Direct .02 -.10 -.92 .99 .54  MOLE .17 .26 -.96 .94 .50 Average Error 
 Direct 39.9 29.8 6.8 226.4 36.0  MOLE 46.3 36.9 8.1 120.4 26.6 
0.5 
0.4 
0.3 
0.2 
0.1 
0 
-0.1 
-0.2 
Elicitation Method
Figure 2: Correlations between anchors and best estimates by  condition.  
Additionally, it was hypothesised that anchoring would not  be evident in the MOLE task. Repeated measures correlation  analysis confirmed this with no correlation detected between  MOLE best estimates and anchor values, rrm(247) = .02, p =  .76. The correlations and their confidence intervals are  displayed in Figure 2, from which it can, from inspection of  the CIs, be seen that two correlations also differ significantly  from one another – providing further support for the linked  hypotheses that anchors would affect best estimates obtained  via Direct elicitation but not those generated by the MOLE.  
Anchoring and Accuracy The extent to which anchors  affected best estimates was assessed using two related  measures: accuracy score and error. Firstly, within the direct  elicitation condition, accuracy scores were expected to be  negatively correlated with anchoring scores. Analysis  confirmed a medium correlation of r(60) = -.33, p = .01. This  suggests that participants who were more affected by anchors  produced less accurate best estimates (when accuracy is  defined as the correlation between best estimates and actual  
1509
values, at the individual level). Secondly, error was expected  to be positively correlated to anchoring score in direct  elicitation. Analysis confirmed a large correlation of r(60) =  .44, p <.001. This indicates that participants who were more  affected by anchors produced best estimates that were further  away from actual values. Overall, the results indicate that  anchoring was associated with less accurate best estimates,  regardless of whether accuracy was measured by the average  error between best estimates and actual values, or the degree  to which best estimates correlated with actual values. Finally,  no significant correlations were seen between anchoring and  accuracy in the MOLE condition, which is to be expected as  anchoring was not evident in this condition.  
Anchoring and Knowledge It was anticipated that greater  knowledge (specifically, level of engagement with ARF)  would be associated with less anchoring in the direct  elicitation condition. Analysis confirmed a large negative  correlation, r(60) = -.46, p < .001, as shown in the scatterplot  below (Figure 3). That is, participants with greater  knowledge of the subject were less affected by anchors.  

Figure 3: Scatterplot of anchoring score versus engagement  with trendline.  
Discussion  
The results suggest More-or-Less Elicitation (MOLE) is  successful in overcoming the effects of anchoring. In the  direct elicitation condition, a typical anchoring effect was  seen; that is, estimates correlating with the anchor values seen  by participants. The same participants, however, showed no  evidence of anchoring when using the MOLE, supporting the  hypothesis that the large quantity of numbers presented to  MOLE users prevents anchoring on any specific value.  
In addition to detecting the presence of anchoring, the  study demonstrated that anchoring was associated with less  accurate estimates. This confirms the detrimental effect of  anchoring on estimation.  
The final component of the study was the investigation of  the relationship between knowledge and anchoring. The  positive correlation between accuracy and engagement and  the large negative correlation between the degree to which  
participants engaged with the subject matter and their  anchoring scores supports the common-sense expectation  that those with greater expertise produce better estimates and  are less likely to be led astray by anchors.  
Caveats and Future research  
The above conclusion is an encouraging finding for those  depending on estimates made by experts, but it needs to be  balanced with a key understanding. The processes of  anchoring (adjustment and priming) both imply that a  person’s degree of uncertainty limits the effect that an anchor  can have. Expert opinion is most valuable, however, where  uncertainty is highest and typical elicitation processes may,  thus, occur in situations more akin to lower levels of  knowledge where anchoring remains a problem.  
A related result requiring further comment is the negative  slope of the trendline (see Figure 3), which supports the idea  that knowledge reduces anchoring, but also implies that  greater knowledge could be associated with negative  anchoring – that is, more expert people reacting against the  influence of the anchor. If averaged for higher engagement  participants (Engagement ≥8), however, the anchoring values  tend to be close to zero (Engagement  8, M = -.03, Min = - .96, Max = .73, SD = .43). Additionally, at the lowest level  of engagement there is a high degree of anchoring  (Engagement = 4, M = .58, Min = -.17, Max = .99, SD = .30).  Thus, it seems likely that the study indicates only that a lack  of knowledge is associated with anchoring, whereas greater  knowledge is associated with less or no anchoring. Future  research with a larger sample could, however, directly test the  possibility of participant reactions against anchors.  
Inspection of the scatterplot of engagement versus  anchoring (see Figure 3) also indicates a cluster of  participants at 4 for engagement. This reflects participants  who responded “rarely or never” to all four engagement  questions. In hindsight, this category would have been better  divided into separate categories for “rarely” and “never” to  obtain a better spread of data.  
Table 1 indicates that average error is greater for the  MOLE than for direct elicitation. This result seems to  contradict the generally proposed benefits of the MOLE.  However, while the MOLE estimates tend to be further away,  they correlate more closely to actual values. This is likely  due to the estimates being located within a much wider range  of possible answers, with the wider ranges produced by the  MOLE being the key mechanism in reducing overconfidence  (Clausen, 2017). This highlights the benefit of calculating  error in terms of both magnitude and trend. While this result  is not detrimental to the current findings, future research  could investigate improvements to the algorithm that  calculates MOLE estimates.  
Conclusions  
Anchoring is a fundamental cause of bias in estimation and,  as such, a central concern during the elicitation of expert  opinion. The results support the idea that expertise can limit  the impact of anchors on estimates - with more  
1510
knowledgeable participants in the direct elicitation condition  making estimates that were both more accurate and less  affected by anchors. In contrast, estimates elicited using the  MOLE showed no relationship to the anchoring values,  regardless of participant expertise. That is, it seems the  MOLE process of providing multiple, paired choices washes  out the effect of any previously observed anchoring value in  both knowledgeable and naïve estimators – in addition to  reducing overconfidence as has been shown previously  (Welsh & Begg, 2018). Given this, the MOLE seems to  provide a better alternative to direct elicitation in situations  where anchoring and overconfidence biases are of concern.  
Acknowledgments  
The data in this paper are a subset of those collected for  MHC’s 2017 Honours thesis in Psychology at the University  of Adelaide. MBW’s position is supported by Australian  Research Council Grant LP160101460, which includes  support from industry partners Santos and Woodside.  
References  
Bakdash, J. Z., & Marusich, L. R. (2017). Repeated measures  correlation. Frontiers in psychology, 8, 456.  
Chapman, G.B., Johnson, E.J., 1994. The limits of anchoring.  Journal of Behavioral Decision Making 7, 223–242.  Clausen, M. H. (2017). Overconfidence and the MOLE:  Investigating the role of anchoring and individual  differences. Unpublished Psychology Honours thesis,  University of Adelaide.  
Costa, P. T., & MacCrae, R. R. (1992). Revised NEO  personality inventory (NEO PI-R) and NEO five-factor  inventory (NEO-FFI): Professional manual. Psychological  Assessment Resources, Incorporated.  
Furnham, A., & Boo, H. C. (2011). A literature review of the  anchoring effect. The Journal of Socio-Economics, 40(1),  35-42.  
Gosling, S. D., Rentfrow, P. J., & Swann Jr, W. B. (2003). A  very brief measure of the Big-Five personality domains.  Journal of Research in personality, 37(6), 504-528.  
Herzog, S. M. and R. Hertwig (2009). The wisdom of many  in one mind: improving individual judgments with  dialectical bootstrapping. Psychological Science 20(2):  231-237.  
Kaesler, M., Welsh, M. B., & Semmler, C. (2016). Predicting  overprecision in range estimation. In A. Papafragou,  Grodner, D., Mirman, D., & Trueswell, J.C. (Ed.),  Proceedings of the 38th Annual Conference of the  Cognitive Science Society. Austin, TX: Cognitive Science  Society.  
Kahneman, D. (2011). Thinking, Fast and Slow. New York,  NY: Farrar, Straus, Giroux.  
Kahneman, D., Lovallo, D., & Sibony, O. (2011). Before you  make that big decision. Harvard Business Review, 89(6),  50-60.  
Kahneman, D., Slovic, P., & Tversky, A. (1982). Judgement  under uncertainty: heuristics and bias. Cambridge:  Cambridge University Press.  
McElroy, T., & Dowd, K. (2007). Susceptibility to anchoring  effects: How openness-to-experience influences responses  to anchoring cues. Judgment and decision making, 2(1),  48-53.  
Miller, G. A. (1956). The magical number seven, plus or  minus two: some limits on our capacity for processing  information. Psychological Review 63: 81-97.  
Moore, D. A., & Healy, P. J. (2008). The trouble with  overconfidence. Psychological Review, 115(2), 502-517.  Mussweiler, T., & Strack, F. (1999). Hypothesis-consistent  
testing and semantic priming in the anchoring paradigm: A  selective accessibility model. Journal of Experimental  Social Psychology, 35(2), 136-164.  
Mussweiler, T., Strack, F., & Pfeiffer, T. (2000). Overcoming  the inevitable anchoring effect: considering the opposite  compensates for selective accessibility. Personality and  Social Psychology Bulletin, 26(9), 1142-1150.  
Northcraft, G. B., & Neale, M. A. (1987). Experts, amateurs  and real estate: an anchoring-and-adjustment perspective  on property pricing decisions. Organizational Behavior  and Human Decision Processes, 39, 84-97.  
Russo, E. J., & Schoemaker, P. J. H. (1992). Managing  Overconfidence. Sloan Management Review, 33, 7-17.  Stroop, J. R. (1932). Is the judgment of the group better than  that of the average member of the group? Journal of  Experimental Psychology 15(5): 550-562.  
Tversky, A., & Kahneman, D. (1974). Judgment under  uncertainty: Heuristics and biases. Science, 185, 1124- 1131.  
Vul, E. and H. Pashler (2008). Measuring the crowd within:  probabilistic representations within individuals.  Psychological Science 19(7): 645-647.  
Webster, D. M., & Kruglanski, A. W. (1994). Individual  differences in need for cognitive closure. Journal of  Personality and Social Psychology, 67(6), 1049.  
Welsh, M. B. & Begg, S. (2018). More-Or-Less Elicitation  (MOLE): reducing bias in range estimation and  forecasting. EURO Journal on Decision Processes,  https://doi.org/10.1007/s40070-018-0084-5.  
Welsh, M. B., Lee, M. D., & Begg, S. H. (2008). More-or Less Elicitation (MOLE): Testing a heuristic elicitation  method. In V. Sloutsky, B. Love, & K. McRae (Eds.), 30th  Annual Conference of the Cognitive Science Society.  Austin, TX: Cognitive Science Society.  
Welsh, M. B., Lee, M. D., & Begg, S. H. (2009). Repeated  judgments in elicitation tasks: efficacy of the MOLE  method. In N. Taatgen, H. v. Rijn, L. Schomaker, & J.  Nerbonne (Eds.), Proceedings of the 31st Annual  Conference of the Cognitive Science Society. Austin, TX:  Cognitive Science Society.  
Wilson, T. D., Houston, C. E., Etling, K. M., & Brekke, N.  (1996). A new look at anchoring effects: basic anchoring  and its antecedents. Journal of Experimental Psychology:  General, 125(4), 387-402.  
Wolfson, L. J. (2001). Elicitation of probabilities and  probability distributions. International Encyclopedia of the  Social Sciences. E. Science, Elsevier Science: 4413-4417.  
1511
The role of fast speech in sound change 
Uriel Cohen Priva (uriel cohen priva@brown.edu) Brown University 
Providence, RI 02912 USA 
Emily Gleason (emily gleason@alumni.brown.edu) Brown University 
Providence, RI 02912 USA 
Abstract 
Recent research has seen a surge in interest in the role of the individual in sound change processes. Do fast speakers have a unique role in sound change processes? Fast speech leads to greater rates of lenition (reduction). But should it mean that fast talkers would be more likely to lenite even when speak ing slowly? In two corpus studies we show that even when fast talkers speak more slowly they are (a) more likely to omit segments and (b) more likely to perform variable reduction of consonants. This draws attention to habitual speech rate as a likely factor in the actuation of lenition processes. 
keywords: lenition, speech rate, individual differences Introduction 
A growing field of research focuses on social factors that may contribute to language change (Labov, 2001; Milroy & Mil roy, 1985), including the influence of the individual. Studies of individual difference tend to revolve around different cog nitive aspects of speakers, including position on the autism spectrum (Yu, 2013), attention to linguistic variation within and outside of personal linguistic subgroups (Garrett & John son, 2013), and differences in perception (Ladd et al., 2013). Other studies correlate the physiological makeup of speakers as a bias toward particular sound change processes (Moisik & Dediu, 2015), such that some properties of speech could be acquired by individuals and become typical of their language use. Baker, Archangeli, and Mielke (2011) proposes that it is differing amounts of variability between speakers which leads to change when an accepted variation by one group is misinterpreted as a new target by another group. 
Individual speakers tend to be consistent in their behavior. In a study that focused on convergence in speech rate (Co hen Priva, Edelist, & Gleason, 2017), the correlation between the speech rate of a speaker in a given conversation and their speech rate in other conversations was very high (β=.79, pre dictors and predicted values were standardized). In contrast, speakers’ speech rate was only mildly correlated (convergent) with the speech rate of their interlocutors in other conversa tions (β=.05). Though speakers do converge with their inter locutors in speech rate, they are more consistent than conver gent or random. Similarly, high self-persistency was found in Cohen Priva and Sanker (2018) for other phonetic prop erties such as median pitch (self β=.97 vs. other β=0.018), pitch variability (self β=.68 vs. other β=0.092), and non phonetic properties such as uh to um ratio (self β=.79 vs. other 
β=.031).1 Speakers are therefore expected to “do what they always do”, even when the local context demands otherwise (e.g. to converge). Exemplar-based representation (e.g. Pier rehumbert, 2001) could explain consistency if speakers sam ple from their own performance, which has many instances of their behavior, biased in a particular direction. Alterna tively, consistency can follow from storage vs. computation considerations (O’Donnell, 2015) if speakers do not always compute the appropriate output, but instead sometimes reuse precomputed stored instances. 
Regardless of the actual underlying mechanism, consis tency in speech rate has surprising predictions for fine grained phonetic behavior: it suggests that fast speakers would repeat “fast speech patterns” even when speaking more slowly. Fast speech has been argued to predict higher rates of lenition (reduction2) at the local context, e.g. in Kirch ner (1998); Lavoie (2001); Gurevich (2004); Cohen Priva (2015). This implies that fast speakers will lenite more than slower speakers, everything else being equal. If fast speakers are repeating their fast speech patterns outside of the typical context, they would also lenite when speaking more slowly, which would implicate them as possible leaders in lenition type sound changes. Theories regarding individual differ ences in sound change such as Yu (2013) assume that actua tion can follow when listeners fail to interpret a particular ex emplar as following only from the context in which it appears. Listeners observing lenition in the context of fast speech can attribute such effects to the speed the speech was produced. If lenited forms are produced out of context, when fast speakers happen to speak more slowly, they may not be attributed to fast speech, which could then follow the pattern discussed in Yu (2013). In such cases fast speakers may function as ini tiators and promoters of new and existing lenition-type sound change processes.3 
We used the Buckeye corpus (Pitt et al., 2007) in several related studies to investigate the impact of fast speakers and 
1For all reported β values in Cohen Priva and Sanker (2018), predicted values and both predictors were standardized. 2Lenition is a poorly defined term surrounded by a signifcant amount of debate (see e.g. Honeybone, 2008), but we use it here as a short-hand for generally accepted reduction processes, i.e. degemi nation, voicing, spirantization, debuccalization, approximantization, flapping, and deletion. 
3Here we do not mean to refer to stable and established lenitions (such as e.g. tapping in intervocalic contexts in American English), but rather new or ongoing sound changes that involve reduction. 
1512
fast speech on non-phonologized obstruent lenition. First, we investigated deletion, which is evident in the absence of un derlying segments in the surface form, and has been used in the past in the Buckeye corpus (Cohen Priva, 2008, 2015, 2017a; Raymond, Dautricourt, & Hume, 2006). We then investigated lenition more broadly using a power difference estimation (Warner & Tucker, 2011). Both studies indicate that fast speakers do have a greater tendency to reduce seg ments, even beyond the effects of the speech rate within the individual phrase. Our findings thus highlight the complex role of individual speech rate in reduction processes. 
/b/ [b] 
/æ/ [ɛ] 
/k/ /s/ [z] 
Method and materials 
Corpus 
For all studies below we used the Buckeye Corpus of Conver sational Speech (Pitt et al., 2007). The Buckeye Corpus is a detailed corpus of American English composed of data from 40 speakers conversing with one of two interviewers at Ohio State University. Only the speaker side of each conversation was recorded. A small amount of demographic information about the speakers was recorded, including gender and age. Age was recorded as a binary value (whether a speaker was older or younger than 40 years of age). 
Following established procedure (Cohen Priva, 2015), phonemes were linked to their surface realization using a weighted edit-distance program, but unlike Cohen Priva (2015), the weights were learned from the data algorithmi cally using a variant of the EM algorithm. The algorithm minimized the perpelxity of aligning the entire Buckeye cor pus by assigining probabilities to every deletion, insertion, and substitution operation. Each individual underlying form was aligned to its corresponding surface form by, in essence, choosing the most probable alignment.4 For example, in a case where the word /bæks/ was produced as [b3z], the al gorithm would align /b/ with [b], /æ/ with [3], /s/ with [z], and regard /k/ as deleted, as shown in Figure 1. Underlying representations were taken from the CMU dictionary (Weide, 2008), as the Buckeye provided forms do not include stress information, which has been previously shown to be relevant to duration and deletion rates (e.g. Lavoie, 2001). Words that occurred less than 4 times in the data were omitted to allow convergence. 
Segments that were not aligned with an underlying form were considered to have been deleted. The intensity in each sound file was extracted from the corpus using Praat (Boersma & Weenink, 2008), and aligned to individual seg ments based on their beginning and ending durations in the corpus. This was done to compare how loud segments were relative to the preceding vowel. 
Speaker speech rate and phrase-level speech rate We followed the procedures used in Cohen Priva (2017b) to measure speech rate. The goal of this procedure is to abstract 
4A side-effect of this process is that mergers were analyzed as a deletion of the segment less likely to emerge as the aligned surface form. 
Figure 1: An example of the alignment of an underlying form with its surface representation 
away from contextual factors and the phonological makeup of words, and measure whether words are pronounced faster relative to their expected pronunciation. The expected dura tion of each word was defined as the prediction of a linear regression that used the mean duration of the word in every context and its contextual predictability (Bell, Brenier, Gre gory, Girand, & Jurafsky, 2009; Jurafsky, Bell, Gregory, & Raymond, 2001). Pointwise speech rate was defined as the actual duration of a word instance, divided by its expected duration. Therefore, if a word’s duration was predicted to be 250ms but was pronounced in 300ms, its pointwise speech rate would be 1.2 (slow), while if that word were pronounced in 200ms, its pointwise speech rate would be 0.8 (fast). Note that higher pointwise speech rates indicate slower articula tion. Speakers’ speech rates were then calculated as the geo metric mean of the pointwise speech rates of all content words they used in a conversation (function words defined as the list of words returned by the function stopwords in R’s tm package, Feinerer & Hornik, 2017). Function words were not included as they have been argued to be retrieved using a dif ferent mechanism than content words (Bell et al., 2009). 
All studies contrasted the effects of average speaker speech rate with phrase-level speech rate. Phrase-level speech rate was defined as average pointwise speech rate of all words in the phrase, except the pointwise speech rate of the word for which lenition was predicted. If a segment had been deleted from the word in question it would be trivially shorter, a con found between lenition and short duration. By considering both individual and phrase level speech rate we were able to compare how relevant an individual’s overall speech pat tern was for increased lenition compared to the previously observed effect of fast speech within a phrase or word it self. For each model the interaction between average speaker speech rate and phrase rate was considered as a variable but did not have any significant effect on the results, and therefore it is not included in the models reported here. 
Speech rate has been shown to depend on social factors, such as age and gender (e.g. Cohen Priva, 2017b; Jacewicz, Fox, & Wei, 2010; Quen, 2008; Yuan, Liberman, & Cieri, 2006). In data taken from the Switchboard and Buckeye cor 
1513
pora, men were found to generally speak faster than women (Cohen Priva, 2017b), despite the complete absence of evi dence that would indicate that men are better than women in language use. Age and gender are therefore included as con trols in the following studies. 
Study 1 
For Study 1, we studied the effect of speech rate on obstruent deletion, a non-phonologized lenition process in American English. Deletion of a segment is easily detected through the alignment of segments to their output (or lack thereof, in the case of a deleted segment) as described above in the descrip tion of the corpus and its alignment. In Study 1a, we looked specifically at intervocalic environment, which is typical for lenition. In Study 1b, we extended the result to post-vocalic (pre-consonantal) environment. 
Study 1a: Intervocalic deletion 
Materials and methods Log odds of deletion were pre dicted by a mixed effects logistic regression using ˜20,500 (intervocalic) segments. Log phrasal speech rate and log speaker average speech rate were both variables of interest. Age (binary), and gender were used as fixed effects. Seg ment, word, the phonological environment of the segment (no stress, secondary stress, or primary stress on either vowel), and speaker were included as random intercepts. All mod els presented here and below were fitted in R (R Core Team, 2017) using the lmerTest package (Kuznetsova, Bruun Brock hoff, & Haubo Bojesen Christensen, 2016), which encapsu lates the lme4 package (Bates, Machler, Bolker, & Walker, ¨ 2015). 
Results Age and gender had no effect and were removed from the model (model comparison p>.21). Faster phrasal speech rate was indeed correlated with higher propensity to delete (β=-0.27, SE=0.04, z=-6.725, p<10-10), consistent with current work on lenition.5 Fast individual-level speech rate also correlated with higher propensity to delete (β=- 0.32, SE=0.14, z=-2.215, p<0.03), suggesting an influence of the individual on lenition rates. The results are not due to collinearity between the two factors, as models that in cluded an interaction term between phrasal and individual speech rate were not significantly different (or better) than the model provided here, and did not affect the significance of the individual-level speech rate. 
Discussion High degrees of segment-deletion were known to correlate with fast speech (e.g. Cohen Priva, 2015), and many authors suggested that articulatory speed is a cause for lenition, perhaps by increasing the effort required by speakers (Kirchner, 1998), or by increasing the chances for hypoarticu lation (Bauer, 2008). However, the results of this study do not have to follow from previous findings: Speech rate is variable, and speakers could have omitted segments only when speak 
5The coefficient is negative here because a larger value for speech rate indicates slower pronunciation. 
ing quickly, and preserved them when speaking more slowly. The evidence that fast talkers do omit segments even when speaking slowly could be explained if fast speakers have a speaking style that facilitates fast speech, and they continue to use it even when speaking more slowly. Causal direction here is only implied: an alternative is that perhaps being fre quent omitters is the reason why they speak fast. Study 1b replicates this study in an additional environment. 
Study 1b: Post-vocalic deletion 
Materials and methods Log odds of deletion were pre dicted by a mixed effects logistic regression using ˜15,600 (post-vocalic, pre-consonantal) segments. Since the follow ing environment in this case was always a consonant, stress in the following environment was not included as a random intercept, but the manner of articulation of the following con sonant was. All other variables and controls were the same as Study 1a. 
Results Age and gender were not significant in this model as well, and were removed following a model comparison (model comparison p>.5). Phrase rate continued to be a significantly correlated with deletion in post-vocalic environ ment (β=-0.3, SE=0.047, z=-6.285, p<10-9). Speaker rate, however, did not have a significant effect (β=-0.087, SE=0.1, z=-0.843, p=0.399). Models that included an interaction term between phrasal and individual speech rate were not signifi cantly different (or better) than the model provided here. 
Discussion While speakers were more likely to omit seg ments when speaking quickly, fast speakers were not more likely to omit segments in post-vocalic positions when speak ing more slowly. It is possible that some reduction occurs, but does not culminate in outright deletion in this environ ment. Study 2a-b therefore focuses on more variable lenition types. 
Study 2 
Although deletion is an easily detected lenition process, it is also possible that fast speakers play a role in the reduction of segments to a less severe degree. To capture this, we es timated reduction using the difference in power between the maximum of the previous segment and the minimum of the segment under investigation. This has two advantages. The first is that, as noted, this measurement is able to capture par tial reductions rather than only outright deletions. The second is that the measurement is continuous rather than categorical. Lenition following modulation of intensity was discussed in Kingston (2008) and Katz (2016), and the procedure we use was inspired by Warner and Tucker (2011). The linear re gressions in Study 2 predict the power differences for ob struents in American English. A greater power difference indicates a less lenited, more reduced, or more consonant like segment. Smaller power differences indicate that the ob struent in question is closer in power to the vowel preceding it. This approach is more sensitive to variable lenition than a binary measurement (i.e. lenited or not), as it can capture 
1514
smaller phonetic differences in articulation. Study 2a focuses on intervocalic environment, while Study 2b focuses on post vocalic, pre-consonantal environment. 
Study 2a: Intervocalic lenition 
Materials and methods Power differences were predicted by a mixed effects linear regression using ˜19,300 intervo calic segments (fewer than in Study 1a as deleted segments are not usable). All other variables and controls were the same as Study 1a. 
Results Gender showed a significant effect, such that male speakers showed greater reduction in speech (β=-1.384, SE=0.53, t=-2.614, p<0.05). Younger speakers did not show a signiicantly different pattern of reduction than older speakers (β=-0.86, SE=0.52, t=-1.655, p=0.107). As ex pected, faster phrasal speech rate was correlated with greater amounts of reduction (β=0.37, SE=0.047, t=7.875, p<10-14). Faster individual speech rate was also correlated with greater amounts of reduction (β=0.91, SE=0.25, t=3.672, p<0.001), following the pattern of Study 1a. Unlike Study 1a, the effect size was greater for individual speech rate than for phrasal speech rate. 
Discussion These results strengthen the findings in Study 1a: Fast speakers are more likely to variably lenite and not only to omit segments even when speaking slowly. This lends support to the possibility that speakers are not fully flexible: speaking fast could affect the way speakers perform regard less of actual speech rate. 
Study 2b: Post-vocalic lenition 
Materials and methods Power differences were predicted by a mixed effects linear regression using ˜14600 post-vocalic pre-consonantal segments. All other variables and controls were the same as Study 1b. 
Results Male speakers were marginally more likely to reduce segments (β=-1.039, SE=0.59, t=-1.769, p<0.1). Younger speakers were significantly correlated with greater reduction of segments (β=-1.505, SE=0.58, t=-2.605, p<0.05). Faster phrasal speech rate was correlated with greater amounts of reduction (β=0.6, SE=0.056, t=10.698, p<10-15), as in Study 1b. Unlike Study 1b (and like Study 1a and Study 2a), faster individual speech rate was also cor related with greater amounts of reduction (β=1.12, SE=0.27, t=4.171, p<0.001), and, like Study 2a, the effect size was greater than that of phrasal speech rate. 
Discussion The results of Study 2b make it more likely that the absence of an effect for individual speech rate in Study 1b was not because fast speakers do not lenite more in post vocalic environment, but such lenition processes do not nec essarily culminate in outright deletion. 
General discussion 
One general result of this paper is the corroboration of previ ous lab-based findings that highlight the difference between 
casual and careful speech (Warner & Tucker, 2011). This paper provides an important extension of these results, and correlates variable lenition (as measured in power difference) with speech rate in a large detailed corpus and not only in lab settings. 
Our findings demonstrate that the typical speech rate of an individual does affect their behavior outside of known local effects. For all models, following the assumptions of most work in lenition, faster speech rate was found to be signif icantly correlated with increased rates of reduction. More over, in three of the four models (excepting Study 1b), the effect went beyond phrasal speech rate, the immediate rate around the word which contained a deletion, and extended to overall individual speech rate. In Study 1 this was shown for the deletion rate of intervocalic oral stops, a process not phonologically licensed in English, while in Study 2 it was replicated with power differences, a more nuanced measure ment of reduction. We therefore conclude that fast speakers show consistency in their lenition patterns, through greater reduction in their speech, over and above the effects of local speech rate. 
Importantly, the results draw attention to the inflexible properties of human speech. Cohen Priva (2017b) shows that fast speakers are more likely to use frequent words and less likely to use infrequent syntactic structures such as passive voice (without implying causal directionality). Though fast speakers could (and probably do) slow down when producing infrequent words and structures as suggested by several stud ies (e.g. Bell et al., 2009), they seem to use them less often. Speakers can and do change their speech rate while communi cating with other speakers, but our results show they are also likely to adopt typical behaviors compatible with their over all performance. Perhaps fast talkers get accustomed to lenit ing articulatory patterns and apply them elsewhere, or, alter natively, the adoption of leniting articulatory patterns makes some speakers fast speakers. At least two mechanisms could support such patterns, as suggested above. First, speakers may sample from their own production and operate on sam pled values before computing the contextual modification: if their own exemplar clouds have a high proportion of exem plars created in fast speech conditions, they are more likely to use them as a basis for subsequent productions. Second, in storage vs. computation frameworks such as O’Donnell (2015), speakers sometimes compute the most appropriate behavior, but sometimes use precomputed stored values: for fast speakers these are more likely to be lenited forms. 
Fast talkers omit more and lenite more than slower speak ers regardless of how fast they speak in an individual instance. For more nuanced reduction (Study 2), this effect coincided with the additional influence of gender and age, demographic variables typically found to influence sound change patterns. We conclude that fast speakers show a different pattern of re duction than slower speakers, and we hypothesize based on this evidence that fast talkers may play a critical role in the actuation of lenition-type sound change. Fast speakers may 
1515
be the first to consistently provide lenited forms in non-fast speech (outside the “natural environment” for lenited forms), or perhaps their increased use of lenited tokens drives the phonologization process. This could have its own interest ing implications for lenition-driven sound change and sound change by other means, such as mergers. In our results, male speakers trended to greater reduction, which goes against typ ical evidence that women and younger people are often the innovators of change (Labov, 2001). The result is consistent, however, with evidence that male speakers of American En glish tend to have faster speech rates than women, everything else being equal (Cohen Priva, 2017b; Cohen Priva et al., 2017). 
To sum, this paper offers two conclusions that should af fect future research in sound change and in the understanding of speech. First, it draws attention to fast speakers as likely initiators of lenition-type processes, as they are more likely to produce lenited forms overall but particularly in slow speech. Furthermore, our results show that even lenition-type behav ior can be persistent, and follow from other properties such as speech rate. 
References 
Baker, A., Archangeli, D., & Mielke, J. (2011). Vari ability in American English s-retraction suggests a so lution to the actuation problem. Language Varia tion and Change, 23(03), 347–374. doi: 10.1017/ S0954394511000135 
Bates, D., Machler, M., Bolker, B., & Walker, S. (2015). ¨ Fitting linear mixed-effects models using lme4. Jour nal of Statistical Software, 67(1), 1–48. doi: 10.18637/ jss.v067.i01 
Bauer, L. (2008). Lenition revisited. Journal of Linguistics, 44(3), 605–624. doi: 10.1017/S0022226708005331 Bell, A., Brenier, J., Gregory, M., Girand, C., & Jurafsky, D. (2009). Predictability effects on durations of content and function words in conversational English. Journal of Memory and Language, 60(1), 92–111. 
Boersma, P., & Weenink, D. (2008). Praat: Doing phonet ics by computer (Version 5.0.26) [Computer program]. (Retrieved June 16, 2008, from http://www.praat.org/) 
Cohen Priva, U. (2008). Using information content to predict phone deletion. In N. Abner & J. Bishop (Eds.), Pro ceedings of the 27th west coast conference on formal linguistics (pp. 90–98). Somerville, MA: Cascadilla Proceedings Project. 
Cohen Priva, U. (2015). Informativity affects consonant du ration and deletion rates. Laboratory Phonology, 6(2), 243–278. doi: 10.1515/lp-2015-0008 
Cohen Priva, U. (2017a). Informativity and the actuation of lenition. Language, 93(3), 569–597. doi: 10.1353/ lan.2017.0037 
Cohen Priva, U. (2017b). Not so fast: Fast speech correlates with lower lexical and structural information. Cog nition, 160, 27–34. doi: 10.1016/j.cognition.2016.12 
.002 
Cohen Priva, U., Edelist, L., & Gleason, E. (2017). Converg ing to the baseline: Corpus evidence for convergence in speech rate to interlocutor’s baseline. The Journal of the Acoustical Society of America, 141(5), 2989–2996. doi: 10.1121/1.4982199 
Cohen Priva, U., & Sanker, C. (2018). Distinct behaviors in convergence across measures. In Proceedings of the 40th annual conference of the cognitive science society. Austin, TX. 
Feinerer, I., & Hornik, K. (2017). tm: Text mining package [Computer software manual]. (R package version 0.7- 3) 
Garrett, A., & Johnson, K. (2013). Phonetic bias in sound change. In A. C. L. Yu (Ed.), Origins of sound change: Approaches to phonologization (pp. 51–97). Oxford, UK: Oxford University Press. 
Gurevich, N. (2004). Lenition and contrast : the func tional consequences of certain phonetically condi tioned sound changes. New York: Routledge. 
Honeybone, P. (2008). Lenition, weakening and consonan tal strength: tracing concepts through the history of phonology. In Lenition and Fortition. Berlin, Boston: De Gruyter. doi: 10.1515/9783110211443.1.9 
Jacewicz, E., Fox, R. A., & Wei, L. (2010). Between-speaker and within-speaker variation in speech tempo of Amer ican English. The Journal of the Acoustical Society of America, 128(2), 839–850. doi: 10.1121/1.3459842 
Jurafsky, D., Bell, A., Gregory, M. L., & Raymond, W. D. (2001). Probabilistic relations between words: Evi dence from reduction in lexical production. In J. L. By bee & P. Hopper (Eds.), Frequency and the emergence of linguistic structure (pp. 229–254). Amsterdam: Benjamins. 
Katz, J. (2016). Lenition, perception and neutralisa tion. Phonology, 33(1), 43–85. doi: 10.1017/ S0952675716000038 
Kingston, J. (2008). Lenition. In Selected proceedings of the 3rd conference on laboratory approaches to Spanish phonology (pp. 1–31). 
Kirchner, R. M. (1998). An effort-based approach to conso nant lenition. Unpublished doctoral dissertation, Uni versity of California Los Angeles. 
Kuznetsova, A., Bruun Brockhoff, P., & Haubo Bojesen Christensen, R. (2016). lmertest: Tests in linear mixed effects models [Computer software manual]. (R pack age version 2.0-33) 
Labov, W. (2001). Principles of linguistic change, volume 2, social factors. Oxford: Blackwell. 
Ladd, D. R., Turnbull, R., Browne, C., Caldwell-Harris, C., Ganushchak, L., Swoboda, K., . . . Dediu, D. (2013). Patterns of individual differences in the perception of missing-fundamental tones. Journal of Experimen tal Psychology: Human Perception and Performance, 39(5), 1386. 
1516
Lavoie, L. M. (2001). Consonant strength: Phonological pat terns and phonetic manifestations. Psychology Press. Milroy, J., & Milroy, L. (1985). Linguistic change, social network and speaker innovation. Journal of Linguis tics, 21(2), 339–384. 
Moisik, S. R., & Dediu, D. (2015). Anatomical biasing and clicks: Preliminary biomechanical modelling. In 18th international congress of phonetic sciences satellite event: The evolution of phonetic capabilities: Causes constraints, consequences (pp. 8–13). 
O’Donnell, T. J. (2015). Productivity and reuse in language: A theory of linguistic computation and storage. MIT Press. 
Pierrehumbert, J. (2001). Exemplar dynamics: Word fre quency, lenition and contrast. In J. Bybee & P. Hop per (Eds.), Frequency and the emergence of linguistic structure (pp. 137–157). John Benjamins Publishing Company. 
Pitt, M., Dilley, L., Johnson, K., Kiesling, S., Raymond, W., Hume, E., & Fosler-Lussier, E. (2007). Buckeye corpus of conversational speech (2nd release). Department of Psychology, Ohio State University. 
Quen, H. (2008). Multilevel modeling of between-speaker and within-speaker variation in spontaneous speech tempo. The Journal of the Acoustical Society of Amer ica, 123(2), 1104–1113. doi: 10.1121/1.2821762 
R Core Team. (2017). R: A language and environment for statistical computing [Computer software manual]. Vi enna, Austria. 
Raymond, W. D., Dautricourt, R., & Hume, E. (2006). Word medial /t,d/ deletion in spontaneous speech: Modeling the effects of extra-linguistic, lexical, and phonological factors. Language Variation and Change, 18. 
Warner, N., & Tucker, B. V. (2011). Phonetic variability of stops and flaps in spontaneous and careful speech. The Journal of the Acoustical Society of America, 130(3), 1606–1617. doi: 10.1121/1.3621306 
Weide, R. (2008). The CMU pronunciation dictionary, re lease 0.7a. (Carnegie Mellon University) 
Yu, A. C. L. (2013). Individual differences in socio-cognitive processing and the actuation of sound change. In A. C. L. Yu (Ed.), Origins of sound change: Ap proaches to phonologization (pp. 201–227). Oxford, UK: Oxford University Press. 
Yuan, J., Liberman, M., & Cieri, C. (2006). Towards an inte grated understanding of speaking rate in conversation. In Proceedings of interspeech (p. 541-544). Pittsburgh, PA. 
1517
Distinct behaviors in convergence across measures 
Uriel Cohen Priva*(uriel cohen priva@brown.edu) Brown University 
Providence, RI 02912 USA 
Chelsea Sanker*(cas443@cornell.edu) 
Brown University 
Providence, RI 02912 USA 
Abstract 
We present data on convergence in the Switchboard corpus, ad dressing differences across measures and across speakers. We measured convergence in four characteristics, to test consis tency in related and unrelated measures: F0 median, F0 vari ance, speech rate, and odds of the fillers uh and um. Conver gence was significant in all measures and exhibited variation both between individuals and within individuals. Most notably, convergence in one measure was not predictive of convergence in other measures, except between closely related measures. The results demonstrate some of the limitations of generaliz ing convergence results from one measure to other measures. 
keywords: convergence, individual differences, pitch, speech rate, fillers 
Introduction 
Speakers’ linguistic patterns are influenced by their recent in put; their characteristics tend to shift towards being more sim ilar to speech of interlocutors, in a phenomenon called con vergence, among other names (cf. Giles, Taylor, & Bourhis, 1973; Goldinger, 1998). In a study of the Switchboard cor pus (Godfrey & Holliman, 1997), we demonstrate that while convergence is observable across multiple measures, there are differences between measures, and that individual variation in convergence is not consistent across measures. 
Convergence has been observed in many behaviors, in cluding non-linguistic behaviors such as fidgeting (Chartrand & Bargh, 1999) and posture (Dijksterhuis & Bargh, 2001) and also many linguistic characteristics, including vowel for mants (e.g. Babel, 2012), VOT (e.g. Nielsen, 2011), F0 (e.g. Babel & Bulatov, 2011), jitter and shimmer (e.g. Levitan et al., 2012), lexical choice (e.g. Branigan, Pickering, Pear son, McLean, & Brown, 2011), syntactic constructions (e.g. Branigan, Pickering, & Cleland, 2000), and timing of turns and pauses (e.g. Natale, 1975; Street, 1984), among oth ers. For many measures, convergence has been demon strated both at the conversation level (e.g. Levitan et al., 2012; Nielsen, 2011) and dynamically within conversations (e.g. Street, 1984; Vaughan, 2011). Convergent shifts can persist even after the end of the conversation or exposure to stimuli (e.g. Babel, 2012; Pardo, 2006). 
Convergence is also apparent in holistic measures based on listeners’ decisions about the similarity of participants’ utterances before and after a task to the utterances of the model talker (e.g. Goldinger, 1998; Pardo, 2006). There is 
*Both authors contributed equally to this manuscript. 
some evidence for a correlation between results from holistic perceptual measures of convergence and acoustic character istics (Pardo, 2009), though other studies have found a lack of correlation between perceptual results and acoustic results (Babel & Bulatov, 2011; Pardo, Gibbons, Suppes, & Krauss, 2012). 
Despite the existence of much data demonstrating that con vergence occurs in many characteristics, there is little work examining whether or not convergence is comparable across different characteristics. Much work only measures conver gence in one characteristic, based on its relevance for a par ticular phenomenon or theory, e.g. effects of socially salient dialect differences (Drager, Hay, & Walker, 2010) or gener alization across phonological features (e.g. Nielsen, 2011). Other work includes multiple measures but does not directly compare the results across characteristics, instead focusing on testing factors that can influence degree of convergence, such as race (Babel, 2012), gender (Bilous & Krauss, 1988; Pardo, Jay, & Krauss, 2010), and status (Gregory & Webster, 1996). Such work usually does not explicitly address how the choice of measure might influence the result. 
Different measures of convergence have been demon strated to correlate with some of the same social factors, such as social characteristics of the speaker (Natale, 1975), part ners’ ratings of their relationship (Pardo et al., 2012), and ob servers’ ratings of the partners’ relationship (Levitan et al., 2012). Some convergence measures have also been observed to correlate similarly with the same objective measures, such as amount of overlapping speech (Levitan et al., 2012). 
However, there are few direct comparisons across mea sures. There is substantial variation in how each characteristic is influenced by aspects of the task and the participants, such as word frequency and speaker gender (e.g. Bilous & Krauss, 1988; Pardo, Urmanche, Wilman, & Wiener, 2017), as well as in the degree of convergence exhibited by different char acteristics, both unrelated characteristics, e.g. formants, F0, and turn durations (Sanker, 2015) and related characteristics, e.g. formants of different vowels (Babel, 2012). 
There is even less data on individual speakers’ tendency to converge. While previous studies have found variation in convergence across participants (e.g. Babel, 2012; Pardo et al., 2010), it is not clear how much of the variation re flects consistent characteristics of particular individuals and whether these patterns would be present in the same speakers in different tasks or using different measures of convergence. 
1518
Tamminga, Wade, and Lai (2018) found consistency in in dividual speakers’ patterns of convergence in two instances of the same task. Sanker (2015) similarly found consistency within the same interlocutor pair in different conversational tasks and in the same speaker in different pairs, but no ten dency for individual consistency across different measures. Bilous and Krauss (1988) also found differences in individual pairs’ degree of convergence in different measures. On the other hand, Rahimi, Kumar, Litman, Paletz, and Yu (2017) found a trend for positive correlations of convergence in some different measures, though it was not consistent across differ ent comparisons. 
Some scholars have identified certain personality traits that partially predict differences in convergence (Chartrand & Bargh, 1999; Natale, 1975; Yu, Abrego-Collier, & Sondereg ger, 2013), and argue that individual differences in attention to detail make some individuals more likely to exhibit con vergence in laboratory studies and also, on a larger scale, to propagate sound change (Yu et al., 2013). This model of in dividual difference suggests that different phonetic character istics are likely to behave the same way, which is not clearly reflected in existing data. While such studies offer correla tions of convergence or performance in other tasks with these external measures, they have not demonstrated that individual variation is consistent across tasks. 
In this paper, we examine variation in convergence across speakers and across measures. Beyond demonstrating that both types of variation exist, we show that the patterns ex hibited by each speaker in one measure are not predictive of that speaker’s patterns in other measures. To the best of our knowledge, this study is the first large-scale investigation of variation in convergence both across measures and across speakers. It is also the first study to examine individual ten dencies in convergence with a method that controls for effects that could be due to particular conversations and not conver gence per se. 
Methods 
Corpus 
The data for this study is the Switchboard Corpus (Godfrey & Holliman, 1997), a large collection of telephone conversa tions. Each speaker was randomly paired with other speak ers and given a topic for each conversation, providing a large corpus of natural speech data for many speakers in similar conversations with several different partners; recordings have caller identification information that can be used to compare the conversation with other instances of that caller. Each side of the conversation is a distinct recording, so measurements can reliably be taken for each speaker separately. 
Each conversation has associated information quantifying the clarity of the recording; after omitting calls with high lev els of background noise, echoing, or other issues, as indicated in the annotations of these calls, the set of data that we used had 464 speakers, in 3782 conversations. 
Speech rate measures were based on the manually cor 
rected word annotations produced at MS State (Harkins, Fe instein, Lindsey, Martin, & Winter, 2003), which allow mea surements of word duration. 
Measures Used 
The measures used were selected to provide a range of speech characteristics, both related and unrelated, to compare con vergence patterns in different types of measures. The meth ods for calculating each measure are given below; the F0 measures differ in some ways from more common versions of these measures established in prior work, in order to mini mize errors due to automated measurements in a large corpus. 
F0 median: Measured in Mels (Stevens, Volkmann, & Newman, 1937) and excluding tokens beyond density min ima at either end of the distribution, to exclude data points due to pitch tracking errors. While this might also exclude some cases of actual extreme F0s, checking some of the ap parent outliers confirmed that most of them are the result of erroneous pitch tracking halving or doubling the actual F0. 
F0 range: Log of the ratio of the 75th percentile to 25th percentile of F0 measurements in Hertz. Using the quotient rather than the difference was aimed at reducing the artificial correlation between F0 median and F0 range; with a differ ence or a standard deviation, the range would scale up in pro portion to the center of the distribution in a way that does not align with listener perceptions (cf. Jessen, Koster, & Gfroerer, ¨ 2005; Stevens et al., 1937). Using the quartiles reduced the sensitivity of the measurements to outliers and the outlier ex clusion method employed in the F0 data. 
Speech rate: Measured relative to the predicted dura tion based on each word’s median duration median dura tion within the Switchboard corpus, the length of the utter ance, and the distance from the end of the utterance. See Cohen Priva, Edelist, and Gleason (2017) and Cohen Priva (2017) for a discussion of the benefits of measuring speech rate in this way rather than as a raw value. 
Uh-Um log odds: The relative odds of encountering each of the fillers uh and um in the speech of each talker (cf. Ac ton, 2011; see also Clark & Fox Tree, 2002). To calculate log odds, a logistic regression was performed for all pairs of counts (e.g. <20 um, 2 uh>), and the log odds ratio was the predicted value of the regression plus the residuals of the regression. The advantage of using this method is that it can produce log odds for speakers who never used one or the other. Usage of fillers has been observed to vary, and some usage patterns align with interspeaker differences (Ac ton, 2011). 
Measuring Convergence 
Convergence was measured by comparing how speakers’ pro ductions within a conversation differ from their baselines in the direction of their partner’s baselines, measured from all conversations except for the one with the partner under con sideration, i.e. establishing independent baselines for each speaker and looking at the degree to which partners are con verge to the baselines of their interlocutors (cf. Cohen Priva 
1519
et al., 2017). This method is aimed at removing influences due to the conversation rather than the interlocutor. Many convergence studies compare speakers’ produc tions within a conversation to their interlocutor’s productions within that conversation. Using this method, situations in which both speakers shift in the same way will appear to be convergent, even though increase in similarity within a con versation can have a variety of causes that do not depend on sensitivity to the interlocutor’s speech, e.g. effects of the con versational topic or task. 
Some work includes comparisons with other speakers per forming the same task, to control for task-related effects (e.g. Levitan & Hirschberg, 2011; Sanker, 2015), but this does not control for effects of the particular conversation. There is sig nificantly greater similarity of speakers to their interlocutors as compared within a conversation than as compared to inter locutors’ characteristics from other conversations (Gregory & Webster, 1996). While this method may decrease the amount of actual convergence captured, convergence is still apparent when tested in this way. 
Establishing reliable baselines depends on having a large corpus, so that baselines are averaged across enough conver sations to not be thrown off by the particular characteristics of any particular conversation. 
Statistical Models 
Data was analyzed in R (R Core Team, 2017) with mixed effects models. There were separate models with each of the linguistic measures as the variable, for a total of four models. All models had two fixed effects: (1) the mean of the speaker’s performance in other conversations, and (2) the mean of the interlocutor’s performance in other conver sations. Thus, strong consistency across conversations would be reflected in high coefficient values for the speaker’s perfor mance in other conversations, and strong convergence would be reflected in high coefficient values for the interlocutor’s performance in other conversations. Speaker identity was not used as a random intercept, because characteristic pat terns of individual speakers are better modeled by their re spective baselines and models including both factors might fail to converge due to high collinearity. The models in cluded a random slope for the interlocutor’s baseline per formance, which was used to model the different degrees of convergence different that speakers may exhibit. The coeffi cients provided below were modeled using the lmerTest pack age (Kuznetsova, Bruun Brockhoff, & Haubo Bojesen Chris tensen, 2015) which builds on lme4 (Bates, Machler, Bolker, ¨ & Walker, 2015) to include a calculation of degrees of free dom and p-values. 
Models produced by lme4 returned zero variance for sev eral of the random slopes for speaker, which seemed to be an unlikely estimate of individual speakers’ consistency in convergence across conversations. We therefore retrained the models using the brms package (Burkner, 2017), which found ¨ more non-zero random slopes, but produced less consistent results due to its sampling-based nature. In order to estab 
lish a more consistent estimate of individual variation, we re peated the sampling procedure 350 times for each model and used the median value of per-speaker estimates. These values are used below when testing correlations between speakers’ convergence in different domains (cf. Tamminga, 2017). 
Results 
Speaker and interlocutor baselines as predictors Within the mixed effects models for each speech characteris tic, by far the main predictor of that variable is the speaker’s mean performance in that measure from other conversations, which was highly significant for all of the four measures in vestigated, as given in Table 1. That is, speakers were very consistent in their production patterns across conversations. 
Table 1: Speaker baseline as a predictor of each variable β SE t value p value 
F0 median 0.971 0.00387 250.71 < 0.0001 F0 var. 0.676 0.012 56.39 < 0.0001 log(uh:um) 0.788 0.009 87.65 < 0.0001 speech rate 0.795 0.0088 90.37 < 0.0001 
Interlocutor baseline as a predictor of each variable was also significant, though the effect was much smaller than speaker baseline. This measure was capturing convergence; the positive coefficient in all cases, as given in Table 2, indi cates convergence in all measures. 
Table 2: Interlocutor baseline as a predictor of each variable β SE t value p value 
F0 median 0.0176 0.00404 4.36 < 0.0001 F0 var. 0.0924 0.0124 7.47 < 0.0001 log(uh:um) 0.0311 0.0099 3.14 0.00186 speech rate 0.0471 0.0088 5.35 < 0.0001 
Some characteristics exhibited more convergence than oth ers, but all measures exhibited significant convergence and the size of the effect was within the same order of magni tude. The measure which exhibited the strongest evidence of convergence was F0 variability. 
Correlations between measures, by individual The individual-level variation between speakers in degree of convergence, i.e. the extent to which their productions were predicted by the interlocutor’s baseline in a particular mea sure, would be close to zero if speakers were not consistent in the degree to which they converged across conversations. Consistency of convergent behavior within a speaker would be reflected in non-zero standard deviation for the random slope in each model. The models generated by the brms pack age consistently resulted in standard deviation estimates that were positive and of the same order of magnitude as the coef ficient for the interlocutor’s baseline performance; however, 
1520
the 95% confidence interval included 0 for all models except the uh to um ratio model. This result indicates that speakers’ degree of convergence in one conversation was only a weak predictor of their convergence in other conversations. 
However, differences between individual speakers’ conver gence tendencies were large enough to allow a comparison of convergence between different measures, by speaker. Speak ers exhibited little consistency in degree of convergent change across different characteristics, as illustrated in Table 3; a speaker’s convergence patterns in one measure were not pre dictive of that speaker’s convergence patterns in other mea sures. 
Table 3: Correlations between speaker-level convergence in each pair of measures (F0 median, F0 range, log odds of uh:um ratio, and speech rate). 
F0 var. uh:um speech rate 
F0 median Pearson r 0.22 0.07200 -0.0305 Sig. (two-tailed) < 0.0001 0.12200 0.5120 
F0 var. Pearson r 0.00027 0.0596 Sig. (two-tailed) 0.99500 0.2000 
log(uh:um) Pearson r -0.0702 Sig. (two-tailed) 0.1260 
The only comparison in which the correlation in conver gence across speakers was significant was between F0 median and F0 variability. To rule out the possibility that the correla tion was due to the strong correlation that exists in production between F0 variance and F0 median (Jessen et al., 2005), we fitted a linear regression with cubic functions applied to F0 median in Mels, log F0 median in Hz, and F0 median. We then used the model to extract the residuals of F0 variance, i.e. the component of F0 variance that was not explained by the three predictors. We then repeated the procedure outlined above for the F0 variance model, using the residualized val ues. The two measures were still significantly correlated, al beit to a lesser extent (Pearson r = 0.106, p= 0.022). 
Among pairs of measures other than F0 median and F0 variability, there were no significant correlations in conver gence, nor any trend towards positive correlation. Notably, there is no intrinsic correlation in production between any of these pairs of measures. The large number of speakers (n = 464) makes it unlikely that a lack of correlation could result from an inadequate sample size, which would be a concern in a smaller scale study. In addition, the significant correla tion between F0 variance and F0 median demonstrates that these methods can capture individuals’ consistency across measures when a relationship exists, despite individual speak ers having only a weak trend towards consistency in conver gence across conversations. Thus, the results are likely to be capturing an actual lack of relationship between convergence in different measures. 
Discussion and Conclusion 
In convergence studies, the measure used can have a large im pact on the results, as different measures can exhibit different overall degrees of convergence as well as different influences (Bilous & Krauss, 1988; Pardo et al., 2017; Sanker, 2015). We extend the data on this variation within a large corpus of natural speech, confirming differences in convergence mea sured in different characteristics. 
The different size of the convergence effect in different measures has potential implications for design of future con vergence studies. While all of the measures exhibited signif icant convergence, the differences are large enough that in a smaller sample, they might not all reach significance, which makes the measure with the most convergence, F0 variability, a promising characteristic to use in measuring convergence, at least within conversational tasks; it is not frequently used, though there are some convergence studies that have included it (e.g. Vaughan, 2011). It may be that measures of variation have a slight advantage in capturing the dynamic aspects of convergence, while means and medians obscure some of it by collapsing over long time spans. 
In addition to variation in convergence across measures, there is variation across speakers. However, speakers were not strongly consistent in their degree of convergence across conversations, suggesting that convergence is more influ enced by aspects of particular conversations than characteris tics of each individual independently. Other work has found a larger effect of speaker consistency, at least within closely related tasks: Between instances of same task, either a con versation with a set topic (Sanker, 2015) or shadowing of set stimuli (Tamminga et al., 2018), or between different conver sational tasks with the same partner (Sanker, 2015). Individ ual tendencies in convergence might be more apparent with more constrained conditions across the tasks being compared, because there is less possibility of an effect of contextual fac tors like interlocutor and conversational topic. 
Convergence exhibited by a speaker in one measure was not correlated with convergence in other measures. The lack of correlation between measures indicates that variation in convergence across speakers cannot be attributed to consis tent differences in processing style, with different listeners fo cusing more or less on low-level detail (cf. Yu, 2013). Rather, the results suggest that individual differences in attention to detail might depend on the particular characteristic, which is consistent with variation across listeners in which acoustic cues they attend to for the same phonological and structural contrasts (e.g. Hazan & Rosen, 1991; Roy, Cole, & Mahrt, 2017). On the other hand, other studies have found a lack of correlation in cue weighting between perception and pro duction, e.g. in participants’ use of F0 and VOT as cues for stop contrasts (e.g. Schertz, Cho, Lotto, & Warner, 2015), so it is not clear whether perceptual weighting of different cues would extend to convergence or not. The lack of consistency in convergence across measures might also in part be due to speakers’ variability in convergence across conversations. 
1521
The one exception to the independent patterns of conver gence in different measures was the relationship between F0 median and in F0 range. Adapting the model for F0 vari ance to include F0 median as a predictor substantially reduced the correlation in convergence, which suggests that this cor relation is largely an effect of the correlation in production between F0 median and F0 range. However, the correlation in convergence between these two measures was still signif icant in this model, which might suggest a perceptual link, with attention to low-level detail in F0 reflected in both mea sures. On the other hand, it could also be due to an indi rect effect of other factors that are correlated with both char acteristics, though comparing speakers’ productions to their partners’ baselines rather than their partners’ measurements within their shared conversations makes this less likely. 
Phonetic convergence is often presented as evidence for episodic memory of utterances, in which speakers store de tails of each instance of hearing a word or phoneme, with greater weight given to recent exemplars (e.g. Goldinger, 1998). Though such models do not specifically address pre dictions about variation across speakers and across linguis tic characteristics, the observed differences in convergent be havior could be consistent with a hybrid exemplar model in which exemplar clouds are shaped by a system of abstractions (e.g. Pierrehumbert, 2002), such that speakers can differ in how they weight exemplars for different characteristics. This different weighting of cues could easily be integrated into the model, as it already allows differential weighting of recent and otherwise salient exemplars. 
While a shift in representation based on episodic memory is strongly supported by convergent shifts which continue at long delays after input, this effect has not been tested for all measures, and they might not all behave similarly. Some convergent effects may be based on priming and perceptual behavioral links rather than a shift in representation, as is proposed by some analyses, particularly for non-linguistic convergence (e.g. Dijksterhuis & Bargh, 2001; Giles et al., 1973). Characteristics which are cues to a phonological con trast, such as vowel formants, might also have different repre sentations than characteristics which are not associated with a phonological contrast, such as F0 in English. The existence of multiple explanations underlying convergence would be con sistent with different convergence patterns in different mea sures; comparisons across measures can help test the predic tions made by different explanations and representations. 
The differences in convergence in different characteristics demonstrate the importance of considering convergence sep arately for different measures, not just in building linguis tic models but also when interpreting experimental results, as convergence patterns observed in one characteristic might not be paralleled in other characteristics. In addition, the lack of correlation in individuals’ behavior across measures demonstrates a limitation of using individual variation in re sults from a single measure to characterize individual differ ences in perception or phonological processing. 
References 
Acton, E. K. (2011). On gender differences in the distribu tion of um and uh. In University of Pennsylvania working papers in linguistics (Vol. 17, chap. 2). 
Babel, M. (2012). Evidence for phonetic and social se lectivity in spontaneous phonetic imitation. Journal of Phonetics, 40(1), 177–189. doi: http://dx.doi.org/10.1016/ j.wocn.2011.09.001 
Babel, M., & Bulatov, D. (2011). The role of fundamental fre quency in phonetic accommodation. Language and Speech, 55(2), 231–248. doi: 10.1177/0023830911417695 
Bates, D., Machler, M., Bolker, B., & Walker, S. (2015). ¨ Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 1–48. doi: 10.18637/jss.v067 .i01 
Bilous, F. R., & Krauss, R. M. (1988). Dominance and ac commodation in the conversational behaviours of same-and mixed-gender dyads. Language & Communication, 8(3), 183–194. doi: 10.1016/0271-5309(88)90016-X 
Branigan, H. P., Pickering, M. J., & Cleland, A. A. (2000). Syntactic co-ordination in dialogue. Cognition, 75, B13– 25. doi: 10.1016/S0010-0277(99)00081-5 
Branigan, H. P., Pickering, M. J., Pearson, J., McLean, J. F., & Brown, A. (2011). The role of beliefs in lexical alignment: Evidence from dialogs with humans and computers. Cog nition, 121, 41–57. doi: 10.1016/j.cognition.2011.05.011 
Burkner, P.-C. (2017). brms: An R package for Bayesian ¨ multilevel models using Stan. Journal of Statistical Soft ware, 80(1), 1–28. doi: 10.18637/jss.v080.i01 
Chartrand, T. L., & Bargh, J. A. (1999). The chameleon effect: The perception–behavior link and social interaction. Journal of Personality and Social Psychology, 76(6), 893. doi: 10.1037/0022-3514.76.6.893 
Clark, H. H., & Fox Tree, J. E. (2002). Using uh and um in spontaneous speaking. Cognition, 84(1), 73–111. doi: 10.1016/S0010-0277(02)00017-3 
Cohen Priva, U. (2017). Not so fast: Fast speech correlates with lower lexical and structural information. Cognition, 160, 27–34. doi: 10.1016/j.cognition.2016.12.002 
Cohen Priva, U., Edelist, L., & Gleason, E. (2017). Con verging to the baseline: Corpus evidence for convergence in speech rate to interlocutor’s baseline. Journal of the Acoustical Society of America, 141(5), 2989–2996. doi: 10.1121/1.4982199 
Dijksterhuis, A., & Bargh, J. (2001). The perception behavior expressway: Automatic effects of social percep tion on social behavior. Advances in Experimental Social Psychology, 33, 1–40. doi: 10.1016/S0065-2601(01)80003 -4 
Drager, K., Hay, J., & Walker, A. (2010). Pronounced rival ries: Attitudes and speech production. Te Reo, 53, 27–53. Giles, H., Taylor, D., & Bourhis, R. (1973). Towards a theory 
of interpersonal accommodation through language: Some Canadian data. Language in Society, 2, 177–192. Godfrey, J. J., & Holliman, E. (1997). Switchboard-1 release 
1522
2. (Linguistic Data Consortium, Philadelphia) Goldinger, S. D. (1998). Echoes of echoes? An episodic the ory of lexical access. Psychological Review, 105(2), 251– 279. doi: 10.1037/0033-295X.105.2.251 
Gregory, S. W. J., & Webster, S. (1996). A nonverbal sig nal in voices of interview partners effectively predicts com munication accommodation and social status perceptions. Journal of Personality and Social Psychology, 70(6), 1231– 1240. 
Harkins, D., Feinstein, D., Lindsey, T., Martin, S., & Winter, G. (2003). Switchboard MS State manu ally corrected word alignments. https://www.isip .piconepress.com/projects/switchboard/. 
Hazan, V., & Rosen, S. (1991). Individual variability in the perception of cues to place contrasts in initial stops. Per ception & Psychophysics, 49(2), 187–200. doi: 10.3758/ BF03205038 
Jessen, M., Koster, O., & Gfroerer, S. (2005). Influence ¨ of vocal effort on average and variability of fundamental frequency. International Journal of Speech, Language and the Law, 12(2), 174–213. 
Kuznetsova, A., Bruun Brockhoff, P., & Haubo Bojesen Christensen, R. (2015). lmerTest: Tests in linear mixed effects models [Computer software manual]. (R package version 2.0-29) 
Levitan, R., Gravano, A., Willson, L., Benuˇ s,ˇ S., Hirschberg, ˇ J., & Nenkova, A. (2012). Acoustic-prosodic entrainment and social behavior. In Proceedings of the conference of the North American chapter of the Association for Com putational Linguistics: Human language technologies (pp. 11–19). Stroudsburg: Association for Computational Lin guistics. 
Levitan, R., & Hirschberg, J. B. (2011). Measuring acoustic prosodic entrainment with respect to multiple levels and di mensions. In Proceedings of Interspeech. Brisbane: Inter national Speech Communications Association. 
Natale, M. (1975). Social desirability as related to conver gence of temporal speech patterns. Perceptual and Motor Skills, 40, 827–830. 
Nielsen, K. (2011). Specificity and abstractness of VOT im itation. Journal of Phonetics, 39, 132–142. doi: 10.1016/ j.wocn.2010.12.007 
Pardo, J. S. (2006). On phonetic convergence during conver sational interaction. The Journal of the Acoustical Society of America, 119(4), 2382–2393. doi: 10.1121/1.2178720 
Pardo, J. S. (2009). Expressing oneself in conversa tional interaction. In E. Morsella (Ed.), Expressing one self/expressing one’s self: Communication, cognition, lan guage, and identity (pp. 183–196). Hove: Psychology Press. 
Pardo, J. S., Gibbons, R., Suppes, A., & Krauss, R. M. (2012). Phonetic convergence in college roommates. Jour nal of Phonetics, 40, 190–197. doi: 10.1016/j.wocn.2011 .10.001 
Pardo, J. S., Jay, I. C., & Krauss, R. M. (2010). Conversa 
tional role influences speech imitation. Attention, Percep tion, and Psychophysics, 72(8), 2254–2264. doi: 10.3758/ BF03196699 
Pardo, J. S., Urmanche, A., Wilman, S., & Wiener, J. (2017). Phonetic convergence across multiple measures and model talkers. Attention, Perception, & Psychophysics, 79, 637– 659. doi: 10.3758/s13414-016-1226-0 
Pierrehumbert, J. (2002). Word-specific phonetics. In C. Gussenhoven & N. Warner (Eds.), Laboratory phonol ogy VII (pp. 101–140). Berlin: Mouton de Gruyter. 
R Core Team. (2017). R: A language and environment for sta tistical computing [Computer software manual]. Vienna. Rahimi, Z., Kumar, A., Litman, D., Paletz, S., & Yu, M. 
(2017). Entrainment in multi-party spoken dialogues at multiple linguistic levels. In Proceedings of Interspeech (pp. 1696–1700). Stockholm, Sweden. 
Roy, J., Cole, J., & Mahrt, T. (2017). Individual differences and patterns of convergence in prosody perception. Labo ratory Phonology, 8(1), 1–36. doi: 10.5334/labphon.108 
Sanker, C. (2015). Comparison of phonetic convergence in multiple measures. In Cornell working papers in phonetics and phonology 2015 (pp. 60–75). 
Schertz, J., Cho, T., Lotto, A., & Warner, N. (2015). In dividual differences in phonetic cue use in production and perception of a non-native sound contrast. Journal of Pho netics, 52, 183–204. doi: 10.1016/j.wocn.2015.07.003 
Stevens, S., Volkmann, J., & Newman, E. (1937). A scale for the measurement of psychological magnitude pitch. Jour nal of the Acoustical Society of America, 8, 185–190. 
Street, R. L. (1984). Speech convergence and speech eval uation in fact-finding interviews. Human Communication Research, 11(2), 139–169. doi: 10.1111/j.1468-2958.1984 .tb00043.x 
Tamminga, M. (2017, May). How much do individuals vary? Talk presented at the Stanford Linguistics Depart ment’s colloquium. 
Tamminga, M., Wade, L., & Lai, W. (2018, January). Stability and variability in phonetic flexibility. Talk presented at the Linguistic Society of America annual meeting. (https://www.linguisticsociety.org/ abstract/stability-and-variability-phonetic 
-flexibility) 
Vaughan, B. (2011). Prosodic synchrony in co-operative task based dialogues: A measure of agreement and disagree ment. In S. Ohlsson & R. Catrambone (Eds.), Proceedings of Interspeech (pp. 1865–1868). ISCA. 
Yu, A. (2013). Individual differences in socio-cognitive pro cessing and the actuation of sound change. In A. Yu (Ed.), Origins of sound change: Approaches to phonologization (pp. 201–227). Oxford, England: Oxford University Press. 
Yu, A., Abrego-Collier, C., & Sonderegger, M. (2013). Pho netic imitation from an individual-difference perspective: Subjective attitude, personality and “autistic” traits. Jour nal of Statistical Software, 8(9), e74746. doi: 10.1371/ journal.pone.0074746 
1523
Are emoji a poor substitute for words? Sentence processing with emoji substitutions 
Neil Cohn (neilcohn@visuallanguagelab.com),  
Tim Roijackers, Robin Schaap, and Jan Engelen (j.a.a.engelen@tilburguniversity.edu) Tilburg University, Tilburg center for Cognition and Communication (TiCC)  
P.O. Box 90153, 5000 LE Tilburg, The Netherlands 
Abstract 
With the integration of emoji into digital keyboards, people are  
increasingly using multimodal interactions between text and  
image in real-time interactions. One technique of using emoji  
is to substitute them into sentences. We here investigate the  
online processing of these interactions, by modulating either  
the grammatical category of those substitutions (Experiment 1:  
nouns vs. verbs) or the type and location of substitutions  
(Experiment 2: emoji vs. logos, within sentences vs. at their  
end). We found a processing cost for self-paced reading times  
of images compared to words, which indeed extended past the  
emoji itself, but no difference in comprehensibility ratings  
between word and congruent-image substitutions. Overall,  these results suggest that, despite costs of switching modalities,  text and images can be integrated into holistic multimodal  expressions.  
Keywords: multimodality; sentence processing; emoji; visual  language 
General Introduction 
Human communication is naturally multimodal, exemplified  in face-to-face interaction by the convergence of speech and  gesture (McNeill, 2000). However, digital text-based  communication renders such bodily features unavailable to  speakers. In their place, emoji have become a prevalent non 
verbal indicator of emotional and pragmatic information.  Emoji are pictographic expressions integrated as a semi standardized inventory with messaging applications and  computer operating systems (Danesi, 2016). They typically  fall in two distributions relative to sentences: either following  a sentence or substituted into it (Cramer, de Juan, & Tetreault,  2016): 
John loves eating �every Friday. 
� 
John loves eating pizza every Friday.  
Here, we examine this relationship between emoji and  sentence structure. 
The substitution of one modality into another is a possible  feature of nearly all multimodal interactions (Cohn, 2016):  Gestures can replace words in speech (McNeill, 2000),  images can replace written words in sentences, and words can  substitute for images in the structure of a visual narrative  sequence (Cohn, 2016). In all of these cases, the structure of  one modality (syntax, narrative) is retained while a unit from  another modality substitutes for a unit in that dominant  sequence. For emoji, such substitutions are now increasingly facilitated by messaging programs (Apple Messenger,  WhatsApp) which suggest emoji to replace for words while a  user is typing, as in Figure 1. 
Figure 1: Suggestions of emoji for words in Apple iOS 
Only some research has examined these types of  substitutive relationships between images and words. Potter  et al. (1986) presented participants with sentences at a rapid  presentation rate where images were either congruous or  incongruous with their substituted nouns. Comprehension  and recall of substituted images were only marginally more  strained than regular words, and this effect was maintained  regardless of the substitutions’ ordinal position in the  sentence (middle vs. end) or the number of words which were  replaced. Additional work has also suggested that  participants can accurately interpret sentences whether nouns  or verbs are substituted for images (Mihalcea & Leong,  2008). Nevertheless, these effects were greater for sentences  which are shorter and less syntactically complex, and with  high frequency words.  
Additional research has examined the time it takes to read  images or emoji substituted into sentences. In general,  substituting emoji for words in a message requires a reader to  take more time to read than a message of only text  (Gustafsson, 2017). Other work has suggested that the time it  takes to view an image replacing a word can be modulated by  the sentence context. For example, verb aspect can modulate  a replaced-image depending on whether it depicts a state  congruous with the type of event described, i.e., ongoing vs.  completed actions (Madden & Therriault, 2009). 
Further studies measuring event-related brain potentials  has implicated that a common semantic system underlies the  processing of both modalities. When images have been  substituted for sentence-final words, incongruous images  elicit neural responses indicating more strained semantic  processing (N400) than substitutions by congruous images,  and these waveforms are similar to those to incongruous or  unexpected words (Federmeier & Kutas, 2001; Ganis, Kutas,  & Sereno, 1996; Nigam, Hoffman, & Simons, 1992). These  results imply that sentence contexts modulate the semantic  processing of images similar to their modulation by words,  
1524
despite the modality-switch. In addition, the inverse effect  has been observed for the substitution of words for images in  visual narrative sequences of comics (such as Pow! replacing  a climactic punch)—words incongruous to the narrative  sequence elicit a larger N400 than congruous words  (Manfredi, Cohn, & Kutas, 2017). Altogether, this work  implies that a common semantic system can be expressed by  multiple modalities, while negotiating the grammatical  structure of one of them (Cohn, 2016).  
In this paper, we investigate further aspects of multimodal  text-image interactions in sentences by going beyond  semantic manipulations alone. We used a self-paced reading  paradigm (e.g., Aaronson & Scarborough, 1976) to  investigate the online processing of sentences while  modulating either the grammatical categories of those  substitutions (Experiment 1: nouns vs. verbs) or the type of  image substitutions (Experiment 2: emoji vs. logos). 
Experiment 1: Grammatical categories Prior studies have primarily investigated the semantics of  images substituted for words in sentences, and the  
sentences were manipulated further by creating “normal  substitutions” which replaced an emoji either for a noun (the  grammatical object) or for a verb. “Switched substitutions”  were created by then reversing the positions of the “normal”  noun and verb emoji. For example, as in Table 1 a normal  sentence substitutes a pizza emoji for the noun pizza, and a  heart for the verb loves. In the switched versions, a pizza  replaces loves and a heart replaces pizza. Experimental  conditions were interleaved with additional sentences (see  Experiment 2) which together were counterbalanced across 8  lists in a Latin Square design such that no list repeated a  sentence.  
Following Mihalcea and Leong (2008), all sentences  were designed to be simple and easy to read, with no difficult  words. All sentences used the present simple tense and were  based on the available emoji vocabulary set from Apple iOS.  Canonical meanings of emoji were used, as outlined by  Emojipedia.org. 
Table 1: Example sentences used in Experiment 1 Noun Verb 
comprehension of those sentences. However, most of these  studies have only substituted images for nouns in sentences.  Nevertheless, perhaps the most well-known image  substitution replaces a verb: I ❤ NY. We thus asked whether  substitutions differ depending on their grammatical category. 
A difference in processing noun and verb substitutions by  emoji may be expected, given that objects (typical of nouns)  
No emoji John loves eating  pizza every Friday. 
Normal John loves eating  � every Friday. 
Switched John � eating  pizza every Friday. 
John loves eating  pizza every Friday. John ❤ eating pizza  every Friday. 
John loves eating ❤ every Friday. 
are easier to depict in a straightforward pictorial  representation than events (typical of verbs). Indeed, the  argument structure of verbs (e.g., Gruber, 1965) means that a  pictorial depiction would collapse across both the verb (e.g.,  run) and its arguments (e.g., a person), thus conveying more  information in one unit (e.g., �). If an emoji were to replace  a verb in this fashion, the arguments may thus be repeated,  and the relationships between the verb and nouns weakened  by the absence of a lexicalized verb. Such a prediction was  made by Potter et al. (1986) in their study of image  substitutions for nouns. 
We therefore first asked whether substitutions for verbs  differ in online processing from verbs, and further, whether  emoji more typical of one grammatical category would be  anomalous if perceived in a different syntactic position (such  as emoji replaceable for nouns moved to verb position). In  addition, because previous substitutions have appeared at the  sentence-final position, it did not allow for assessing any  downstream effects of substitutions on sentence processing.  We therefore presented participants with sentences replacing  emoji for nouns and verbs, or reversing their positions, in a  self-paced reading task where we measured how long  participants viewed each word in a sentence. Our analysis  focused on both the critical position, and downstream effects  up to two subsequent positions after the critical word. 
Methods 
Stimuli We created 32 unique base sentences which  described a variety of actions and events. These “no-emoji”  
Participants We recruited 72 participants (31 female; mean  age: 26.8, range: 17-62) in the online study. Using a 1 (low)  to 7 (high) scale, participants reported having a fluent level  of English proficiency (M=5.5, SD=1.32) though most  participants were Dutch and spoke English as a second  language. They also reported high levels of frequency of  using texting applications (M = 6.44, SD=.9), using emoji (M  = 4.97, SD = 1.2), and emoji familiarity (M = 5.78, SD = 1.1). 
Procedure Participants were presented with an online  experiment via Qualtrics, and we used the jspsych javascript  plugin for the self-paced reading experiment. After  consenting to their participation, participants were given  instructions for the experiment where they were told to read  sentences by pressing a button for each subsequent word.  Trials began with a screen reading “Ready” followed by each  sentence word-by-word, centered on the screen, which  advanced with a button press. After each sentence,  participants rated the sentence for its comprehensibility (How  much did this sentence make sense?) and enjoyability (How  enjoyable was this sentence?) using 7-point Likert scales. If  there was a substitution, they were then asked to fill in which  word they thought the emoji replaced in the sentence. 
Data Analysis Reading times were analyzed using a subjects  analysis which collapsed across items. Outlier removal was  performed on reading times which omitted all datapoints  greater than 2.5 standard deviations above the mean, and all  below a threshold of 300 milliseconds. We first used a 2  
1525
Figure 2: Self-paced reading times from Experiment 1 across sequence positions. Error bars depict standard error. 
(Category: Noun, Verb) x 3 (Type: No emoji, Normal,  Switched) x 3 (Critical Position: Critical Word, CW+1,  CW+2) repeated measures ANOVA to analyze reading times. Additional follow up ANOVAs at each critical position were  used to further analyze the relations between sentence types,  
and we used post-hoc tests with a Bonferroni correction. Because both No Emoji conditions (noun and verb) came  from the same sentences, our analysis of comprehension  scores could not use a factorial ANOVA. We therefore  measured ratings using a one-way repeated-measures  ANOVA across all five sentence types. Finally, participants’ accuracy for recognizing which words were replaced by  emoji was assessed with a 2 (Category: Noun, Verb) x 2  (Type: Normal, Switched) repeated-measures ANOVA. 
Results 
Our omnibus analysis found main effects of Type and Critical  Position (all Fs>82.1, all ps<.001), but not Category (p=.233). This arose because, on average, viewing times for  Switched emoji were longer than Normal emoji, which in  turn were longer than normal words. In addition, reading  times at the critical word were longer than those at  subsequent words. Additional interactions appeared between  Type, Position, and/or Category (all Fs>31, all ps>.05). 
Analyses at each critical position clarified these findings.  At the critical word, we found a main effect of Type,  F(2,144)=82.8, p<.001, and Category, F(1,72)=4.6, p<.05,  and an interaction between them, F(2,144)=6.3, p<.005.  These results arose because, as depicted in Figure 2, Normal  emoji substitutions were read longer than normal words, and  Switched emoji were even longer than Normal emoji  (p<.005), but only in the Noun position. Normal and  Switched emoji did not differ in the Verb position (p=.44). 
At the panel after the critical word, we again found a main  effect of Type, F(2,144)=34.6, p<.001, but not Category  (p=.716), and an interaction between them, F(2,144)=3.1,  p<.05. This arose because again, words after Switched emoji  
were read longer than after Normal noun emoji (p<.05), but  not after normal verb emoji (p=.05). Words after normal noun  emoji were also slower than those after normal words  (p<.001). 
Finally, two positions after the critical word, we found a  main effect of Type, F(2,144)=9.04, p<.001, but not of  Category (p=.258), nor an interaction between them  (p=.164). This arose because words two positions after  Switched emoji were still read slower than after both Normal  emoji and words (p<.001), though words after Normal emoji  and words did not differ (p=1.0). No differences were  observed for those following nouns versus verbs (p=.258). 
Figure 3: Ratings for comprehensibility of sentences  (1=hard to 7=easy to understand) in a) Experiment 1, and b)  Experiment 2. Error bars depict standard error. 
1526